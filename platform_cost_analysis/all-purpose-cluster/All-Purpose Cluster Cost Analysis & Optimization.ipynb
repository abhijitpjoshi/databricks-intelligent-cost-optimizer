{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b5b9b1-43b4-43d4-b5ad-a91ff163cfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# All-Purpose Cluster Cost Analysis & Optimization\n",
    "\n",
    "## Overview\n",
    "This notebook analyzes all-purpose cluster costs and identifies optimization opportunities across users, clusters, and instance types.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "### Data Collection & Analysis\n",
    "* Extracts all-purpose cluster usage from `system.billing.usage`\n",
    "* Enriches with telemetry data from `system.compute.node_timeline`\n",
    "* Aggregates costs at user, cluster, and instance levels\n",
    "* Calculates CPU and memory efficiency metrics\n",
    "\n",
    "### Optimization Recommendations\n",
    "* Identifies under-utilized clusters with specific instance type recommendations\n",
    "* Provides actionable recommendations with estimated savings\n",
    "* Includes driver and worker instance type details\n",
    "* Shows autoscaling configuration (min/max workers)\n",
    "\n",
    "## Output Tables (ex_dash_temp.billing_forecast)\n",
    "\n",
    "**Base Tables:**\n",
    "1. `all_purpose_base` - Raw usage data with cluster metadata\n",
    "2. `user_daily_telemetry` - Daily user-level metrics\n",
    "3. `cluster_daily_telemetry` - Daily cluster-level metrics\n",
    "4. `instance_daily_telemetry` - Daily instance-level metrics\n",
    "\n",
    "**Aggregated Tables:**\n",
    "5. `user_total_cost` - One row per user with total costs\n",
    "6. `cluster_total_cost` - One row per cluster with total costs\n",
    "7. `instance_total_cost` - One row per instance type with total costs\n",
    "\n",
    "**Opportunity Tables:**\n",
    "8. `user_opportunities` - User-level optimization recommendations\n",
    "9. `cluster_opportunities` - Cluster-level optimization recommendations\n",
    "10. `instance_opportunities` - Instance-level optimization recommendations\n",
    "\n",
    "## How to Use\n",
    "1. Set the `start_date` widget to your desired analysis period (default: last 30 days)\n",
    "2. Run all cells sequentially\n",
    "3. Review the opportunity tables for cost optimization recommendations\n",
    "4. Focus on CRITICAL and HIGH priority items for maximum impact\n",
    "\n",
    "## Key Features\n",
    "* Specific instance type recommendations (e.g., \"Change r7g.12xlarge ‚Üí r7g.8xlarge\")\n",
    "* Worker configuration details (autoscale vs fixed)\n",
    "* Validated savings (capped at total cost)\n",
    "* One row per entity (no duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae352a1b-493f-48e0-a667-c5cecbf16222",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup: Date Widget and Schema Creation"
    }
   },
   "outputs": [],
   "source": [
    "# Setup: Days Widget and Schema Creation\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, sum as spark_sum, avg, count, countDistinct, round as spark_round, coalesce, lit, when\n",
    "\n",
    "displayHTML(\"<h2>ALL-PURPOSE CLUSTER COST ANALYSIS - SETUP</h2>\")\n",
    "\n",
    "# Create days_back widget with default to 30 days\n",
    "dbutils.widgets.text(\"days_back\", \"30\", \"Days to Go Back\")\n",
    "\n",
    "# Get widget value and calculate start_date\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<p>üìÖ <b>Analysis Period:</b> {start_date} to current date ({days_back} days)</p>\n",
    "<p>‚öôÔ∏è <b>Creating target schema:</b> ex_dash_temp.billing_forecast</p>\n",
    "\"\"\")\n",
    "\n",
    "# Create schema if not exists\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS ex_dash_temp.billing_forecast\")\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<p>‚úÖ <b>Setup Complete!</b></p>\n",
    "<ul>\n",
    "<li>Days back: {days_back}</li>\n",
    "<li>Calculated start date: {start_date}</li>\n",
    "<li>Target schema ready: ex_dash_temp.billing_forecast</li>\n",
    "</ul>\n",
    "<p>üí° You can change the days_back widget to analyze different periods</p>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6f2f62-10ad-409a-85cd-a662726f3ee8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 1: Create All-Purpose Base Table"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create All-Purpose Base Table\n",
    "# Base table with all-purpose cluster usage and cost data\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>STEP 1: CREATE ALL-PURPOSE BASE TABLE</h2><p>üìÖ Date Range: {start_date} to current ({days_back} days)</p>\")\n",
    "\n",
    "# Check raw cost from billing data\n",
    "raw_cost_check = spark.sql(f\"\"\"\n",
    "SELECT ROUND(SUM(usage_quantity * 0.65), 2) as raw_billing_cost\n",
    "FROM system.billing.usage\n",
    "WHERE usage_date >= '{start_date}'\n",
    "  AND sku_name LIKE '%ALL_PURPOSE%'\n",
    "  AND usage_unit = 'DBU'\n",
    "  AND usage_metadata.cluster_id IS NOT NULL\n",
    "  AND COALESCE(product_features.is_serverless, false) = false\n",
    "\"\"\")\n",
    "raw_cost = raw_cost_check.collect()[0]['raw_billing_cost']\n",
    "\n",
    "# Create base table with cluster metadata\n",
    "base_table_df = spark.sql(f\"\"\"\n",
    "WITH cluster_metadata AS (\n",
    "  SELECT cluster_id, \n",
    "    FIRST(cluster_name) as cluster_name,\n",
    "    FIRST(owned_by) as owned_by,\n",
    "    MAX(auto_termination_minutes) as auto_termination_minutes\n",
    "  FROM system.compute.clusters\n",
    "  GROUP BY cluster_id\n",
    "),\n",
    "cluster_daily_usage AS (\n",
    "  SELECT \n",
    "    u.usage_date,\n",
    "    u.workspace_id,\n",
    "    u.usage_metadata.cluster_id as cluster_id,\n",
    "    SUM(u.usage_quantity) as dbus,\n",
    "    SUM(u.usage_quantity * 0.65) as total_cost_usd,\n",
    "    FIRST(u.usage_metadata.node_type) as node_type,\n",
    "    MAX(COALESCE(u.product_features.is_photon, false)) as is_photon\n",
    "  FROM system.billing.usage u\n",
    "  WHERE u.usage_date >= '{start_date}'\n",
    "    AND u.sku_name LIKE '%ALL_PURPOSE%'\n",
    "    AND u.usage_unit = 'DBU'\n",
    "    AND u.usage_metadata.cluster_id IS NOT NULL\n",
    "    AND COALESCE(u.product_features.is_serverless, false) = false\n",
    "  GROUP BY u.usage_date, u.workspace_id, u.usage_metadata.cluster_id\n",
    ")\n",
    "SELECT \n",
    "  c.usage_date,\n",
    "  c.workspace_id,\n",
    "  COALESCE(w.workspace_name, 'Unknown') as workspace_name,\n",
    "  c.cluster_id,\n",
    "  COALESCE(cm.cluster_name, 'Unknown') as cluster_name,\n",
    "  cm.owned_by as cluster_owner,\n",
    "  c.node_type,\n",
    "  cm.owned_by as principal_email,\n",
    "  CASE WHEN cm.owned_by LIKE '%@%' THEN 'user' ELSE 'service_principal' END as principal_type,\n",
    "  c.dbus,\n",
    "  c.total_cost_usd,\n",
    "  0.65 as list_price_per_dbu,\n",
    "  c.is_photon,\n",
    "  cm.auto_termination_minutes,\n",
    "  nt.core_count,\n",
    "  nt.memory_mb,\n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "FROM cluster_daily_usage c\n",
    "LEFT JOIN prod_sandbox.group_details.workspaces w ON c.workspace_id = w.workspace_id\n",
    "LEFT JOIN cluster_metadata cm ON c.cluster_id = cm.cluster_id\n",
    "LEFT JOIN system.compute.node_types nt ON c.node_type = nt.node_type\n",
    "\"\"\")\n",
    "\n",
    "base_table_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"ex_dash_temp.billing_forecast.all_purpose_base\")\n",
    "\n",
    "displayHTML(\"‚úÖ Base table created: ex_dash_temp.billing_forecast.all_purpose_base\")\n",
    "\n",
    "# Validate table against raw billing cost\n",
    "validation = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as records,\n",
    "  COUNT(DISTINCT cluster_id) as clusters,\n",
    "  COUNT(DISTINCT principal_email) as users,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(SUM(dbus), 2) as total_dbus\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "\"\"\")\n",
    "\n",
    "table_cost = validation.collect()[0]['total_cost_usd']\n",
    "user_count = validation.collect()[0]['users']\n",
    "variance = abs(raw_cost - table_cost)\n",
    "variance_pct = (variance / raw_cost * 100) if raw_cost > 0 else 0\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY & VALIDATION:</h3>\")\n",
    "display(validation)\n",
    "\n",
    "if variance_pct < 1:\n",
    "    displayHTML(f\"<p>‚úÖ <b>Validation Passed:</b> Table cost (${table_cost:,.2f}) matches raw billing cost (${raw_cost:,.2f})<br>Variance: ${variance:,.2f} ({variance_pct:.2f}%)</p>\")\n",
    "else:\n",
    "    displayHTML(f\"<p>‚ö†Ô∏è <b>Validation Warning:</b> Table cost (${table_cost:,.2f}) differs from raw billing cost (${raw_cost:,.2f})<br>Variance: ${variance:,.2f} ({variance_pct:.2f}%)</p>\")\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.all_purpose_base ORDER BY total_cost_usd DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a8d55c-3ae5-4c2a-a2a3-b221e989d698",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 2: Create Per User Daily Telemetry Table"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Create Per User Daily Telemetry Table\n",
    "# Includes actual CPU, Memory, and Network metrics from system.compute.node_timeline\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<h2>STEP 2: CREATE PER USER DAILY TELEMETRY TABLE</h2>\n",
    "<p>üë§ Creating user-level daily analysis with telemetry</p>\n",
    "<ul>\n",
    "<li>CPU utilization (user + system)</li>\n",
    "<li>Memory utilization</li>\n",
    "<li>Network I/O (sent + received)</li>\n",
    "<li>Daily cost per user</li>\n",
    "</ul>\n",
    "\"\"\")\n",
    "\n",
    "# Create per user daily telemetry table\n",
    "user_daily_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.user_daily_telemetry\n",
    "USING DELTA\n",
    "AS\n",
    "WITH telemetry_aggregated AS (\n",
    "  SELECT \n",
    "    b.principal_email,\n",
    "    b.usage_date,\n",
    "    b.workspace_name,\n",
    "    \n",
    "    -- Telemetry from node_timeline\n",
    "    ROUND(AVG(nt.cpu_user_percent + nt.cpu_system_percent), 2) as avg_cpu_pct,\n",
    "    ROUND(MAX(nt.cpu_user_percent + nt.cpu_system_percent), 2) as max_cpu_pct,\n",
    "    ROUND(AVG(nt.mem_used_percent), 2) as avg_mem_pct,\n",
    "    ROUND(MAX(nt.mem_used_percent), 2) as max_mem_pct,\n",
    "    ROUND(SUM(nt.network_sent_bytes + nt.network_received_bytes) / 1024 / 1024 / 1024, 2) as total_network_gb,\n",
    "    ROUND(AVG((nt.network_sent_bytes + nt.network_received_bytes) / 1024 / 1024), 2) as avg_network_mb,\n",
    "    COUNT(DISTINCT nt.cluster_id) as clusters_with_telemetry\n",
    "    \n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "  INNER JOIN system.compute.node_timeline nt \n",
    "    ON b.cluster_id = nt.cluster_id \n",
    "    AND DATE(nt.start_time) = b.usage_date\n",
    "  WHERE b.usage_date >= '{start_date}'\n",
    "  GROUP BY b.principal_email, b.usage_date, b.workspace_name\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  b.usage_date,\n",
    "  b.workspace_id,\n",
    "  b.workspace_name,\n",
    "  b.principal_email,\n",
    "  b.principal_type,\n",
    "  \n",
    "  -- Cost metrics\n",
    "  SUM(b.dbus) as total_dbus,\n",
    "  SUM(b.total_cost_usd) as total_cost_usd,\n",
    "  AVG(b.list_price_per_dbu) as avg_price_per_dbu,\n",
    "  COUNT(DISTINCT b.cluster_id) as clusters_used,\n",
    "  COUNT(DISTINCT b.node_type) as instance_types_used,\n",
    "  \n",
    "  -- Configuration metrics\n",
    "  AVG(CASE WHEN b.is_photon THEN 1.0 ELSE 0.0 END) as photon_usage_rate,\n",
    "  AVG(b.auto_termination_minutes) as avg_autoterm_minutes,\n",
    "  AVG(b.core_count) as avg_cores,\n",
    "  AVG(b.memory_mb) as avg_memory_mb,\n",
    "  \n",
    "  -- Telemetry metrics\n",
    "  t.avg_cpu_pct,\n",
    "  t.max_cpu_pct,\n",
    "  t.avg_mem_pct,\n",
    "  t.max_mem_pct,\n",
    "  t.total_network_gb,\n",
    "  t.avg_network_mb,\n",
    "  COALESCE(t.clusters_with_telemetry, 0) as clusters_with_telemetry,\n",
    "  \n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "  \n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "LEFT JOIN telemetry_aggregated t \n",
    "  ON b.principal_email = t.principal_email \n",
    "  AND b.usage_date = t.usage_date\n",
    "  AND b.workspace_name = t.workspace_name\n",
    "WHERE b.usage_date >= '{start_date}'\n",
    "GROUP BY \n",
    "  b.usage_date, b.workspace_id, b.workspace_name, b.principal_email, b.principal_type,\n",
    "  t.avg_cpu_pct, t.max_cpu_pct, t.avg_mem_pct, t.max_mem_pct, \n",
    "  t.total_network_gb, t.avg_network_mb, t.clusters_with_telemetry\n",
    "ORDER BY b.usage_date DESC, total_cost_usd DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(user_daily_query)\n",
    "\n",
    "displayHTML(\"‚úÖ User daily telemetry table created: ex_dash_temp.billing_forecast.user_daily_telemetry\")\n",
    "\n",
    "# Show summary\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as total_user_days,\n",
    "  COUNT(DISTINCT principal_email) as unique_users,\n",
    "  COUNT(DISTINCT usage_date) as days,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost_usd,\n",
    "  COUNT(CASE WHEN avg_cpu_pct IS NOT NULL THEN 1 END) as days_with_telemetry,\n",
    "  ROUND(AVG(CASE WHEN avg_cpu_pct IS NOT NULL THEN avg_cpu_pct END), 2) as avg_cpu_utilization,\n",
    "  ROUND(AVG(CASE WHEN avg_mem_pct IS NOT NULL THEN avg_mem_pct END), 2) as avg_memory_utilization\n",
    "FROM ex_dash_temp.billing_forecast.user_daily_telemetry\n",
    "WHERE usage_date >= '{start_date}'\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.user_daily_telemetry ORDER BY total_cost_usd DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c02ee9e-bdfd-4531-be44-241437503bc3",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1763445611793}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 1
      }
     },
     "title": "Step 3: Create Per Cluster Daily Telemetry Table"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Create Per Cluster Daily Telemetry Table\n",
    "# Cluster-level daily analysis with telemetry metrics\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>STEP 3: CREATE PER CLUSTER DAILY TELEMETRY TABLE</h2><p>üíª Creating cluster-level daily analysis</p>\")\n",
    "\n",
    "# Create per cluster daily telemetry\n",
    "cluster_daily_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.cluster_daily_telemetry\n",
    "USING DELTA\n",
    "AS\n",
    "WITH telemetry_by_cluster AS (\n",
    "  SELECT \n",
    "    cluster_id,\n",
    "    DATE(start_time) as telemetry_date,\n",
    "    ROUND(AVG(cpu_user_percent + cpu_system_percent), 2) as avg_cpu_pct,\n",
    "    ROUND(MAX(cpu_user_percent + cpu_system_percent), 2) as max_cpu_pct,\n",
    "    ROUND(AVG(mem_used_percent), 2) as avg_mem_pct,\n",
    "    ROUND(MAX(mem_used_percent), 2) as max_mem_pct,\n",
    "    ROUND(SUM(network_sent_bytes + network_received_bytes) / 1024 / 1024 / 1024, 2) as total_network_gb,\n",
    "    ROUND(AVG((network_sent_bytes + network_received_bytes) / 1024 / 1024), 2) as avg_network_mb\n",
    "  FROM system.compute.node_timeline\n",
    "  WHERE DATE(start_time) >= '{start_date}'\n",
    "  GROUP BY cluster_id, DATE(start_time)\n",
    ")\n",
    "SELECT \n",
    "  b.usage_date,\n",
    "  b.workspace_id,\n",
    "  b.workspace_name,\n",
    "  b.cluster_id,\n",
    "  b.cluster_name,\n",
    "  b.cluster_owner,\n",
    "  b.node_type as instance_type,\n",
    "  b.principal_type,\n",
    "  b.dbus as total_dbus,\n",
    "  b.total_cost_usd,\n",
    "  b.list_price_per_dbu as avg_price_per_dbu,\n",
    "  b.is_photon as photon_enabled,\n",
    "  b.auto_termination_minutes as autoterm_minutes,\n",
    "  b.core_count,\n",
    "  b.memory_mb,\n",
    "  t.avg_cpu_pct,\n",
    "  t.max_cpu_pct,\n",
    "  t.avg_mem_pct,\n",
    "  t.max_mem_pct,\n",
    "  t.total_network_gb,\n",
    "  t.avg_network_mb,\n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "LEFT JOIN telemetry_by_cluster t \n",
    "  ON b.cluster_id = t.cluster_id \n",
    "  AND b.usage_date = t.telemetry_date\n",
    "WHERE b.usage_date >= '{start_date}'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(cluster_daily_query)\n",
    "\n",
    "displayHTML(\"‚úÖ Cluster daily telemetry table created: ex_dash_temp.billing_forecast.cluster_daily_telemetry\")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as records,\n",
    "  COUNT(DISTINCT cluster_id) as clusters,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost,\n",
    "  ROUND(AVG(avg_cpu_pct), 2) as avg_cpu,\n",
    "  ROUND(AVG(avg_mem_pct), 2) as avg_mem\n",
    "FROM ex_dash_temp.billing_forecast.cluster_daily_telemetry\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.cluster_daily_telemetry ORDER BY total_cost_usd DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4118411c-10f0-4c97-a9ea-1ac8de6a7cc3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Create Per Instance Daily Telemetry Table"
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Create Per Instance Daily Telemetry Table\n",
    "# Instance-level daily analysis with telemetry metrics\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(\n",
    "    f\"<h2>STEP 4: CREATE PER INSTANCE DAILY TELEMETRY TABLE</h2>\"\n",
    "    f\"<p>üñ•Ô∏è Creating instance-level daily analysis</p>\"\n",
    ")\n",
    "\n",
    "# Create per instance daily telemetry\n",
    "instance_daily_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.instance_daily_telemetry\n",
    "USING DELTA\n",
    "AS\n",
    "WITH telemetry_by_instance AS (\n",
    "  SELECT \n",
    "    b.node_type,\n",
    "    b.usage_date,\n",
    "    ROUND(AVG(nt.cpu_user_percent + nt.cpu_system_percent), 2) as avg_cpu_pct,\n",
    "    ROUND(MAX(nt.cpu_user_percent + nt.cpu_system_percent), 2) as max_cpu_pct,\n",
    "    ROUND(AVG(nt.mem_used_percent), 2) as avg_mem_pct,\n",
    "    ROUND(MAX(nt.mem_used_percent), 2) as max_mem_pct,\n",
    "    ROUND(\n",
    "      SUM(nt.network_sent_bytes + nt.network_received_bytes) / 1024 / 1024 / 1024, \n",
    "      2\n",
    "    ) as total_network_gb,\n",
    "    ROUND(\n",
    "      AVG((nt.network_sent_bytes + nt.network_received_bytes) / 1024 / 1024), \n",
    "      2\n",
    "    ) as avg_network_mb\n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "  INNER JOIN system.compute.node_timeline nt \n",
    "    ON b.cluster_id = nt.cluster_id \n",
    "    AND DATE(nt.start_time) = b.usage_date\n",
    "  WHERE b.usage_date >= '{start_date}'\n",
    "  GROUP BY b.node_type, b.usage_date\n",
    ")\n",
    "SELECT \n",
    "  b.usage_date,\n",
    "  b.node_type as instance_type,\n",
    "  SUM(b.dbus) as total_dbus,\n",
    "  SUM(b.total_cost_usd) as total_cost_usd,\n",
    "  AVG(b.list_price_per_dbu) as avg_price_per_dbu,\n",
    "  COUNT(DISTINCT b.cluster_id) as clusters_using,\n",
    "  COUNT(DISTINCT b.principal_email) as users_using,\n",
    "  COUNT(DISTINCT b.workspace_name) as workspaces_using,\n",
    "  AVG(CASE WHEN b.is_photon THEN 1.0 ELSE 0.0 END) as photon_usage_rate,\n",
    "  AVG(b.auto_termination_minutes) as avg_autoterm_minutes,\n",
    "  MAX(b.core_count) as core_count,\n",
    "  MAX(b.memory_mb) as memory_mb,\n",
    "  t.avg_cpu_pct,\n",
    "  t.max_cpu_pct,\n",
    "  t.avg_mem_pct,\n",
    "  t.max_mem_pct,\n",
    "  t.total_network_gb,\n",
    "  t.avg_network_mb,\n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "LEFT JOIN telemetry_by_instance t \n",
    "  ON b.node_type = t.node_type \n",
    "  AND b.usage_date = t.usage_date\n",
    "WHERE b.usage_date >= '{start_date}'\n",
    "GROUP BY \n",
    "  b.usage_date, \n",
    "  b.node_type, \n",
    "  t.avg_cpu_pct, \n",
    "  t.max_cpu_pct, \n",
    "  t.avg_mem_pct, \n",
    "  t.max_mem_pct, \n",
    "  t.total_network_gb, \n",
    "  t.avg_network_mb\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(instance_daily_query)\n",
    "\n",
    "displayHTML(\n",
    "    \"‚úÖ Instance daily telemetry table created: \"\n",
    "    \"ex_dash_temp.billing_forecast.instance_daily_telemetry\"\n",
    ")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as records,\n",
    "  COUNT(DISTINCT instance_type) as instance_types,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost\n",
    "FROM ex_dash_temp.billing_forecast.instance_daily_telemetry\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\n",
    "    \"SELECT * FROM ex_dash_temp.billing_forecast.instance_daily_telemetry \"\n",
    "    \"ORDER BY total_cost_usd DESC LIMIT 50\"\n",
    ")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c250aa1-a460-44ae-a7ef-398f7a19fd04",
     "showTitle": true,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762563616626}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": "Step 5: Create Per User Total Cost Table (One Row Per User)"
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Create Per User Total Cost Table\n",
    "# One row per user with aggregated costs and average telemetry\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<h2>STEP 5: CREATE PER USER TOTAL COST TABLE</h2>\n",
    "<p>üë§ Creating user-level total cost analysis (one row per user)</p>\n",
    "\"\"\")\n",
    "\n",
    "# Create per user total cost table\n",
    "user_total_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.user_total_cost\n",
    "USING DELTA\n",
    "AS\n",
    "SELECT \n",
    "  principal_email,\n",
    "  principal_type,\n",
    "  \n",
    "  -- Primary workspace (most used)\n",
    "  FIRST(workspace_name) as primary_workspace,\n",
    "  COUNT(DISTINCT workspace_name) as workspaces_used,\n",
    "  \n",
    "  -- Cost metrics\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(SUM(total_dbus), 2) as total_dbus,\n",
    "  ROUND(AVG(avg_price_per_dbu), 2) as avg_price_per_dbu,\n",
    "  \n",
    "  -- Usage metrics\n",
    "  COUNT(DISTINCT usage_date) as days_active,\n",
    "  SUM(clusters_used) as total_cluster_days,\n",
    "  COUNT(DISTINCT clusters_used) as unique_clusters,\n",
    "  SUM(instance_types_used) as total_instance_type_days,\n",
    "  \n",
    "  -- Configuration metrics\n",
    "  ROUND(AVG(photon_usage_rate) * 100, 1) as photon_usage_pct,\n",
    "  ROUND(AVG(avg_autoterm_minutes), 0) as avg_autoterm_minutes,\n",
    "  ROUND(AVG(avg_cores), 1) as avg_cores,\n",
    "  ROUND(AVG(avg_memory_mb) / 1024, 1) as avg_memory_gb,\n",
    "  \n",
    "  -- Telemetry averages\n",
    "  ROUND(AVG(CASE WHEN avg_cpu_pct IS NOT NULL THEN avg_cpu_pct END), 2) as avg_cpu_pct,\n",
    "  ROUND(MAX(CASE WHEN max_cpu_pct IS NOT NULL THEN max_cpu_pct END), 2) as max_cpu_pct,\n",
    "  ROUND(AVG(CASE WHEN avg_mem_pct IS NOT NULL THEN avg_mem_pct END), 2) as avg_mem_pct,\n",
    "  ROUND(MAX(CASE WHEN max_mem_pct IS NOT NULL THEN max_mem_pct END), 2) as max_mem_pct,\n",
    "  ROUND(SUM(CASE WHEN total_network_gb IS NOT NULL THEN total_network_gb ELSE 0 END), 2) as total_network_gb,\n",
    "  ROUND(AVG(CASE WHEN avg_network_mb IS NOT NULL THEN avg_network_mb END), 2) as avg_network_mb,\n",
    "  \n",
    "  -- Telemetry coverage\n",
    "  COUNT(CASE WHEN avg_cpu_pct IS NOT NULL THEN 1 END) as days_with_telemetry,\n",
    "  ROUND(COUNT(CASE WHEN avg_cpu_pct IS NOT NULL THEN 1 END) * 100.0 / COUNT(*), 1) as telemetry_coverage_pct,\n",
    "  \n",
    "  MIN(usage_date) as first_usage_date,\n",
    "  MAX(usage_date) as last_usage_date,\n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "  \n",
    "FROM ex_dash_temp.billing_forecast.user_daily_telemetry\n",
    "WHERE usage_date >= '{start_date}'\n",
    "GROUP BY principal_email, principal_type\n",
    "ORDER BY total_cost_usd DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(user_total_query)\n",
    "\n",
    "displayHTML(\"‚úÖ User total cost table created: ex_dash_temp.billing_forecast.user_total_cost\")\n",
    "\n",
    "# Validate against base table\n",
    "validation = spark.sql(f\"\"\"\n",
    "WITH base_total AS (\n",
    "  SELECT ROUND(SUM(total_cost_usd), 2) as base_cost\n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "  WHERE usage_date >= '{start_date}'\n",
    "),\n",
    "user_total AS (\n",
    "  SELECT ROUND(SUM(total_cost_usd), 2) as user_cost, COUNT(*) as user_count\n",
    "  FROM ex_dash_temp.billing_forecast.user_total_cost\n",
    ")\n",
    "SELECT \n",
    "  b.base_cost,\n",
    "  u.user_cost,\n",
    "  u.user_count,\n",
    "  ROUND(b.base_cost - COALESCE(u.user_cost, 0), 2) as difference,\n",
    "  ROUND(ABS(b.base_cost - COALESCE(u.user_cost, 0)) / NULLIF(b.base_cost, 0) * 100, 2) as variance_pct\n",
    "FROM base_total b, user_total u\n",
    "\"\"\")\n",
    "\n",
    "val_data = validation.collect()[0]\n",
    "variance_pct = val_data['variance_pct'] or 0\n",
    "\n",
    "displayHTML(\"<h3>üîç COST VALIDATION:</h3>\")\n",
    "display(validation)\n",
    "\n",
    "if variance_pct < 1:\n",
    "    displayHTML(f\"<p>‚úÖ <b>Validation Passed:</b> User aggregated cost matches base table cost (Variance: {variance_pct:.2f}%)</p>\")\n",
    "else:\n",
    "    displayHTML(f\"<p>‚ö†Ô∏è <b>Validation Warning:</b> User aggregated cost differs from base table (Variance: {variance_pct:.2f}%)</p>\")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as total_users,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(AVG(total_cost_usd), 2) as avg_cost_per_user,\n",
    "  ROUND(AVG(days_active), 1) as avg_days_active,\n",
    "  ROUND(AVG(avg_cpu_pct), 2) as avg_cpu_utilization,\n",
    "  ROUND(AVG(avg_mem_pct), 2) as avg_memory_utilization,\n",
    "  ROUND(AVG(telemetry_coverage_pct), 1) as avg_telemetry_coverage\n",
    "FROM ex_dash_temp.billing_forecast.user_total_cost\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.user_total_cost ORDER BY total_cost_usd DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec414a8-4d26-4952-95ac-da178cafdb8d",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762563671870}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "Step 6: Create Per Cluster Total Cost Table (One Row Per Cluster)"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Create Per Cluster Total Cost Table\n",
    "# One row per cluster with aggregated costs and telemetry\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>STEP 6: CREATE PER CLUSTER TOTAL COST TABLE</h2><p>üíª Creating cluster-level total cost (one row per cluster)</p>\")\n",
    "\n",
    "# Create per cluster total cost\n",
    "cluster_total_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.cluster_total_cost\n",
    "USING DELTA\n",
    "AS\n",
    "WITH cluster_telemetry_avg AS (\n",
    "  SELECT \n",
    "    cluster_id,\n",
    "    ROUND(AVG(avg_cpu_pct), 2) as avg_cpu_pct,\n",
    "    ROUND(MAX(max_cpu_pct), 2) as max_cpu_pct,\n",
    "    ROUND(AVG(avg_mem_pct), 2) as avg_mem_pct,\n",
    "    ROUND(MAX(max_mem_pct), 2) as max_mem_pct,\n",
    "    ROUND(SUM(total_network_gb), 2) as total_network_gb,\n",
    "    ROUND(AVG(avg_network_mb), 2) as avg_network_mb\n",
    "  FROM ex_dash_temp.billing_forecast.cluster_daily_telemetry\n",
    "  WHERE avg_cpu_pct IS NOT NULL\n",
    "  GROUP BY cluster_id\n",
    "),\n",
    "cluster_config AS (\n",
    "  SELECT \n",
    "    cluster_id,\n",
    "    FIRST(driver_node_type) as driver_instance_type,\n",
    "    FIRST(worker_node_type) as worker_instance_type,\n",
    "    FIRST(worker_count) as worker_count,\n",
    "    FIRST(min_autoscale_workers) as min_workers,\n",
    "    FIRST(max_autoscale_workers) as max_workers\n",
    "  FROM system.compute.clusters\n",
    "  WHERE change_time >= '{start_date}'\n",
    "  GROUP BY cluster_id\n",
    ")\n",
    "SELECT \n",
    "  b.cluster_id,\n",
    "  FIRST(b.cluster_name) as cluster_name,\n",
    "  FIRST(b.cluster_owner) as cluster_owner,\n",
    "  FIRST(b.workspace_name) as workspace_name,\n",
    "  FIRST(b.node_type) as primary_instance_type,\n",
    "  COALESCE(cc.driver_instance_type, FIRST(b.node_type)) as driver_instance_type,\n",
    "  COALESCE(cc.worker_instance_type, FIRST(b.node_type)) as worker_instance_type,\n",
    "  cc.worker_count,\n",
    "  cc.min_workers,\n",
    "  cc.max_workers,\n",
    "  ROUND(SUM(b.total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(SUM(b.dbus), 2) as total_dbus,\n",
    "  COUNT(DISTINCT b.usage_date) as days_active,\n",
    "  t.avg_cpu_pct,\n",
    "  t.max_cpu_pct,\n",
    "  t.avg_mem_pct,\n",
    "  t.max_mem_pct,\n",
    "  t.total_network_gb,\n",
    "  t.avg_network_mb,\n",
    "  ROUND(t.avg_cpu_pct / NULLIF(MAX(b.core_count) * 100, 0) * 100, 1) as cpu_efficiency_pct,\n",
    "  ROUND(t.avg_mem_pct, 1) as memory_efficiency_pct,\n",
    "  MAX(b.core_count) as core_count,\n",
    "  ROUND(MAX(b.memory_mb) / 1024, 1) as memory_gb,\n",
    "  MAX(b.is_photon) as photon_enabled,\n",
    "  MAX(b.auto_termination_minutes) as autoterm_minutes,\n",
    "  COUNT(CASE WHEN t.avg_cpu_pct IS NOT NULL THEN 1 END) * 100.0 / COUNT(*) as telemetry_coverage_pct,\n",
    "  MIN(b.usage_date) as first_usage_date,\n",
    "  MAX(b.usage_date) as last_usage_date,\n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "LEFT JOIN cluster_telemetry_avg t ON b.cluster_id = t.cluster_id\n",
    "LEFT JOIN cluster_config cc ON b.cluster_id = cc.cluster_id\n",
    "WHERE b.usage_date >= '{start_date}'\n",
    "GROUP BY b.cluster_id, t.avg_cpu_pct, t.max_cpu_pct, t.avg_mem_pct, t.max_mem_pct, t.total_network_gb, t.avg_network_mb,\n",
    "         cc.driver_instance_type, cc.worker_instance_type, cc.worker_count, cc.min_workers, cc.max_workers\n",
    "ORDER BY total_cost_usd DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(cluster_total_query)\n",
    "\n",
    "displayHTML(\"‚úÖ Cluster total cost table created: ex_dash_temp.billing_forecast.cluster_total_cost\")\n",
    "\n",
    "# Validate against base table\n",
    "validation = spark.sql(f\"\"\"\n",
    "WITH base_total AS (\n",
    "  SELECT ROUND(SUM(total_cost_usd), 2) as base_cost\n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "  WHERE usage_date >= '{start_date}'\n",
    "),\n",
    "cluster_total AS (\n",
    "  SELECT ROUND(SUM(total_cost_usd), 2) as cluster_cost, COUNT(*) as cluster_count\n",
    "  FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    ")\n",
    "SELECT \n",
    "  b.base_cost,\n",
    "  c.cluster_cost,\n",
    "  c.cluster_count,\n",
    "  ROUND(b.base_cost - c.cluster_cost, 2) as difference,\n",
    "  ROUND(ABS(b.base_cost - c.cluster_cost) / NULLIF(b.base_cost, 0) * 100, 2) as variance_pct\n",
    "FROM base_total b, cluster_total c\n",
    "\"\"\")\n",
    "\n",
    "val_data = validation.collect()[0]\n",
    "variance_pct = val_data['variance_pct'] or 0\n",
    "\n",
    "displayHTML(\"<h3>üîç COST VALIDATION:</h3>\")\n",
    "display(validation)\n",
    "\n",
    "if variance_pct < 1:\n",
    "    displayHTML(f\"<p>‚úÖ <b>Validation Passed:</b> Cluster aggregated cost matches base table cost (Variance: {variance_pct:.2f}%)</p>\")\n",
    "else:\n",
    "    displayHTML(f\"<p>‚ö†Ô∏è <b>Validation Warning:</b> Cluster aggregated cost differs from base table (Variance: {variance_pct:.2f}%)</p>\")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as total_clusters,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(AVG(total_cost_usd), 2) as avg_cost_per_cluster,\n",
    "  ROUND(AVG(days_active), 1) as avg_days_active,\n",
    "  ROUND(AVG(cpu_efficiency_pct), 1) as avg_cpu_efficiency,\n",
    "  ROUND(AVG(memory_efficiency_pct), 1) as avg_memory_efficiency\n",
    "FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT cluster_id, cluster_name, driver_instance_type, worker_instance_type, worker_count, min_workers, max_workers, total_cost_usd, cpu_efficiency_pct, memory_efficiency_pct FROM ex_dash_temp.billing_forecast.cluster_total_cost ORDER BY total_cost_usd DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4607156b-7fd9-4804-8aab-7b7173022ef7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 6b: Add Driver and Worker Instance Types"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6b: Enrich Cluster Table with Driver and Worker Instance Types\n",
    "# Adds specific driver and worker instance information from system.compute.clusters\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(\"<h2>STEP 6B: ENRICH CLUSTER TABLE WITH DRIVER/WORKER INSTANCE TYPES</h2><p>üîß Adding driver and worker instance type columns</p>\")\n",
    "\n",
    "# First, add the columns if they don't exist\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "    ALTER TABLE ex_dash_temp.billing_forecast.cluster_total_cost \n",
    "    ADD COLUMNS (\n",
    "        driver_instance_type STRING COMMENT 'Driver node instance type',\n",
    "        worker_instance_type STRING COMMENT 'Worker node instance type',\n",
    "        worker_count BIGINT COMMENT 'Fixed worker count',\n",
    "        min_workers BIGINT COMMENT 'Min autoscale workers',\n",
    "        max_workers BIGINT COMMENT 'Max autoscale workers'\n",
    "    )\n",
    "    \"\"\")\n",
    "    displayHTML(\"<p>‚úÖ Columns added successfully</p>\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower() or \"duplicate\" in str(e).lower():\n",
    "        displayHTML(\"<p>‚ÑπÔ∏è Columns already exist, proceeding to update</p>\")\n",
    "    else:\n",
    "        displayHTML(f\"<p>‚ö†Ô∏è Error adding columns: {str(e)}</p>\")\n",
    "\n",
    "# Populate the columns from system.compute.clusters (get latest configuration)\n",
    "update_query = f\"\"\"\n",
    "MERGE INTO ex_dash_temp.billing_forecast.cluster_total_cost AS target\n",
    "USING (\n",
    "    WITH ranked_configs AS (\n",
    "        SELECT \n",
    "            cluster_id,\n",
    "            driver_node_type,\n",
    "            worker_node_type,\n",
    "            worker_count,\n",
    "            min_autoscale_workers,\n",
    "            max_autoscale_workers,\n",
    "            change_time,\n",
    "            ROW_NUMBER() OVER (PARTITION BY cluster_id ORDER BY change_time DESC) as rn\n",
    "        FROM system.compute.clusters\n",
    "        WHERE driver_node_type IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        cluster_id,\n",
    "        driver_node_type as driver_instance_type,\n",
    "        worker_node_type as worker_instance_type,\n",
    "        worker_count,\n",
    "        min_autoscale_workers as min_workers,\n",
    "        max_autoscale_workers as max_workers\n",
    "    FROM ranked_configs\n",
    "    WHERE rn = 1\n",
    ") AS source\n",
    "ON target.cluster_id = source.cluster_id\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.driver_instance_type = source.driver_instance_type,\n",
    "    target.worker_instance_type = source.worker_instance_type,\n",
    "    target.worker_count = source.worker_count,\n",
    "    target.min_workers = source.min_workers,\n",
    "    target.max_workers = source.max_workers\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(update_query)\n",
    "\n",
    "displayHTML(\"‚úÖ Driver and worker instance types populated from latest cluster configurations\")\n",
    "\n",
    "# Verify the update\n",
    "verification = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(*) as total_clusters,\n",
    "    COUNT(driver_instance_type) as clusters_with_driver_type,\n",
    "    COUNT(worker_instance_type) as clusters_with_worker_type,\n",
    "    COUNT(CASE WHEN driver_instance_type != worker_instance_type THEN 1 END) as clusters_with_different_types,\n",
    "    COUNT(worker_count) as clusters_with_fixed_workers,\n",
    "    COUNT(CASE WHEN min_workers IS NOT NULL AND max_workers IS NOT NULL THEN 1 END) as clusters_with_autoscale\n",
    "FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1a0a07-2cab-4bc2-91de-939ac314663a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 7: Create Per Instance Total Cost Table (One Row Per Instance)"
    }
   },
   "outputs": [],
   "source": [
    "# Step 7: Create Per Instance Total Cost Table\n",
    "# One row per instance type with aggregated costs and telemetry\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>STEP 7: CREATE PER INSTANCE TOTAL COST TABLE</h2><p>üñ•Ô∏è Creating instance-level total cost (one row per instance)</p>\")\n",
    "\n",
    "# Create per instance total cost\n",
    "instance_total_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.instance_total_cost\n",
    "USING DELTA\n",
    "AS\n",
    "WITH instance_telemetry_avg AS (\n",
    "  SELECT \n",
    "    instance_type,\n",
    "    ROUND(AVG(avg_cpu_pct), 2) as avg_cpu_pct,\n",
    "    ROUND(MAX(max_cpu_pct), 2) as max_cpu_pct,\n",
    "    ROUND(AVG(avg_mem_pct), 2) as avg_mem_pct,\n",
    "    ROUND(MAX(max_mem_pct), 2) as max_mem_pct,\n",
    "    ROUND(SUM(total_network_gb), 2) as total_network_gb,\n",
    "    ROUND(AVG(avg_network_mb), 2) as avg_network_mb\n",
    "  FROM ex_dash_temp.billing_forecast.instance_daily_telemetry\n",
    "  WHERE avg_cpu_pct IS NOT NULL\n",
    "  GROUP BY instance_type\n",
    ")\n",
    "SELECT \n",
    "  b.node_type as instance_type,\n",
    "  ROUND(SUM(b.total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(SUM(b.dbus), 2) as total_dbus,\n",
    "  COUNT(DISTINCT b.cluster_id) as unique_clusters,\n",
    "  COUNT(DISTINCT b.principal_email) as unique_users,\n",
    "  COUNT(DISTINCT b.workspace_name) as unique_workspaces,\n",
    "  COUNT(DISTINCT b.usage_date) as days_active,\n",
    "  t.avg_cpu_pct,\n",
    "  t.max_cpu_pct,\n",
    "  t.avg_mem_pct,\n",
    "  t.max_mem_pct,\n",
    "  t.total_network_gb,\n",
    "  t.avg_network_mb,\n",
    "  ROUND(t.avg_cpu_pct / NULLIF(MAX(b.core_count) * 100, 0) * 100, 1) as cpu_efficiency_pct,\n",
    "  ROUND(t.avg_mem_pct, 1) as memory_efficiency_pct,\n",
    "  MAX(b.core_count) as core_count,\n",
    "  ROUND(MAX(b.memory_mb) / 1024, 1) as memory_gb,\n",
    "  AVG(CASE WHEN b.is_photon THEN 100.0 ELSE 0.0 END) as photon_usage_pct,\n",
    "  AVG(b.auto_termination_minutes) as avg_autoterm_minutes,\n",
    "  100.0 as telemetry_coverage_pct,\n",
    "  MIN(b.usage_date) as first_usage_date,\n",
    "  MAX(b.usage_date) as last_usage_date,\n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base b\n",
    "LEFT JOIN instance_telemetry_avg t ON b.node_type = t.instance_type\n",
    "WHERE b.usage_date >= '{start_date}'\n",
    "GROUP BY b.node_type, t.avg_cpu_pct, t.max_cpu_pct, t.avg_mem_pct, t.max_mem_pct, t.total_network_gb, t.avg_network_mb\n",
    "ORDER BY total_cost_usd DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(instance_total_query)\n",
    "\n",
    "displayHTML(\"‚úÖ Instance total cost table created: ex_dash_temp.billing_forecast.instance_total_cost\")\n",
    "\n",
    "# Validate against base table\n",
    "validation = spark.sql(f\"\"\"\n",
    "WITH base_total AS (\n",
    "  SELECT ROUND(SUM(total_cost_usd), 2) as base_cost\n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "  WHERE usage_date >= '{start_date}'\n",
    "),\n",
    "instance_total AS (\n",
    "  SELECT ROUND(SUM(total_cost_usd), 2) as instance_cost, COUNT(*) as instance_count\n",
    "  FROM ex_dash_temp.billing_forecast.instance_total_cost\n",
    ")\n",
    "SELECT \n",
    "  b.base_cost,\n",
    "  i.instance_cost,\n",
    "  i.instance_count,\n",
    "  ROUND(b.base_cost - i.instance_cost, 2) as difference,\n",
    "  ROUND(ABS(b.base_cost - i.instance_cost) / NULLIF(b.base_cost, 0) * 100, 2) as variance_pct\n",
    "FROM base_total b, instance_total i\n",
    "\"\"\")\n",
    "\n",
    "val_data = validation.collect()[0]\n",
    "variance_pct = val_data['variance_pct'] or 0\n",
    "\n",
    "displayHTML(\"<h3>üîç COST VALIDATION:</h3>\")\n",
    "display(validation)\n",
    "\n",
    "if variance_pct < 1:\n",
    "    displayHTML(f\"<p>‚úÖ <b>Validation Passed:</b> Instance aggregated cost matches base table cost (Variance: {variance_pct:.2f}%)</p>\")\n",
    "else:\n",
    "    displayHTML(f\"<p>‚ö†Ô∏è <b>Validation Warning:</b> Instance aggregated cost differs from base table (Variance: {variance_pct:.2f}%)</p>\")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) as total_instance_types,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost_usd,\n",
    "  ROUND(AVG(total_cost_usd), 2) as avg_cost_per_instance,\n",
    "  ROUND(AVG(cpu_efficiency_pct), 1) as avg_cpu_efficiency,\n",
    "  ROUND(AVG(memory_efficiency_pct), 1) as avg_memory_efficiency\n",
    "FROM ex_dash_temp.billing_forecast.instance_total_cost\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.instance_total_cost ORDER BY total_cost_usd DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e6ab6d-a3c4-4c29-a195-cc9f02b3c4b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 8: Per User Opportunity Savings and Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "# Step 8: Create Per User Opportunity Recommendations\n",
    "# Identifies cost optimization opportunities for each user\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(\"<h2>STEP 8: CREATE PER USER OPPORTUNITY RECOMMENDATIONS</h2><p>üéØ Creating user-level cost optimization opportunities</p>\")\n",
    "\n",
    "# Create per user opportunities table\n",
    "user_opportunities_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.user_opportunities\n",
    "USING DELTA\n",
    "AS\n",
    "WITH base_cost AS (\n",
    "  SELECT SUM(total_cost_usd) as total_all_purpose_cost\n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "  WHERE usage_date >= '{start_date}'\n",
    ")\n",
    "SELECT \n",
    "  u.principal_email,\n",
    "  u.principal_type,\n",
    "  u.primary_workspace,\n",
    "  u.total_cost_usd,\n",
    "  u.days_active,\n",
    "  u.unique_clusters,\n",
    "  u.avg_cpu_pct,\n",
    "  u.avg_mem_pct,\n",
    "  u.avg_network_mb,\n",
    "  u.total_network_gb,\n",
    "  u.avg_cores,\n",
    "  u.avg_memory_gb,\n",
    "  u.photon_usage_pct,\n",
    "  u.avg_autoterm_minutes,\n",
    "  u.telemetry_coverage_pct,\n",
    "  \n",
    "  -- Opportunity identification\n",
    "  CASE \n",
    "    WHEN u.avg_cpu_pct < 20 AND u.avg_mem_pct < 30 THEN 'CRITICAL'\n",
    "    WHEN u.avg_cpu_pct < 30 OR u.avg_mem_pct < 40 THEN 'HIGH'\n",
    "    WHEN u.avg_autoterm_minutes > 60 OR u.avg_autoterm_minutes IS NULL THEN 'MEDIUM'\n",
    "    WHEN u.photon_usage_pct < 50 THEN 'LOW'\n",
    "    ELSE 'OPTIMAL'\n",
    "  END as opportunity_priority,\n",
    "  \n",
    "  -- Detailed recommendations\n",
    "  CASE \n",
    "    WHEN u.avg_cpu_pct < 20 AND u.avg_mem_pct < 30 \n",
    "      THEN CONCAT('CRITICAL: Severe under-utilization (CPU: ', ROUND(u.avg_cpu_pct, 1), '%, Memory: ', ROUND(u.avg_mem_pct, 1), '%). Switch to smaller instance types immediately.')\n",
    "    WHEN u.avg_cpu_pct < 30 \n",
    "      THEN CONCAT('HIGH: Low CPU utilization (', ROUND(u.avg_cpu_pct, 1), '%). Consider compute-optimized instances or reduce cluster size.')\n",
    "    WHEN u.avg_mem_pct < 40 \n",
    "      THEN CONCAT('HIGH: Low memory utilization (', ROUND(u.avg_mem_pct, 1), '%). Consider memory-optimized instances or reduce memory allocation.')\n",
    "    WHEN u.avg_autoterm_minutes > 60 OR u.avg_autoterm_minutes IS NULL \n",
    "      THEN CONCAT('MEDIUM: Auto-termination set to ', COALESCE(CAST(u.avg_autoterm_minutes AS STRING), 'NONE'), ' minutes. Reduce to 15-30 minutes to save on idle time.')\n",
    "    WHEN u.photon_usage_pct < 50 \n",
    "      THEN CONCAT('LOW: Photon usage at ', ROUND(u.photon_usage_pct, 1), '%. Enable Photon for 2-3x performance improvement.')\n",
    "    ELSE 'OPTIMAL: Resource utilization appears efficient. Continue monitoring.'\n",
    "  END as recommendation,\n",
    "  \n",
    "  -- Savings calculation\n",
    "  CASE \n",
    "    WHEN u.avg_cpu_pct < 20 AND u.avg_mem_pct < 30 THEN ROUND(u.total_cost_usd * 0.40, 2)\n",
    "    WHEN u.avg_cpu_pct < 30 THEN ROUND(u.total_cost_usd * 0.25, 2)\n",
    "    WHEN u.avg_mem_pct < 40 THEN ROUND(u.total_cost_usd * 0.20, 2)\n",
    "    WHEN u.avg_autoterm_minutes > 60 OR u.avg_autoterm_minutes IS NULL THEN ROUND(u.total_cost_usd * 0.15, 2)\n",
    "    WHEN u.photon_usage_pct < 50 THEN ROUND(u.total_cost_usd * 0.10, 2)\n",
    "    ELSE 0\n",
    "  END as estimated_monthly_savings,\n",
    "  \n",
    "  -- Action items\n",
    "  CASE \n",
    "    WHEN u.avg_cpu_pct < 20 AND u.avg_mem_pct < 30 \n",
    "      THEN 'Downsize to instance with 50% fewer cores and memory'\n",
    "    WHEN u.avg_cpu_pct < 30 \n",
    "      THEN 'Switch to compute-optimized instance family'\n",
    "    WHEN u.avg_mem_pct < 40 \n",
    "      THEN 'Reduce memory allocation by 30-40%'\n",
    "    WHEN u.avg_autoterm_minutes > 60 OR u.avg_autoterm_minutes IS NULL \n",
    "      THEN 'Set auto-termination to 20 minutes'\n",
    "    WHEN u.photon_usage_pct < 50 \n",
    "      THEN 'Enable Photon on all clusters'\n",
    "    ELSE 'No immediate action required'\n",
    "  END as action_item,\n",
    "  \n",
    "  -- Validated savings (capped at total cost)\n",
    "  ROUND(\n",
    "    LEAST(\n",
    "      CASE \n",
    "        WHEN u.avg_cpu_pct < 20 AND u.avg_mem_pct < 30 THEN u.total_cost_usd * 0.40\n",
    "        WHEN u.avg_cpu_pct < 30 THEN u.total_cost_usd * 0.25\n",
    "        WHEN u.avg_mem_pct < 40 THEN u.total_cost_usd * 0.20\n",
    "        WHEN u.avg_autoterm_minutes > 60 OR u.avg_autoterm_minutes IS NULL THEN u.total_cost_usd * 0.15\n",
    "        WHEN u.photon_usage_pct < 50 THEN u.total_cost_usd * 0.10\n",
    "        ELSE 0\n",
    "      END,\n",
    "      u.total_cost_usd,\n",
    "      (SELECT total_all_purpose_cost FROM base_cost)\n",
    "    ), 2\n",
    "  ) as validated_savings,\n",
    "  \n",
    "  CURRENT_TIMESTAMP() as created_at\n",
    "  \n",
    "FROM ex_dash_temp.billing_forecast.user_total_cost u\n",
    "ORDER BY estimated_monthly_savings DESC, u.total_cost_usd DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(user_opportunities_query)\n",
    "\n",
    "displayHTML(\"‚úÖ User opportunities table created: ex_dash_temp.billing_forecast.user_opportunities\")\n",
    "\n",
    "# Show summary by priority\n",
    "summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  opportunity_priority,\n",
    "  COUNT(*) as users_count,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost,\n",
    "  ROUND(SUM(validated_savings), 2) as total_potential_savings,\n",
    "  ROUND(AVG(avg_cpu_pct), 2) as avg_cpu_utilization,\n",
    "  ROUND(AVG(avg_mem_pct), 2) as avg_memory_utilization\n",
    "FROM ex_dash_temp.billing_forecast.user_opportunities\n",
    "GROUP BY opportunity_priority\n",
    "ORDER BY \n",
    "  CASE opportunity_priority\n",
    "    WHEN 'CRITICAL' THEN 1\n",
    "    WHEN 'HIGH' THEN 2\n",
    "    WHEN 'MEDIUM' THEN 3\n",
    "    WHEN 'LOW' THEN 4\n",
    "    ELSE 5\n",
    "  END\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY BY PRIORITY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.user_opportunities ORDER BY validated_savings DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25170882-150c-4552-a656-a3cd5281e128",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1763441517165}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 1
      }
     },
     "title": "Step 9: Per Cluster Opportunity Savings and Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "# Step 9: Create Per Cluster Opportunity Recommendations\n",
    "# Identifies cost optimization opportunities for each cluster with specific instance type recommendations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(\"<h2>STEP 9: CREATE PER CLUSTER OPPORTUNITY RECOMMENDATIONS</h2><p>üéØ Creating cluster-level cost optimization opportunities</p>\")\n",
    "\n",
    "# Get total all-purpose cost for savings validation\n",
    "total_cost_val = spark.sql(f\"\"\"\n",
    "SELECT ROUND(SUM(total_cost_usd), 2) as total_cost\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "WHERE usage_date >= '{start_date}'\n",
    "\"\"\").collect()[0]['total_cost']\n",
    "\n",
    "# Create cluster opportunities with specific instance type recommendations\n",
    "cluster_opp_df = spark.sql(f\"\"\"\n",
    "WITH cluster_analysis AS (\n",
    "  SELECT \n",
    "    cluster_id,\n",
    "    cluster_name,\n",
    "    cluster_owner,\n",
    "    workspace_name,\n",
    "    primary_instance_type,\n",
    "    driver_instance_type,\n",
    "    worker_instance_type,\n",
    "    worker_count,\n",
    "    min_workers,\n",
    "    max_workers,\n",
    "    total_cost_usd,\n",
    "    days_active,\n",
    "    avg_cpu_pct,\n",
    "    avg_mem_pct,\n",
    "    avg_network_mb,\n",
    "    total_network_gb,\n",
    "    cpu_efficiency_pct,\n",
    "    memory_efficiency_pct,\n",
    "    core_count,\n",
    "    memory_gb,\n",
    "    telemetry_coverage_pct,\n",
    "    autoterm_minutes,\n",
    "    \n",
    "    -- Calculate raw savings\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 AND memory_efficiency_pct < 25 THEN total_cost_usd * 0.45\n",
    "      WHEN cpu_efficiency_pct < 25 THEN total_cost_usd * 0.30\n",
    "      WHEN memory_efficiency_pct < 40 THEN total_cost_usd * 0.20\n",
    "      WHEN autoterm_minutes > 60 THEN total_cost_usd * 0.15\n",
    "      ELSE total_cost_usd * 0.05\n",
    "    END as raw_savings,\n",
    "    \n",
    "    -- Priority\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 AND memory_efficiency_pct < 25 THEN 'CRITICAL'\n",
    "      WHEN cpu_efficiency_pct < 25 OR memory_efficiency_pct < 40 THEN 'HIGH'\n",
    "      ELSE 'LOW'\n",
    "    END as opportunity_priority,\n",
    "    \n",
    "    -- Suggested instances (downsize by one level)\n",
    "    CASE \n",
    "      WHEN driver_instance_type LIKE '%12xlarge%' THEN REGEXP_REPLACE(driver_instance_type, '12xlarge', '8xlarge')\n",
    "      WHEN driver_instance_type LIKE '%16xlarge%' THEN REGEXP_REPLACE(driver_instance_type, '16xlarge', '8xlarge')\n",
    "      WHEN driver_instance_type LIKE '%8xlarge%' THEN REGEXP_REPLACE(driver_instance_type, '8xlarge', '4xlarge')\n",
    "      WHEN driver_instance_type LIKE '%4xlarge%' THEN REGEXP_REPLACE(driver_instance_type, '4xlarge', '2xlarge')\n",
    "      WHEN driver_instance_type LIKE '%2xlarge%' THEN REGEXP_REPLACE(driver_instance_type, '2xlarge', 'xlarge')\n",
    "      ELSE driver_instance_type\n",
    "    END as suggested_driver_instance,\n",
    "    \n",
    "    CASE \n",
    "      WHEN worker_instance_type LIKE '%12xlarge%' THEN REGEXP_REPLACE(worker_instance_type, '12xlarge', '8xlarge')\n",
    "      WHEN worker_instance_type LIKE '%16xlarge%' THEN REGEXP_REPLACE(worker_instance_type, '16xlarge', '8xlarge')\n",
    "      WHEN worker_instance_type LIKE '%8xlarge%' THEN REGEXP_REPLACE(worker_instance_type, '8xlarge', '4xlarge')\n",
    "      WHEN worker_instance_type LIKE '%4xlarge%' THEN REGEXP_REPLACE(worker_instance_type, '4xlarge', '2xlarge')\n",
    "      WHEN worker_instance_type LIKE '%2xlarge%' THEN REGEXP_REPLACE(worker_instance_type, '2xlarge', 'xlarge')\n",
    "      ELSE worker_instance_type\n",
    "    END as suggested_worker_instance,\n",
    "    \n",
    "    -- Current worker configuration\n",
    "    CASE \n",
    "      WHEN worker_count IS NOT NULL THEN CONCAT('Fixed: ', worker_count, ' workers')\n",
    "      WHEN min_workers IS NOT NULL AND max_workers IS NOT NULL THEN CONCAT('Autoscale: ', min_workers, '-', max_workers, ' workers')\n",
    "      ELSE 'Unknown'\n",
    "    END as current_worker_config\n",
    "    \n",
    "  FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    "  WHERE telemetry_coverage_pct > 50\n",
    "),\n",
    "cluster_with_recommendations AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    -- Detailed recommendation with current configuration\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 AND memory_efficiency_pct < 25 THEN \n",
    "        CASE \n",
    "          WHEN driver_instance_type != worker_instance_type THEN\n",
    "            CONCAT('CRITICAL: Cluster \"', cluster_name, '\" severely under-utilized (CPU: ', ROUND(cpu_efficiency_pct, 1), '%, Memory: ', ROUND(memory_efficiency_pct, 1), \n",
    "                   '%). Current: Driver=', driver_instance_type, ', Workers=', worker_instance_type, ' (', current_worker_config, '). ',\n",
    "                   'Recommended: Change driver to ', suggested_driver_instance, ' and workers to ', suggested_worker_instance, '.')\n",
    "          ELSE\n",
    "            CONCAT('CRITICAL: Cluster \"', cluster_name, '\" severely under-utilized (CPU: ', ROUND(cpu_efficiency_pct, 1), '%, Memory: ', ROUND(memory_efficiency_pct, 1), \n",
    "                   '%). Current: Driver & Workers=', driver_instance_type, ' (', current_worker_config, '). ',\n",
    "                   'Recommended: Change both to ', suggested_driver_instance, '.')\n",
    "        END\n",
    "      WHEN cpu_efficiency_pct < 25 THEN \n",
    "        CASE \n",
    "          WHEN driver_instance_type != worker_instance_type THEN\n",
    "            CONCAT('HIGH: Cluster \"', cluster_name, '\" has low CPU efficiency (', ROUND(cpu_efficiency_pct, 1), \n",
    "                   '%). Current: Driver=', driver_instance_type, ', Workers=', worker_instance_type, ' (', current_worker_config, '). ',\n",
    "                   'Recommended: Change driver to ', suggested_driver_instance, ' and workers to ', suggested_worker_instance, '.')\n",
    "          ELSE\n",
    "            CONCAT('HIGH: Cluster \"', cluster_name, '\" has low CPU efficiency (', ROUND(cpu_efficiency_pct, 1), \n",
    "                   '%). Current: Driver & Workers=', driver_instance_type, ' (', current_worker_config, '). ',\n",
    "                   'Recommended: Change both to ', suggested_driver_instance, '.')\n",
    "        END\n",
    "      WHEN memory_efficiency_pct < 40 THEN \n",
    "        CASE \n",
    "          WHEN driver_instance_type != worker_instance_type THEN\n",
    "            CONCAT('HIGH: Cluster \"', cluster_name, '\" has low memory efficiency (', ROUND(memory_efficiency_pct, 1), \n",
    "                   '%). Current: Driver=', driver_instance_type, ', Workers=', worker_instance_type, ' (', current_worker_config, '). ',\n",
    "                   'Recommended: Switch to compute-optimized instances.')\n",
    "          ELSE\n",
    "            CONCAT('HIGH: Cluster \"', cluster_name, '\" has low memory efficiency (', ROUND(memory_efficiency_pct, 1), \n",
    "                   '%). Current: Driver & Workers=', driver_instance_type, ' (', current_worker_config, '). ',\n",
    "                   'Recommended: Switch to compute-optimized instances.')\n",
    "        END\n",
    "      ELSE \n",
    "        CONCAT('LOW: Cluster \"', cluster_name, '\" is reasonably utilized. Continue monitoring.')\n",
    "    END as recommendation,\n",
    "    \n",
    "    -- Action item with specific instance types\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 AND memory_efficiency_pct < 25 THEN \n",
    "        CASE \n",
    "          WHEN driver_instance_type != worker_instance_type THEN\n",
    "            CONCAT('Change Driver: ', driver_instance_type, ' ‚Üí ', suggested_driver_instance, '; Change Workers: ', worker_instance_type, ' ‚Üí ', suggested_worker_instance, ' (Keep ', current_worker_config, ')')\n",
    "          ELSE\n",
    "            CONCAT('Change Driver & Workers: ', driver_instance_type, ' ‚Üí ', suggested_driver_instance, ' (Keep ', current_worker_config, ')')\n",
    "        END\n",
    "      WHEN cpu_efficiency_pct < 25 THEN \n",
    "        CASE \n",
    "          WHEN driver_instance_type != worker_instance_type THEN\n",
    "            CONCAT('Change Driver: ', driver_instance_type, ' ‚Üí ', suggested_driver_instance, '; Change Workers: ', worker_instance_type, ' ‚Üí ', suggested_worker_instance, ' (Keep ', current_worker_config, ')')\n",
    "          ELSE\n",
    "            CONCAT('Change Driver & Workers: ', driver_instance_type, ' ‚Üí ', suggested_driver_instance, ' (Keep ', current_worker_config, ')')\n",
    "        END\n",
    "      WHEN memory_efficiency_pct < 40 THEN \n",
    "        CASE \n",
    "          WHEN driver_instance_type != worker_instance_type THEN\n",
    "            CONCAT('Switch Driver (', driver_instance_type, ') and Workers (', worker_instance_type, ') to compute-optimized (Keep ', current_worker_config, ')')\n",
    "          ELSE\n",
    "            CONCAT('Switch Driver & Workers (', driver_instance_type, ') to compute-optimized (Keep ', current_worker_config, ')')\n",
    "        END\n",
    "      ELSE \n",
    "        'Continue monitoring'\n",
    "    END as action_item\n",
    "    \n",
    "  FROM cluster_analysis\n",
    "),\n",
    "total_savings AS (\n",
    "  SELECT SUM(raw_savings) as total_raw_savings\n",
    "  FROM cluster_with_recommendations\n",
    ")\n",
    "SELECT \n",
    "  c.cluster_id,\n",
    "  c.cluster_name,\n",
    "  c.cluster_owner,\n",
    "  c.workspace_name,\n",
    "  c.primary_instance_type,\n",
    "  c.driver_instance_type,\n",
    "  c.worker_instance_type,\n",
    "  c.suggested_driver_instance,\n",
    "  c.suggested_worker_instance,\n",
    "  c.worker_count,\n",
    "  c.min_workers,\n",
    "  c.max_workers,\n",
    "  c.current_worker_config,\n",
    "  c.total_cost_usd,\n",
    "  c.days_active,\n",
    "  c.avg_cpu_pct,\n",
    "  c.avg_mem_pct,\n",
    "  c.avg_network_mb,\n",
    "  c.total_network_gb,\n",
    "  c.cpu_efficiency_pct,\n",
    "  c.memory_efficiency_pct,\n",
    "  c.core_count,\n",
    "  c.memory_gb,\n",
    "  c.telemetry_coverage_pct,\n",
    "  c.autoterm_minutes,\n",
    "  c.opportunity_priority,\n",
    "  c.recommendation,\n",
    "  c.action_item,\n",
    "  -- Cap individual savings proportionally if total exceeds all-purpose cost\n",
    "  CASE \n",
    "    WHEN (SELECT total_raw_savings FROM total_savings) > {total_cost_val} THEN \n",
    "      ROUND(c.raw_savings * {total_cost_val} / (SELECT total_raw_savings FROM total_savings), 2)\n",
    "    ELSE \n",
    "      ROUND(c.raw_savings, 2)\n",
    "  END as validated_savings\n",
    "FROM cluster_with_recommendations c\n",
    "ORDER BY validated_savings DESC, total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "# Write table\n",
    "cluster_opp_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"ex_dash_temp.billing_forecast.cluster_opportunities\")\n",
    "\n",
    "displayHTML(\"‚úÖ Cluster opportunities table created: ex_dash_temp.billing_forecast.cluster_opportunities\")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  opportunity_priority,\n",
    "  COUNT(*) as clusters,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost,\n",
    "  ROUND(SUM(validated_savings), 2) as total_savings\n",
    "FROM ex_dash_temp.billing_forecast.cluster_opportunities\n",
    "GROUP BY opportunity_priority\n",
    "ORDER BY \n",
    "  CASE opportunity_priority \n",
    "    WHEN 'CRITICAL' THEN 1 \n",
    "    WHEN 'HIGH' THEN 2 \n",
    "    ELSE 3 \n",
    "  END\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY BY PRIORITY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  cluster_name,\n",
    "  driver_instance_type,\n",
    "  worker_instance_type,\n",
    "  current_worker_config,\n",
    "  suggested_driver_instance,\n",
    "  suggested_worker_instance,\n",
    "  total_cost_usd,\n",
    "  cpu_efficiency_pct,\n",
    "  memory_efficiency_pct,\n",
    "  opportunity_priority,\n",
    "  recommendation,\n",
    "  action_item,\n",
    "  validated_savings\n",
    "FROM ex_dash_temp.billing_forecast.cluster_opportunities \n",
    "ORDER BY validated_savings DESC \n",
    "LIMIT 50\n",
    "\"\"\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebf3091-5a4e-4394-ae7f-9a5797542d55",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 10: Per Instance Opportunity Savings and Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "# Step 10: Create Per Instance Opportunity Recommendations\n",
    "# Identifies cost optimization opportunities for each instance type\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(\"<h2>STEP 10: CREATE PER INSTANCE OPPORTUNITY RECOMMENDATIONS</h2><p>üéØ Creating instance-level cost optimization opportunities</p>\")\n",
    "\n",
    "# Get total all-purpose cost for savings validation\n",
    "total_cost_val = spark.sql(f\"\"\"\n",
    "SELECT ROUND(SUM(total_cost_usd), 2) as total_cost\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "WHERE usage_date >= '{start_date}'\n",
    "\"\"\").collect()[0]['total_cost']\n",
    "\n",
    "# Create instance opportunities\n",
    "instance_opp_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE ex_dash_temp.billing_forecast.instance_opportunities\n",
    "USING DELTA\n",
    "AS\n",
    "WITH instance_analysis AS (\n",
    "  SELECT \n",
    "    instance_type,\n",
    "    total_cost_usd,\n",
    "    unique_clusters,\n",
    "    unique_users,\n",
    "    unique_workspaces,\n",
    "    days_active,\n",
    "    avg_cpu_pct,\n",
    "    avg_mem_pct,\n",
    "    avg_network_mb,\n",
    "    total_network_gb,\n",
    "    cpu_efficiency_pct,\n",
    "    memory_efficiency_pct,\n",
    "    core_count,\n",
    "    memory_gb,\n",
    "    telemetry_coverage_pct,\n",
    "    \n",
    "    -- Calculate raw savings\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 THEN total_cost_usd * 0.50\n",
    "      WHEN cpu_efficiency_pct < 25 THEN total_cost_usd * 0.35\n",
    "      WHEN memory_efficiency_pct < 30 THEN total_cost_usd * 0.25\n",
    "      ELSE total_cost_usd * 0.10\n",
    "    END as raw_savings,\n",
    "    \n",
    "    -- Priority\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 THEN 'CRITICAL'\n",
    "      WHEN cpu_efficiency_pct < 25 OR memory_efficiency_pct < 30 THEN 'HIGH'\n",
    "      ELSE 'LOW'\n",
    "    END as opportunity_priority,\n",
    "    \n",
    "    -- Recommendation\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 THEN \n",
    "        CONCAT('CRITICAL: Instance type \"', instance_type, '\" is severely under-utilized across ', unique_clusters, ' clusters (CPU: ', ROUND(cpu_efficiency_pct, 1), '%, Memory: ', ROUND(memory_efficiency_pct, 1), '%). Migrate all workloads to smaller instance family.')\n",
    "      WHEN cpu_efficiency_pct < 25 THEN \n",
    "        CONCAT('HIGH: Instance type \"', instance_type, '\" has low CPU efficiency (', ROUND(cpu_efficiency_pct, 1), '%) across ', unique_clusters, ' clusters. Consider compute-optimized alternatives.')\n",
    "      WHEN memory_efficiency_pct < 30 THEN \n",
    "        CONCAT('HIGH: Instance type \"', instance_type, '\" has low memory efficiency (', ROUND(memory_efficiency_pct, 1), '%) across ', unique_clusters, ' clusters. Consider compute-optimized alternatives.')\n",
    "      ELSE \n",
    "        CONCAT('LOW: Instance type \"', instance_type, '\" is reasonably utilized across ', unique_clusters, ' clusters.')\n",
    "    END as recommendation,\n",
    "    \n",
    "    -- Suggested action\n",
    "    CASE \n",
    "      WHEN cpu_efficiency_pct < 15 THEN \n",
    "        CONCAT('Migrate to instance with ', CAST(CEIL(core_count * 0.4) AS INT), ' cores, ', ROUND(memory_gb * 0.4, 1), ' GB')\n",
    "      WHEN cpu_efficiency_pct < 25 THEN \n",
    "        CONCAT('Switch to compute-optimized with ', CAST(CEIL(core_count * 0.6) AS INT), ' cores')\n",
    "      WHEN memory_efficiency_pct < 30 THEN \n",
    "        'Switch to compute-optimized instance'\n",
    "      ELSE \n",
    "        'Continue monitoring'\n",
    "    END as suggested_action,\n",
    "    \n",
    "    CONCAT('Affects ', unique_clusters, ' clusters, ', unique_users, ' users across ', unique_workspaces, ' workspaces') as impact_scope\n",
    "    \n",
    "  FROM ex_dash_temp.billing_forecast.instance_total_cost\n",
    "  WHERE telemetry_coverage_pct > 50\n",
    "),\n",
    "total_savings AS (\n",
    "  SELECT SUM(raw_savings) as total_raw_savings\n",
    "  FROM instance_analysis\n",
    ")\n",
    "SELECT \n",
    "  i.*,\n",
    "  -- Cap individual savings proportionally if total exceeds all-purpose cost\n",
    "  CASE \n",
    "    WHEN (SELECT total_raw_savings FROM total_savings) > {total_cost_val} THEN \n",
    "      ROUND(i.raw_savings * {total_cost_val} / (SELECT total_raw_savings FROM total_savings), 2)\n",
    "    ELSE \n",
    "      ROUND(i.raw_savings, 2)\n",
    "  END as validated_savings\n",
    "FROM instance_analysis i\n",
    "ORDER BY validated_savings DESC, total_cost_usd DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(instance_opp_query)\n",
    "\n",
    "displayHTML(\"‚úÖ Instance opportunities table created: ex_dash_temp.billing_forecast.instance_opportunities\")\n",
    "\n",
    "# Summary\n",
    "summary = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  opportunity_priority,\n",
    "  COUNT(*) as instances,\n",
    "  ROUND(SUM(total_cost_usd), 2) as total_cost,\n",
    "  ROUND(SUM(validated_savings), 2) as total_savings\n",
    "FROM ex_dash_temp.billing_forecast.instance_opportunities\n",
    "GROUP BY opportunity_priority\n",
    "ORDER BY \n",
    "  CASE opportunity_priority \n",
    "    WHEN 'CRITICAL' THEN 1 \n",
    "    WHEN 'HIGH' THEN 2 \n",
    "    ELSE 3 \n",
    "  END\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"<h3>üìä SUMMARY BY PRIORITY:</h3>\")\n",
    "display(summary)\n",
    "\n",
    "# Display sample\n",
    "displayHTML(\"<h3>üìã SAMPLE DATA (50 rows):</h3>\")\n",
    "sample_data = spark.sql(\"SELECT * FROM ex_dash_temp.billing_forecast.instance_opportunities ORDER BY validated_savings DESC LIMIT 50\")\n",
    "display(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee022ccb-2df0-4bfc-a814-77120b45cf78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 11: Comprehensive Summary and All Tables Display"
    }
   },
   "outputs": [],
   "source": [
    "# Step 11: Comprehensive Summary\n",
    "# Display all created tables and overall analysis summary with validation\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>COMPREHENSIVE ALL-PURPOSE CLUSTER COST ANALYSIS - SUMMARY</h2><p>üìÖ Analysis Period: {start_date} to current date</p>\")\n",
    "\n",
    "# Get total all-purpose cost\n",
    "total_cost_summary = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  ROUND(SUM(total_cost_usd), 2) as total_all_purpose_cost,\n",
    "  COUNT(DISTINCT usage_date) as days_analyzed,\n",
    "  COUNT(DISTINCT cluster_id) as unique_clusters,\n",
    "  COUNT(DISTINCT principal_email) as unique_users,\n",
    "  COUNT(DISTINCT workspace_name) as unique_workspaces\n",
    "FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "WHERE usage_date >= '{start_date}'\n",
    "\"\"\")\n",
    "\n",
    "total_cost_data = total_cost_summary.collect()[0]\n",
    "total_all_purpose_cost = total_cost_data['total_all_purpose_cost'] or 0\n",
    "\n",
    "displayHTML(f\"<p>üí∞ <b>TOTAL ALL-PURPOSE COST:</b> ${total_all_purpose_cost:,.2f}</p>\")\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<h3>üìä ANALYSIS SCOPE:</h3>\n",
    "<ul>\n",
    "<li>Days Analyzed: {total_cost_data['days_analyzed']}</li>\n",
    "<li>Unique Clusters: {total_cost_data['unique_clusters']}</li>\n",
    "<li>Unique Users: {total_cost_data['unique_users']}</li>\n",
    "<li>Unique Workspaces: {total_cost_data['unique_workspaces']}</li>\n",
    "</ul>\n",
    "\"\"\")\n",
    "\n",
    "# Calculate total savings potential\n",
    "savings_summary = spark.sql(\"\"\"\n",
    "WITH user_savings AS (\n",
    "  SELECT COALESCE(SUM(validated_savings), 0) as user_savings, COUNT(*) as user_count\n",
    "  FROM ex_dash_temp.billing_forecast.user_opportunities\n",
    "),\n",
    "cluster_savings AS (\n",
    "  SELECT COALESCE(SUM(validated_savings), 0) as cluster_savings, COUNT(*) as cluster_count\n",
    "  FROM ex_dash_temp.billing_forecast.cluster_opportunities\n",
    "),\n",
    "instance_savings AS (\n",
    "  SELECT COALESCE(SUM(validated_savings), 0) as instance_savings, COUNT(*) as instance_count\n",
    "  FROM ex_dash_temp.billing_forecast.instance_opportunities\n",
    ")\n",
    "SELECT \n",
    "  ROUND(u.user_savings, 2) as user_level_savings,\n",
    "  u.user_count,\n",
    "  ROUND(c.cluster_savings, 2) as cluster_level_savings,\n",
    "  c.cluster_count,\n",
    "  ROUND(i.instance_savings, 2) as instance_level_savings,\n",
    "  i.instance_count,\n",
    "  ROUND(GREATEST(u.user_savings, c.cluster_savings, i.instance_savings), 2) as max_potential_savings\n",
    "FROM user_savings u, cluster_savings c, instance_savings i\n",
    "\"\"\")\n",
    "\n",
    "savings_data = savings_summary.collect()[0]\n",
    "max_savings = savings_data['max_potential_savings'] or 0\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<h3>üí∏ POTENTIAL SAVINGS ANALYSIS:</h3>\n",
    "<ul>\n",
    "<li>User-level Opportunities: ${savings_data['user_level_savings']:,.2f} ({savings_data['user_count']} users)</li>\n",
    "<li>Cluster-level Opportunities: ${savings_data['cluster_level_savings']:,.2f} ({savings_data['cluster_count']} clusters)</li>\n",
    "<li>Instance-level Opportunities: ${savings_data['instance_level_savings']:,.2f} ({savings_data['instance_count']} instance types)</li>\n",
    "<li>Maximum Potential Savings: ${max_savings:,.2f} ({(max_savings/total_all_purpose_cost*100) if total_all_purpose_cost > 0 else 0:.1f}%)</li>\n",
    "</ul>\n",
    "\"\"\")\n",
    "\n",
    "# Validate savings don't exceed total cost\n",
    "if max_savings <= total_all_purpose_cost:\n",
    "    displayHTML(f\"<p>‚úÖ <b>Validation Passed:</b> Total savings (${max_savings:,.2f}) ‚â§ Total cost (${total_all_purpose_cost:,.2f})</p>\")\n",
    "else:\n",
    "    displayHTML(f\"<p>‚ö†Ô∏è <b>Validation Warning:</b> Total savings (${max_savings:,.2f}) exceeds total cost (${total_all_purpose_cost:,.2f})</p>\")\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "<h3>üìä ALL TABLES CREATED IN: ex_dash_temp.billing_forecast</h3>\n",
    "<ol>\n",
    "<li>all_purpose_base - Base table with all-purpose cluster usage</li>\n",
    "<li>user_daily_telemetry - Per user daily cost with telemetry</li>\n",
    "<li>cluster_daily_telemetry - Per cluster daily cost with telemetry</li>\n",
    "<li>instance_daily_telemetry - Per instance daily cost with telemetry</li>\n",
    "<li>user_total_cost - Per user total cost (one row per user)</li>\n",
    "<li>cluster_total_cost - Per cluster total cost (one row per cluster)</li>\n",
    "<li>instance_total_cost - Per instance total cost (one row per instance)</li>\n",
    "<li>user_opportunities - Per user savings opportunities</li>\n",
    "<li>cluster_opportunities - Per cluster savings opportunities</li>\n",
    "<li>instance_opportunities - Per instance savings opportunities</li>\n",
    "</ol>\n",
    "<p>‚úÖ <b>ANALYSIS COMPLETE - ALL TABLES READY FOR QUERYING</b></p>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38b6c95-5232-4334-abfa-7e4e40c3dae1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display: User Total Cost Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Display: User Total Cost Analysis\n",
    "# Complete results for all users\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>üë§ USER TOTAL COST ANALYSIS</h2><p>Period: {start_date} onwards</p>\")\n",
    "\n",
    "user_results = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  principal_email,\n",
    "  principal_type,\n",
    "  primary_workspace,\n",
    "  workspaces_used,\n",
    "  total_cost_usd,\n",
    "  total_dbus,\n",
    "  days_active,\n",
    "  unique_clusters,\n",
    "  avg_cpu_pct,\n",
    "  avg_mem_pct,\n",
    "  avg_network_mb,\n",
    "  total_network_gb,\n",
    "  avg_cores,\n",
    "  avg_memory_gb,\n",
    "  photon_usage_pct,\n",
    "  avg_autoterm_minutes,\n",
    "  telemetry_coverage_pct,\n",
    "  first_usage_date,\n",
    "  last_usage_date\n",
    "FROM ex_dash_temp.billing_forecast.user_total_cost\n",
    "ORDER BY total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "user_count = user_results.count()\n",
    "total_user_cost = user_results.agg({'total_cost_usd': 'sum'}).collect()[0][0] or 0\n",
    "\n",
    "displayHTML(f\"<p><b>Total Users:</b> {user_count}<br><b>Total Cost:</b> ${total_user_cost:,.2f}</p>\")\n",
    "\n",
    "if user_count > 0:\n",
    "    display(user_results)\n",
    "else:\n",
    "    displayHTML(\"<p>‚ö†Ô∏è No user data found for the selected date range</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6486bdcd-4267-4ae6-ba79-80bd13999845",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display: Cluster Total Cost Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Display: Cluster Total Cost Analysis\n",
    "# Complete results for all clusters\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>üíª CLUSTER TOTAL COST ANALYSIS</h2><p>Period: {start_date} onwards</p>\")\n",
    "\n",
    "cluster_results = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  cluster_id,\n",
    "  cluster_name,\n",
    "  cluster_owner,\n",
    "  workspace_name,\n",
    "  primary_instance_type,\n",
    "  driver_instance_type,\n",
    "  worker_instance_type,\n",
    "  worker_count,\n",
    "  min_workers,\n",
    "  max_workers,\n",
    "  total_cost_usd,\n",
    "  total_dbus,\n",
    "  days_active,\n",
    "  avg_cpu_pct,\n",
    "  avg_mem_pct,\n",
    "  avg_network_mb,\n",
    "  total_network_gb,\n",
    "  cpu_efficiency_pct,\n",
    "  memory_efficiency_pct,\n",
    "  core_count,\n",
    "  memory_gb,\n",
    "  photon_enabled,\n",
    "  autoterm_minutes,\n",
    "  telemetry_coverage_pct,\n",
    "  first_usage_date,\n",
    "  last_usage_date\n",
    "FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    "ORDER BY total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "cluster_count = cluster_results.count()\n",
    "total_cluster_cost = cluster_results.agg({'total_cost_usd': 'sum'}).collect()[0][0] or 0\n",
    "\n",
    "displayHTML(f\"<p><b>Total Clusters:</b> {cluster_count}<br><b>Total Cost:</b> ${total_cluster_cost:,.2f}</p>\")\n",
    "\n",
    "if cluster_count > 0:\n",
    "    display(cluster_results)\n",
    "else:\n",
    "    displayHTML(\"<p>‚ö†Ô∏è No cluster data found for the selected date range</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f13094-3c4a-45fb-909d-7717cb86d3ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display: Instance Total Cost Analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Display: Instance Total Cost Analysis\n",
    "# Complete results for all instance types\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>üñ•Ô∏è INSTANCE TOTAL COST ANALYSIS</h2><p>Period: {start_date} onwards</p>\")\n",
    "\n",
    "instance_results = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  instance_type,\n",
    "  total_cost_usd,\n",
    "  total_dbus,\n",
    "  unique_clusters,\n",
    "  unique_users,\n",
    "  unique_workspaces,\n",
    "  days_active,\n",
    "  avg_cpu_pct,\n",
    "  avg_mem_pct,\n",
    "  avg_network_mb,\n",
    "  total_network_gb,\n",
    "  cpu_efficiency_pct,\n",
    "  memory_efficiency_pct,\n",
    "  core_count,\n",
    "  memory_gb,\n",
    "  photon_usage_pct,\n",
    "  avg_autoterm_minutes,\n",
    "  telemetry_coverage_pct,\n",
    "  first_usage_date,\n",
    "  last_usage_date\n",
    "FROM ex_dash_temp.billing_forecast.instance_total_cost\n",
    "ORDER BY total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "instance_count = instance_results.count()\n",
    "total_instance_cost = instance_results.agg({'total_cost_usd': 'sum'}).collect()[0][0] or 0\n",
    "\n",
    "displayHTML(f\"<p><b>Total Instance Types:</b> {instance_count}<br><b>Total Cost:</b> ${total_instance_cost:,.2f}</p>\")\n",
    "\n",
    "if instance_count > 0:\n",
    "    display(instance_results)\n",
    "else:\n",
    "    displayHTML(\"<p>‚ö†Ô∏è No instance data found for the selected date range</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53888aa0-5bce-44ce-9f32-ec94d21c1cd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display: User Opportunities and Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "# Display: User Opportunities and Recommendations\n",
    "# Complete results for all users with opportunities\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>üéØ USER OPPORTUNITIES AND RECOMMENDATIONS</h2><p>Period: {start_date} onwards</p>\")\n",
    "\n",
    "user_opp_results = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  principal_email,\n",
    "  primary_workspace,\n",
    "  total_cost_usd,\n",
    "  days_active,\n",
    "  avg_cpu_pct,\n",
    "  avg_mem_pct,\n",
    "  avg_network_mb,\n",
    "  total_network_gb,\n",
    "  opportunity_priority,\n",
    "  recommendation,\n",
    "  action_item,\n",
    "  validated_savings,\n",
    "  telemetry_coverage_pct\n",
    "FROM ex_dash_temp.billing_forecast.user_opportunities\n",
    "ORDER BY validated_savings DESC, total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "user_opp_count = user_opp_results.count()\n",
    "total_user_savings = user_opp_results.agg({'validated_savings': 'sum'}).collect()[0][0] or 0\n",
    "\n",
    "displayHTML(f\"<p><b>Total Users with Opportunities:</b> {user_opp_count}<br><b>Total Potential Savings:</b> ${total_user_savings:,.2f}</p>\")\n",
    "\n",
    "if user_opp_count > 0:\n",
    "    display(user_opp_results)\n",
    "else:\n",
    "    displayHTML(\"<p>‚ö†Ô∏è No user opportunities found for the selected date range</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f08a7fa-6421-4ac2-94dc-34a751456298",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display: Cluster Opportunities and Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "# Display: Cluster Opportunities and Recommendations\n",
    "# Complete results for all clusters with opportunities\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>üéØ CLUSTER OPPORTUNITIES AND RECOMMENDATIONS</h2><p>Period: {start_date} onwards</p>\")\n",
    "\n",
    "cluster_opp_results = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  cluster_id,\n",
    "  cluster_name,\n",
    "  cluster_owner,\n",
    "  workspace_name,\n",
    "  driver_instance_type,\n",
    "  worker_instance_type,\n",
    "  current_worker_config,\n",
    "  suggested_driver_instance,\n",
    "  suggested_worker_instance,\n",
    "  total_cost_usd,\n",
    "  days_active,\n",
    "  avg_cpu_pct,\n",
    "  avg_mem_pct,\n",
    "  cpu_efficiency_pct,\n",
    "  memory_efficiency_pct,\n",
    "  opportunity_priority,\n",
    "  recommendation,\n",
    "  action_item,\n",
    "  validated_savings,\n",
    "  telemetry_coverage_pct\n",
    "FROM ex_dash_temp.billing_forecast.cluster_opportunities\n",
    "ORDER BY validated_savings DESC, total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "cluster_opp_count = cluster_opp_results.count()\n",
    "total_cluster_savings = cluster_opp_results.agg({'validated_savings': 'sum'}).collect()[0][0] or 0\n",
    "\n",
    "displayHTML(f\"<p><b>Total Clusters with Opportunities:</b> {cluster_opp_count}<br><b>Total Potential Savings:</b> ${total_cluster_savings:,.2f}</p>\")\n",
    "\n",
    "if cluster_opp_count > 0:\n",
    "    display(cluster_opp_results)\n",
    "else:\n",
    "    displayHTML(\"<p>‚ö†Ô∏è No cluster opportunities found for the selected date range</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4d0ae6-e111-4581-ac56-0c344439ce72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display: Instance Opportunities and Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "# Display: Instance Opportunities and Recommendations\n",
    "# Complete results for all instance types with opportunities\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>üéØ INSTANCE OPPORTUNITIES AND RECOMMENDATIONS</h2><p>Period: {start_date} onwards</p>\")\n",
    "\n",
    "instance_opp_results = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  instance_type,\n",
    "  total_cost_usd,\n",
    "  unique_clusters,\n",
    "  unique_users,\n",
    "  unique_workspaces,\n",
    "  days_active,\n",
    "  avg_cpu_pct,\n",
    "  avg_mem_pct,\n",
    "  avg_network_mb,\n",
    "  total_network_gb,\n",
    "  cpu_efficiency_pct,\n",
    "  memory_efficiency_pct,\n",
    "  opportunity_priority,\n",
    "  recommendation,\n",
    "  suggested_action,\n",
    "  impact_scope,\n",
    "  validated_savings,\n",
    "  telemetry_coverage_pct\n",
    "FROM ex_dash_temp.billing_forecast.instance_opportunities\n",
    "ORDER BY validated_savings DESC, total_cost_usd DESC\n",
    "\"\"\")\n",
    "\n",
    "instance_opp_count = instance_opp_results.count()\n",
    "total_instance_savings = instance_opp_results.agg({'validated_savings': 'sum'}).collect()[0][0] or 0\n",
    "\n",
    "displayHTML(f\"<p><b>Total Instance Types with Opportunities:</b> {instance_opp_count}<br><b>Total Potential Savings:</b> ${total_instance_savings:,.2f}</p>\")\n",
    "\n",
    "if instance_opp_count > 0:\n",
    "    display(instance_opp_results)\n",
    "else:\n",
    "    displayHTML(\"<p>‚ö†Ô∏è No instance opportunities found for the selected date range</p>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a359846c-c7bf-4cb3-b983-9faa809ffe92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Executive Summary - Key Insights and Next Steps"
    }
   },
   "outputs": [],
   "source": [
    "# Executive Summary - Key Insights and Recommendations\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate start_date from days_back widget\n",
    "days_back = int(dbutils.widgets.get(\"days_back\"))\n",
    "start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "\n",
    "displayHTML(f\"<h2>EXECUTIVE SUMMARY - ALL-PURPOSE CLUSTER COST ANALYSIS</h2><p>üìÖ Analysis Period: {start_date} to current date</p>\")\n",
    "\n",
    "# Get key metrics\n",
    "key_metrics = spark.sql(f\"\"\"\n",
    "WITH base_metrics AS (\n",
    "  SELECT \n",
    "    ROUND(SUM(total_cost_usd), 2) as total_cost,\n",
    "    COUNT(DISTINCT usage_date) as days_analyzed,\n",
    "    COUNT(DISTINCT cluster_id) as total_clusters,\n",
    "    COUNT(DISTINCT workspace_name) as total_workspaces\n",
    "  FROM ex_dash_temp.billing_forecast.all_purpose_base\n",
    "  WHERE usage_date >= '{start_date}'\n",
    "),\n",
    "cluster_opp AS (\n",
    "  SELECT \n",
    "    COUNT(*) as clusters_with_opp,\n",
    "    SUM(CASE WHEN opportunity_priority = 'CRITICAL' THEN 1 ELSE 0 END) as critical_clusters,\n",
    "    SUM(CASE WHEN opportunity_priority = 'HIGH' THEN 1 ELSE 0 END) as high_clusters,\n",
    "    ROUND(SUM(validated_savings), 2) as cluster_savings\n",
    "  FROM ex_dash_temp.billing_forecast.cluster_opportunities\n",
    "),\n",
    "instance_opp AS (\n",
    "  SELECT \n",
    "    COUNT(*) as instances_with_opp,\n",
    "    SUM(CASE WHEN opportunity_priority = 'CRITICAL' THEN 1 ELSE 0 END) as critical_instances,\n",
    "    SUM(CASE WHEN opportunity_priority = 'HIGH' THEN 1 ELSE 0 END) as high_instances,\n",
    "    ROUND(SUM(validated_savings), 2) as instance_savings\n",
    "  FROM ex_dash_temp.billing_forecast.instance_opportunities\n",
    "),\n",
    "top_cluster AS (\n",
    "  SELECT cluster_name, ROUND(total_cost_usd, 2) as cost\n",
    "  FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    "  ORDER BY total_cost_usd DESC LIMIT 1\n",
    "),\n",
    "top_instance AS (\n",
    "  SELECT instance_type, ROUND(total_cost_usd, 2) as cost, unique_clusters\n",
    "  FROM ex_dash_temp.billing_forecast.instance_total_cost\n",
    "  ORDER BY total_cost_usd DESC LIMIT 1\n",
    "),\n",
    "avg_util AS (\n",
    "  SELECT \n",
    "    ROUND(AVG(avg_cpu_pct), 0) as avg_cpu,\n",
    "    ROUND(AVG(avg_mem_pct), 0) as avg_mem,\n",
    "    ROUND(AVG(telemetry_coverage_pct), 0) as avg_telemetry\n",
    "  FROM ex_dash_temp.billing_forecast.cluster_total_cost\n",
    ")\n",
    "SELECT \n",
    "  b.*,\n",
    "  c.clusters_with_opp,\n",
    "  c.critical_clusters,\n",
    "  c.high_clusters,\n",
    "  c.cluster_savings,\n",
    "  i.instances_with_opp,\n",
    "  i.critical_instances,\n",
    "  i.high_instances,\n",
    "  i.instance_savings,\n",
    "  GREATEST(c.cluster_savings, i.instance_savings) as max_savings,\n",
    "  tc.cluster_name as top_cluster_name,\n",
    "  tc.cost as top_cluster_cost,\n",
    "  ti.instance_type as top_instance_type,\n",
    "  ti.cost as top_instance_cost,\n",
    "  ti.unique_clusters as top_instance_clusters,\n",
    "  u.avg_cpu,\n",
    "  u.avg_mem,\n",
    "  u.avg_telemetry\n",
    "FROM base_metrics b, cluster_opp c, instance_opp i, top_cluster tc, top_instance ti, avg_util u\n",
    "\"\"\")\n",
    "\n",
    "metrics = key_metrics.collect()[0]\n",
    "\n",
    "# Convert to float\n",
    "total_cost = float(metrics['total_cost'])\n",
    "max_savings = float(metrics['max_savings'])\n",
    "cluster_savings = float(metrics['cluster_savings'])\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<h3>üí∞ FINANCIAL SUMMARY:</h3>\n",
    "<ul>\n",
    "<li>Total All-Purpose Cost: ${total_cost:,.2f}</li>\n",
    "<li>Maximum Potential Savings: ${max_savings:,.2f}</li>\n",
    "<li>Savings Percentage: {(max_savings/total_cost*100):.1f}%</li>\n",
    "</ul>\n",
    "\n",
    "<h3>üìä SCOPE:</h3>\n",
    "<ul>\n",
    "<li>Days Analyzed: {metrics['days_analyzed']}</li>\n",
    "<li>Total Clusters: {metrics['total_clusters']}</li>\n",
    "<li>Total Workspaces: {metrics['total_workspaces']}</li>\n",
    "<li>Instance Types: {metrics['instances_with_opp']}</li>\n",
    "</ul>\n",
    "\n",
    "<h3>üî¥ CRITICAL PRIORITIES (Immediate Action):</h3>\n",
    "<ul>\n",
    "<li>{metrics['critical_clusters']} clusters with severe under-utilization</li>\n",
    "<li>{metrics['critical_instances']} instance types with &lt;15% CPU efficiency</li>\n",
    "<li>Potential Savings: ‚àº${cluster_savings * 0.7:,.2f}</li>\n",
    "</ul>\n",
    "\n",
    "<h3>üü° HIGH PRIORITIES (Action Within 30 Days):</h3>\n",
    "<ul>\n",
    "<li>{metrics['high_clusters']} clusters with low utilization</li>\n",
    "<li>Potential Savings: ‚àº${cluster_savings * 0.3:,.2f}</li>\n",
    "</ul>\n",
    "\n",
    "<h3>üéØ TOP RECOMMENDATIONS:</h3>\n",
    "<p><b>1. IMMEDIATE ACTIONS (Next 7 Days):</b></p>\n",
    "<ul>\n",
    "<li>Review and downsize the top 10 CRITICAL clusters</li>\n",
    "<li>Focus on top cost drivers: {metrics['top_instance_type']} (${metrics['top_instance_cost']:,.0f})</li>\n",
    "<li>Implement auto-termination policies (20 minutes max)</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>2. SHORT-TERM ACTIONS (Next 30 Days):</b></p>\n",
    "<ul>\n",
    "<li>Migrate HIGH priority clusters to compute-optimized instances</li>\n",
    "<li>Enable Photon on all compatible clusters</li>\n",
    "<li>Standardize instance sizing across workspaces</li>\n",
    "</ul>\n",
    "\n",
    "<p><b>3. GOVERNANCE & MONITORING:</b></p>\n",
    "<ul>\n",
    "<li>Implement cluster policies with max instance sizes</li>\n",
    "<li>Set up cost alerts for high-cost clusters</li>\n",
    "<li>Monthly cost reviews with cluster owners</li>\n",
    "</ul>\n",
    "\n",
    "<h3>üîë KEY INSIGHTS:</h3>\n",
    "<ul>\n",
    "<li>Average CPU utilization: {metrics['avg_cpu']:.0f}%</li>\n",
    "<li>Average memory utilization: {metrics['avg_mem']:.0f}%</li>\n",
    "<li>Telemetry coverage: {metrics['avg_telemetry']:.0f}%</li>\n",
    "<li>Most expensive cluster: ${metrics['top_cluster_cost']:,.0f} ({metrics['top_cluster_name']})</li>\n",
    "<li>Most expensive instance: {metrics['top_instance_type']} (${metrics['top_instance_cost']:,.0f} across {metrics['top_instance_clusters']} clusters)</li>\n",
    "</ul>\n",
    "\n",
    "<p>‚úÖ <b>ANALYSIS COMPLETE - CHANGE days_back WIDGET TO ANALYZE DIFFERENT PERIODS</b></p>\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "c97a0328-8d0e-4e29-beb5-750170bfa902",
     "origId": 3627202947959775,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    },
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "b523c7c4-08aa-4e19-abf9-790a2196a2dc",
     "origId": 3627202947959776,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7880672836907587,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "All-Purpose Cluster Cost Analysis & Optimization",
   "widgets": {
    "days_back": {
     "currentValue": "30",
     "nuid": "8d4069ba-ebcb-422b-bdb6-69db92758716",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "30",
      "label": "Days to Go Back",
      "name": "days_back",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "30",
      "label": "Days to Go Back",
      "name": "days_back",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
