{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5727498151607464,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab28f88-807d-4d01-90dc-7df365e07367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîÑ Cluster Configuration Revert Tool\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook provides a production-ready tool to revert cluster configurations to their state **before a selected batch update**. It uses the selected batch as a **reference point in time** and reverts ALL clusters in the environment to their configuration before that reference date.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "* ‚úÖ **Time-based Revert**: Select any batch as a reference point - all clusters revert to their state BEFORE that date\n",
    "* ‚úÖ **Complete Configuration Restore**: Restores instance types, policies, security modes, AWS attributes, elastic disk, auto-termination, and all other settings\n",
    "* ‚úÖ **Cross-Workspace Support**: Works across different workspaces using Service Principal authentication\n",
    "* ‚úÖ **Environment Selector**: Supports dev, qa, uat, and prod environments\n",
    "* ‚úÖ **Batch History**: View all batch updates with labels and statistics\n",
    "* ‚úÖ **Comprehensive Validation**: Pre-execution checks for authentication, compute, and environment readiness\n",
    "* ‚úÖ **Detailed Reporting**: Execution summary with success/failure tracking\n",
    "\n",
    "---\n",
    "\n",
    "## How It Works\n",
    "\n",
    "### Logic Flow:\n",
    "\n",
    "1. **Select Environment** (dev/qa/uat/prod) ‚Üí Determines which catalog and workspace to use\n",
    "2. **View Batch History** ‚Üí Shows all batch updates in the selected environment\n",
    "3. **Select Reference Batch** ‚Üí Choose which batch date to use as the revert reference point\n",
    "4. **Analyze Impact** ‚Üí Shows ALL clusters in the environment and what will be reverted\n",
    "5. **Execute Revert** ‚Üí Restores all clusters to their state BEFORE the reference date\n",
    "\n",
    "### Important Concepts:\n",
    "\n",
    "**Reference Point Logic:**\n",
    "* The selected batch is a **reference point in time**, not a filter\n",
    "* ALL clusters that have been updated in the environment will be reverted\n",
    "* Each cluster reverts to its last configuration BEFORE the reference date\n",
    "* Each cluster's previous config may be from different dates\n",
    "\n",
    "**Example:**\n",
    "* You select batch \"2025-12-10_20-11\" as reference\n",
    "* Cluster A was last configured on Dec 9 ‚Üí reverts to Dec 9 state\n",
    "* Cluster B was last configured on Nov 15 ‚Üí reverts to Nov 15 state\n",
    "* Cluster C was configured on Dec 11 ‚Üí reverts to its state before Dec 10 20:11\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Required Access:\n",
    "* ‚úÖ Read access to `{catalog}.billing_forecast.cluster_update_log` table\n",
    "* ‚úÖ Read access to `system.compute.clusters` table\n",
    "* ‚úÖ Service Principal credentials in `sp-oauth` secret scope (for cross-workspace)\n",
    "* ‚úÖ Classic compute cluster (required for cross-workspace API calls)\n",
    "\n",
    "### Required Secrets:\n",
    "* `sp-oauth` scope with keys:\n",
    "  * `client` - Service Principal Client ID\n",
    "  * `secret` - Service Principal Client Secret\n",
    "\n",
    "---\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### Step-by-Step:\n",
    "\n",
    "1. **Attach to Classic Compute** (e.g., \"Abhijit Joshi's multinode Cluster\")\n",
    "2. **Run Cell 2**: Select environment (dev/qa/uat/prod)\n",
    "3. **Run Cell 3**: View all batch updates in the environment\n",
    "4. **Run Cell 4**: Select which batch to use as reference point\n",
    "5. **Run Cell 5**: Analyze impact - see what will be reverted\n",
    "6. **Run Cell 6**: Execute the complete revert\n",
    "7. **Verify**: Check target workspace to confirm configurations\n",
    "\n",
    "---\n",
    "\n",
    "## Safety Features\n",
    "\n",
    "* üõ°Ô∏è **Pre-execution validation** of authentication and compute\n",
    "* üõ°Ô∏è **Detailed preview** of all changes before execution\n",
    "* üõ°Ô∏è **Per-cluster error handling** - one failure doesn't stop others\n",
    "* üõ°Ô∏è **Comprehensive logging** - tracks success/failure for each cluster\n",
    "* üõ°Ô∏è **Audit trail** - all operations logged with timestamps and users\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "‚ö†Ô∏è **This tool reverts ALL clusters in the environment, not just those in the selected batch**\n",
    "\n",
    "‚ö†Ô∏è **Each cluster reverts to its state BEFORE the reference date**\n",
    "\n",
    "‚ö†Ô∏è **Always verify results in the target workspace after execution**\n",
    "\n",
    "‚ö†Ô∏è **Cluster policies must exist and be accessible for successful revert**\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to begin? Run the cells in sequence starting with Cell 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5727498151607464,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d459b5e9-24cb-492a-aea9-bb5bf55e8d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ Quick Start Guide\n",
    "\n",
    "### Prerequisites Check\n",
    "\n",
    "Before running this notebook, ensure:\n",
    "\n",
    "1. ‚úÖ **Compute**: Attached to classic compute cluster (e.g., \"Abhijit Joshi's multinode Cluster\")\n",
    "   * ‚ùå Serverless compute will NOT work for cross-workspace operations\n",
    "   \n",
    "2. ‚úÖ **Secrets**: Service Principal credentials available in `sp-oauth` scope\n",
    "   * `client` - Service Principal Client ID\n",
    "   * `secret` - Service Principal Client Secret\n",
    "   \n",
    "3. ‚úÖ **Permissions**: \n",
    "   * Read access to `{catalog}.billing_forecast.cluster_update_log`\n",
    "   * Read access to `system.compute.clusters`\n",
    "   * Service Principal has `CAN_MANAGE` on target clusters\n",
    "\n",
    "---\n",
    "\n",
    "### Execution Steps\n",
    "\n",
    "| Step | Cell | Action | Description |\n",
    "|------|------|--------|-------------|\n",
    "| 1 | Cell 2 | **Select Environment** | Choose dev/qa/uat/prod |\n",
    "| 2 | Cell 3 | **View Batch History** | See all batch updates |\n",
    "| 3 | Cell 4 | **Select Reference Batch** | Choose time reference point |\n",
    "| 4 | Cell 5 | **Analyze Impact** | Preview what will be reverted |\n",
    "| 5 | Cell 6 | **Execute Revert** | Perform the revert operation |\n",
    "| 6 | Cell 7 | **Verify Results** | Check execution summary |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Important Warnings\n",
    "\n",
    "* **This reverts ALL clusters in the environment**, not just those in the selected batch\n",
    "* **Each cluster reverts to its state BEFORE the reference date**\n",
    "* **Cluster policies must exist** in the target workspace\n",
    "* **Always verify results** in the target workspace after execution\n",
    "* **Cannot be undone** - document before executing\n",
    "\n",
    "---\n",
    "\n",
    "**Ready? Start with Cell 2 below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af08ea73-d78a-45ba-9562-7e62a9368bf7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration: Select Environment"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION: ENVIRONMENT SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "import requests\n",
    "from pyspark.sql.functions import col, lit, count, sum as spark_sum, max as spark_max, min as spark_min\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLUSTER CONFIGURATION REVERT TOOL - CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create environment selection widget\n",
    "dbutils.widgets.dropdown(\n",
    "    name=\"environment\",\n",
    "    defaultValue=\"dev_sandbox\",\n",
    "    choices=[\"dev_sandbox\", \"qa_sandbox\", \"uat_sandbox\", \"prod_sandbox\"],\n",
    "    label=\"Select Environment\"\n",
    ")\n",
    "\n",
    "# Get selected environment\n",
    "selected_environment = dbutils.widgets.get(\"environment\")\n",
    "\n",
    "# Environment configuration mapping\n",
    "env_config = {\n",
    "    \"dev_sandbox\": {\n",
    "        \"catalog\": \"dev_sandbox\",\n",
    "        \"workspace_mapping\": {\n",
    "            \"Integrated-Dev\": \"oportun-integrated-dev.cloud.databricks.com\"\n",
    "        }\n",
    "    },\n",
    "    \"qa_sandbox\": {\n",
    "        \"catalog\": \"qa_sandbox\",\n",
    "        \"workspace_mapping\": {\n",
    "            \"QA\": \"oportun-qa.cloud.databricks.com\"\n",
    "        }\n",
    "    },\n",
    "    \"uat_sandbox\": {\n",
    "        \"catalog\": \"uat_sandbox\",\n",
    "        \"workspace_mapping\": {\n",
    "            \"UAT\": \"oportun-uat.cloud.databricks.com\"\n",
    "        }\n",
    "    },\n",
    "    \"prod_sandbox\": {\n",
    "        \"catalog\": \"prod_sandbox\",\n",
    "        \"workspace_mapping\": {\n",
    "            \"Prod\": \"oportun-prod.cloud.databricks.com\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get configuration for selected environment\n",
    "selected_catalog = env_config[selected_environment][\"catalog\"]\n",
    "workspace_mapping = env_config[selected_environment][\"workspace_mapping\"]\n",
    "\n",
    "print(f\"\\n‚úÖ Environment Configuration:\")\n",
    "print(f\"   Selected: {selected_environment}\")\n",
    "print(f\"   Catalog: {selected_catalog}\")\n",
    "print(f\"   Target Workspaces: {', '.join(workspace_mapping.keys())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ CONFIGURATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext: Run Cell 3 to view batch history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4eb0e4-711d-44d5-b88c-5a5eb1066a68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get All Batch Updates for Selected Environment"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GET ALL BATCH UPDATES FOR SELECTED ENVIRONMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH UPDATES HISTORY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Environment: {selected_environment}\")\n",
    "print(f\"Catalog: {selected_catalog}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Get all batch updates from the cluster_update_log table\n",
    "all_batches = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        batch_id,\n",
    "        batch_start_time,\n",
    "        batch_end_time,\n",
    "        execution_label,\n",
    "        executed_by_user,\n",
    "        COUNT(*) as total_updates,\n",
    "        SUM(CASE WHEN update_status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_updates,\n",
    "        SUM(CASE WHEN update_status = 'FAILED' THEN 1 ELSE 0 END) as failed_updates,\n",
    "        SUM(CASE WHEN dry_run = false THEN 1 ELSE 0 END) as actual_updates,\n",
    "        SUM(CASE WHEN dry_run = true THEN 1 ELSE 0 END) as dry_run_updates,\n",
    "        COUNT(DISTINCT workspace_name) as workspace_count,\n",
    "        COLLECT_SET(workspace_name) as workspaces\n",
    "    FROM {selected_catalog}.billing_forecast.cluster_update_log\n",
    "    GROUP BY batch_id, batch_start_time, batch_end_time, execution_label, executed_by_user\n",
    "    ORDER BY batch_end_time DESC\n",
    "    LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "batch_count = all_batches.count()\n",
    "\n",
    "if batch_count == 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  No batch updates found in {selected_catalog}.billing_forecast.cluster_update_log\")\n",
    "    print(\"\\nPlease verify:\")\n",
    "    print(\"  1. The table exists and has data\")\n",
    "    print(\"  2. You have permissions to read the table\")\n",
    "    print(\"  3. The selected environment is correct\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Found {batch_count} batch updates in {selected_environment}\\n\")\n",
    "    \n",
    "    # Display all batches\n",
    "    print(\"üìã Available Batch Updates:\")\n",
    "    display(all_batches.select(\n",
    "        \"batch_id\",\n",
    "        \"execution_label\",\n",
    "        \"batch_end_time\",\n",
    "        \"executed_by_user\",\n",
    "        \"total_updates\",\n",
    "        \"successful_updates\",\n",
    "        \"failed_updates\",\n",
    "        \"actual_updates\",\n",
    "        \"workspace_count\",\n",
    "        \"workspaces\"\n",
    "    ))\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"Total batches available: {batch_count}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n‚úÖ Batch information loaded and ready for selection\")\n",
    "    print(\"\\nNext: Run Cell 4 to select which batch to use as reference point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec76b74-f296-4a1a-9d1b-f71b3e8a6f4b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Select Batch to Revert"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SELECT BATCH REFERENCE POINT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating batch selection dropdown...\\n\")\n",
    "\n",
    "# Get batch labels for dropdown (only actual updates, not dry runs)\n",
    "actual_batches = all_batches.filter(\"actual_updates > 0\").collect()\n",
    "\n",
    "if len(actual_batches) == 0:\n",
    "    print(\"‚ö†Ô∏è  No actual batch updates found (all were dry runs)\")\n",
    "    print(\"\\nPlease run a live batch update first before using the revert tool.\")\n",
    "else:\n",
    "    # Create list of batch labels\n",
    "    batch_choices = []\n",
    "    for row in actual_batches:\n",
    "        label = row['execution_label'] if row['execution_label'] else f\"Batch_{row['batch_end_time']}\"\n",
    "        batch_choices.append(label)\n",
    "    \n",
    "    # Create dropdown widget\n",
    "    dbutils.widgets.dropdown(\n",
    "        name=\"batch_to_revert\",\n",
    "        defaultValue=batch_choices[0],  # Default to most recent batch\n",
    "        choices=batch_choices,\n",
    "        label=\"Select Batch Reference Point\"\n",
    "    )\n",
    "    \n",
    "    # Get selected batch\n",
    "    selected_batch_label = dbutils.widgets.get(\"batch_to_revert\")\n",
    "    \n",
    "    # Find the batch info for selected label\n",
    "    selected_batch_info = None\n",
    "    for row in actual_batches:\n",
    "        label = row['execution_label'] if row['execution_label'] else f\"Batch_{row['batch_end_time']}\"\n",
    "        if label == selected_batch_label:\n",
    "            selected_batch_info = row\n",
    "            break\n",
    "    \n",
    "    if selected_batch_info:\n",
    "        print(\"=\"*80)\n",
    "        print(\"SELECTED BATCH REFERENCE POINT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Label: {selected_batch_label}\")\n",
    "        print(f\"Batch ID: {selected_batch_info['batch_id']}\")\n",
    "        print(f\"Executed By: {selected_batch_info['executed_by_user']}\")\n",
    "        print(f\"Reference Time: {selected_batch_info['batch_start_time']}\")\n",
    "        print(f\"Batch End Time: {selected_batch_info['batch_end_time']}\")\n",
    "        print(f\"Total Updates in Batch: {selected_batch_info['total_updates']}\")\n",
    "        print(f\"Successful: {selected_batch_info['successful_updates']}\")\n",
    "        print(f\"Failed: {selected_batch_info['failed_updates']}\")\n",
    "        print(f\"Workspaces: {', '.join(selected_batch_info['workspaces'])}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Store selected batch details for use in subsequent cells\n",
    "        selected_batch_id = selected_batch_info['batch_id']\n",
    "        selected_batch_start_time = selected_batch_info['batch_start_time']\n",
    "        selected_batch_end_time = selected_batch_info['batch_end_time']\n",
    "        \n",
    "        print(\"\\n‚úÖ Batch reference point selected successfully\")\n",
    "        print(\"\\nüí° Important: ALL clusters in the environment will be reverted\")\n",
    "        print(f\"   to their state BEFORE {selected_batch_start_time}\")\n",
    "        print(\"\\nNext: Run Cell 5 to analyze impact and see what will be reverted\")\n",
    "    else:\n",
    "        print(\"‚ùå Could not find selected batch information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adbc4d6c-579e-4730-936d-25f400ea0b3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyze Selected Batch and Get Previous Configurations"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYZE IMPACT - PREVIEW CHANGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç Analyzing impact of revert operation...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Reference Batch: {selected_batch_label}\")\n",
    "print(f\"Reference Date: {selected_batch_start_time}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüí° Logic: Revert ALL clusters in environment to their state BEFORE reference date\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Get ALL clusters that have been updated in the selected environment\n",
    "print(\"\\nüîç Step 1: Identifying all clusters in environment...\\n\")\n",
    "\n",
    "all_updated_clusters = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        cluster_id,\n",
    "        cluster_name,\n",
    "        workspace_name,\n",
    "        workspace_id,\n",
    "        deployment_url\n",
    "    FROM {selected_catalog}.billing_forecast.cluster_update_log\n",
    "    WHERE update_status = 'SUCCESS'\n",
    "        AND dry_run = false\n",
    "    ORDER BY workspace_name, cluster_name\n",
    "\"\"\")\n",
    "\n",
    "all_cluster_count = all_updated_clusters.count()\n",
    "\n",
    "if all_cluster_count == 0:\n",
    "    print(\"‚ö†Ô∏è  No clusters found in cluster_update_log for this environment\")\n",
    "    print(\"\\nThis environment has no cluster update history.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Found {all_cluster_count} clusters in {selected_environment}\\n\")\n",
    "    \n",
    "    # Show cluster summary\n",
    "    print(\"üìÑ All clusters that will be reverted:\")\n",
    "    display(all_updated_clusters)\n",
    "    \n",
    "    # Get cluster IDs for system table queries\n",
    "    all_cluster_ids = [row['cluster_id'] for row in all_updated_clusters.collect()]\n",
    "    all_cluster_ids_str = \"', '\".join(all_cluster_ids)\n",
    "    \n",
    "    # Step 2: For each cluster, get the last configuration BEFORE the reference date\n",
    "    print(f\"\\nüîç Step 2: Retrieving configurations BEFORE {selected_batch_start_time}...\")\n",
    "    print(\"   (Each cluster's previous config may be from different dates)\\n\")\n",
    "    \n",
    "    previous_configs_batch = spark.sql(f\"\"\"\n",
    "        WITH configs_before_reference AS (\n",
    "            SELECT \n",
    "                cluster_id,\n",
    "                cluster_name,\n",
    "                driver_node_type,\n",
    "                worker_node_type,\n",
    "                min_autoscale_workers,\n",
    "                max_autoscale_workers,\n",
    "                worker_count,\n",
    "                policy_id,\n",
    "                data_security_mode,\n",
    "                dbr_version,\n",
    "                auto_termination_minutes,\n",
    "                enable_elastic_disk,\n",
    "                driver_instance_pool_id,\n",
    "                worker_instance_pool_id,\n",
    "                aws_attributes,\n",
    "                change_time,\n",
    "                owned_by,\n",
    "                ROW_NUMBER() OVER (PARTITION BY cluster_id ORDER BY change_time DESC) as rn\n",
    "            FROM system.compute.clusters\n",
    "            WHERE cluster_id IN ('{all_cluster_ids_str}')\n",
    "                AND change_time < timestamp'{selected_batch_start_time}'\n",
    "                AND delete_time IS NULL\n",
    "        )\n",
    "        SELECT \n",
    "            cluster_id,\n",
    "            cluster_name,\n",
    "            driver_node_type as prev_driver,\n",
    "            worker_node_type as prev_worker,\n",
    "            min_autoscale_workers as prev_min_workers,\n",
    "            max_autoscale_workers as prev_max_workers,\n",
    "            worker_count as prev_worker_count,\n",
    "            policy_id as prev_policy_id,\n",
    "            data_security_mode as prev_data_security_mode,\n",
    "            dbr_version as prev_dbr_version,\n",
    "            auto_termination_minutes as prev_auto_termination,\n",
    "            enable_elastic_disk as prev_enable_elastic_disk,\n",
    "            driver_instance_pool_id as prev_driver_pool,\n",
    "            worker_instance_pool_id as prev_worker_pool,\n",
    "            aws_attributes as prev_aws_attributes,\n",
    "            change_time as prev_config_time,\n",
    "            owned_by as prev_owned_by\n",
    "        FROM configs_before_reference\n",
    "        WHERE rn = 1\n",
    "        ORDER BY cluster_name\n",
    "    \"\"\")\n",
    "    \n",
    "    prev_count_batch = previous_configs_batch.count()\n",
    "    \n",
    "    if prev_count_batch == 0:\n",
    "        print(f\"‚ùå No configurations found before reference date {selected_batch_start_time}\")\n",
    "        print(\"   This could mean:\")\n",
    "        print(\"   - Clusters were created after the reference date\")\n",
    "        print(\"   - System table data doesn't go back that far\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Found previous configurations for {prev_count_batch} clusters\")\n",
    "        \n",
    "        print(\"\\nüìÑ Previous Configurations (Target State):\")\n",
    "        display(previous_configs_batch.select(\n",
    "            \"cluster_id\", \"cluster_name\",\n",
    "            \"prev_driver\", \"prev_worker\",\n",
    "            \"prev_policy_id\", \"prev_data_security_mode\",\n",
    "            \"prev_enable_elastic_disk\", \"prev_auto_termination\",\n",
    "            \"prev_config_time\", \"prev_owned_by\"\n",
    "        ))\n",
    "        \n",
    "        # Get current configurations\n",
    "        print(\"\\nüîç Step 3: Retrieving current configurations...\")\n",
    "        \n",
    "        current_configs_batch = spark.sql(f\"\"\"\n",
    "            WITH ranked AS (\n",
    "                SELECT \n",
    "                    cluster_id,\n",
    "                    cluster_name,\n",
    "                    driver_node_type,\n",
    "                    worker_node_type,\n",
    "                    min_autoscale_workers,\n",
    "                    max_autoscale_workers,\n",
    "                    worker_count,\n",
    "                    policy_id,\n",
    "                    data_security_mode,\n",
    "                    auto_termination_minutes,\n",
    "                    enable_elastic_disk,\n",
    "                    change_time,\n",
    "                    ROW_NUMBER() OVER (PARTITION BY cluster_id ORDER BY change_time DESC) as rn\n",
    "                FROM system.compute.clusters\n",
    "                WHERE cluster_id IN ('{all_cluster_ids_str}')\n",
    "                    AND delete_time IS NULL\n",
    "            )\n",
    "            SELECT \n",
    "                cluster_id,\n",
    "                cluster_name,\n",
    "                driver_node_type as curr_driver,\n",
    "                worker_node_type as curr_worker,\n",
    "                min_autoscale_workers as curr_min_workers,\n",
    "                max_autoscale_workers as curr_max_workers,\n",
    "                worker_count as curr_worker_count,\n",
    "                policy_id as curr_policy_id,\n",
    "                data_security_mode as curr_data_security_mode,\n",
    "                auto_termination_minutes as curr_auto_termination,\n",
    "                enable_elastic_disk as curr_enable_elastic_disk,\n",
    "                change_time as curr_config_time\n",
    "            FROM ranked\n",
    "            WHERE rn = 1\n",
    "            ORDER BY cluster_name\n",
    "        \"\"\")\n",
    "        \n",
    "        print(f\"‚úÖ Found current configurations for {current_configs_batch.count()} clusters\")\n",
    "        \n",
    "        # Create revert plan\n",
    "        print(\"\\nüîç Step 4: Creating comprehensive revert plan...\")\n",
    "        \n",
    "        revert_plan_batch = previous_configs_batch.alias(\"prev\").join(\n",
    "            current_configs_batch.alias(\"curr\"),\n",
    "            col(\"prev.cluster_id\") == col(\"curr.cluster_id\"),\n",
    "            \"inner\"\n",
    "        ).select(\n",
    "            col(\"prev.cluster_id\"),\n",
    "            col(\"prev.cluster_name\"),\n",
    "            # Target (previous) configuration\n",
    "            col(\"prev.prev_driver\").alias(\"target_driver\"),\n",
    "            col(\"prev.prev_worker\").alias(\"target_worker\"),\n",
    "            col(\"prev.prev_min_workers\").alias(\"target_min_workers\"),\n",
    "            col(\"prev.prev_max_workers\").alias(\"target_max_workers\"),\n",
    "            col(\"prev.prev_worker_count\").alias(\"target_worker_count\"),\n",
    "            col(\"prev.prev_policy_id\").alias(\"target_policy_id\"),\n",
    "            col(\"prev.prev_data_security_mode\").alias(\"target_data_security_mode\"),\n",
    "            col(\"prev.prev_auto_termination\").alias(\"target_auto_termination\"),\n",
    "            col(\"prev.prev_enable_elastic_disk\").alias(\"target_enable_elastic_disk\"),\n",
    "            col(\"prev.prev_driver_pool\").alias(\"target_driver_pool\"),\n",
    "            col(\"prev.prev_worker_pool\").alias(\"target_worker_pool\"),\n",
    "            col(\"prev.prev_aws_attributes\").alias(\"target_aws_attributes\"),\n",
    "            col(\"prev.prev_config_time\").alias(\"target_config_time\"),\n",
    "            col(\"prev.prev_owned_by\").alias(\"target_owned_by\"),\n",
    "            # Current configuration\n",
    "            col(\"curr.curr_driver\"),\n",
    "            col(\"curr.curr_worker\"),\n",
    "            col(\"curr.curr_policy_id\"),\n",
    "            col(\"curr.curr_data_security_mode\"),\n",
    "            col(\"curr.curr_enable_elastic_disk\"),\n",
    "            col(\"curr.curr_auto_termination\"),\n",
    "            col(\"curr.curr_config_time\")\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Created revert plan for {revert_plan_batch.count()} clusters\")\n",
    "        \n",
    "        print(\"\\nüìÑ Revert Plan (Current ‚Üí Target):\")\n",
    "        display(revert_plan_batch.select(\n",
    "            \"cluster_id\", \"cluster_name\",\n",
    "            \"curr_driver\", \"target_driver\",\n",
    "            \"curr_worker\", \"target_worker\",\n",
    "            \"curr_policy_id\", \"target_policy_id\",\n",
    "            \"curr_data_security_mode\", \"target_data_security_mode\",\n",
    "            \"curr_enable_elastic_disk\", \"target_enable_elastic_disk\",\n",
    "            \"curr_auto_termination\", \"target_auto_termination\",\n",
    "            \"curr_config_time\", \"target_config_time\", \"target_owned_by\"\n",
    "        ))\n",
    "        \n",
    "        # Analyze changes\n",
    "        policy_changes_count = revert_plan_batch.filter(\n",
    "            (col(\"curr_policy_id\").isNull() & col(\"target_policy_id\").isNotNull()) |\n",
    "            (col(\"curr_policy_id\").isNotNull() & col(\"target_policy_id\").isNull()) |\n",
    "            (col(\"curr_policy_id\") != col(\"target_policy_id\"))\n",
    "        ).count()\n",
    "        \n",
    "        security_changes_count = revert_plan_batch.filter(\n",
    "            (col(\"curr_data_security_mode\").isNull() & col(\"target_data_security_mode\").isNotNull()) |\n",
    "            (col(\"curr_data_security_mode\").isNotNull() & col(\"target_data_security_mode\").isNull()) |\n",
    "            (col(\"curr_data_security_mode\") != col(\"target_data_security_mode\"))\n",
    "        ).count()\n",
    "        \n",
    "        instance_changes_count = revert_plan_batch.filter(\n",
    "            (col(\"curr_driver\") != col(\"target_driver\")) |\n",
    "            (col(\"curr_worker\") != col(\"target_worker\"))\n",
    "        ).count()\n",
    "        \n",
    "        elastic_disk_changes = revert_plan_batch.filter(\n",
    "            col(\"curr_enable_elastic_disk\") != col(\"target_enable_elastic_disk\")\n",
    "        ).count()\n",
    "        \n",
    "        auto_term_changes = revert_plan_batch.filter(\n",
    "            (col(\"curr_auto_termination\").isNull() & col(\"target_auto_termination\").isNotNull()) |\n",
    "            (col(\"curr_auto_termination\").isNotNull() & col(\"target_auto_termination\").isNull()) |\n",
    "            (col(\"curr_auto_termination\") != col(\"target_auto_termination\"))\n",
    "        ).count()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ö†Ô∏è  IMPACT ANALYSIS - CHANGES TO BE REVERTED\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"   ‚Ä¢ Cluster Policy Changes: {policy_changes_count} clusters\")\n",
    "        print(f\"   ‚Ä¢ Data Security Mode Changes: {security_changes_count} clusters\")\n",
    "        print(f\"   ‚Ä¢ Instance Type Changes: {instance_changes_count} clusters\")\n",
    "        print(f\"   ‚Ä¢ Elastic Disk Changes: {elastic_disk_changes} clusters\")\n",
    "        print(f\"   ‚Ä¢ Auto Termination Changes: {auto_term_changes} clusters\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if policy_changes_count > 0 or security_changes_count > 0:\n",
    "            print(\"\\n‚ùå CRITICAL: Security/governance settings will be restored!\")\n",
    "            print(\"   This is important for compliance and access controls.\")\n",
    "        \n",
    "        print(f\"\\nüí° Summary:\")\n",
    "        print(f\"   ‚Ä¢ Total clusters to revert: {all_cluster_count}\")\n",
    "        print(f\"   ‚Ä¢ Clusters with previous configs: {prev_count_batch}\")\n",
    "        print(f\"   ‚Ä¢ Reference point: {selected_batch_start_time}\")\n",
    "        print(f\"   ‚Ä¢ Each cluster reverts to its last state BEFORE reference date\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if all_cluster_count > 0 and prev_count_batch > 0:\n",
    "    print(f\"‚úÖ ANALYSIS COMPLETE: Ready to revert {prev_count_batch} clusters\")\n",
    "    print(\"\\nNext: Run Cell 6 to execute the revert operation\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ANALYSIS INCOMPLETE: Cannot proceed with revert\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27dd4138-93de-4f10-bca8-5ac0e0803aa1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute Complete Revert for Selected Batch"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTE COMPLETE REVERT OPERATION\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nüöÄ EXECUTING CLUSTER CONFIGURATION REVERT...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Selected Batch: {selected_batch_label}\")\n",
    "print(f\"Reference Date: {selected_batch_start_time}\")\n",
    "print(f\"Environment: {selected_environment}\")\n",
    "print(f\"Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if all_cluster_count == 0 or prev_count_batch == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Cannot proceed - no clusters or previous configurations available\")\n",
    "    print(\"\\nPlease run Cell 5 first to analyze the impact.\")\n",
    "else:\n",
    "    try:\n",
    "        # Setup authentication\n",
    "        print(\"\\nüîê Step 1: Setting up authentication...\")\n",
    "        \n",
    "        # Get target workspace from all_updated_clusters\n",
    "        target_ws_info = all_updated_clusters.select(\"workspace_name\", \"deployment_url\").first()\n",
    "        target_workspace_url_exec = target_ws_info['deployment_url'].replace('https://', '')\n",
    "        target_workspace_name_exec = target_ws_info['workspace_name']\n",
    "        \n",
    "        current_workspace_url_exec = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "        \n",
    "        # Check if cross-workspace\n",
    "        if current_workspace_url_exec.lower() != target_workspace_url_exec.lower():\n",
    "            # Cross-workspace - use Service Principal\n",
    "            print(f\"   Cross-workspace operation detected\")\n",
    "            print(f\"   Current: {current_workspace_url_exec}\")\n",
    "            print(f\"   Target: {target_workspace_url_exec}\")\n",
    "            \n",
    "            sp_client_id = dbutils.secrets.get(scope=\"sp-oauth\", key=\"client\")\n",
    "            sp_client_secret = dbutils.secrets.get(scope=\"sp-oauth\", key=\"secret\")\n",
    "            token_url = f\"https://{target_workspace_url_exec}/oidc/v1/token\"\n",
    "            token_data = {\"grant_type\": \"client_credentials\", \"scope\": \"all-apis\"}\n",
    "            token_response = requests.post(token_url, auth=(sp_client_id, sp_client_secret), data=token_data, timeout=30)\n",
    "            \n",
    "            if token_response.status_code != 200:\n",
    "                raise Exception(f\"OAuth authentication failed: {token_response.text}\")\n",
    "            \n",
    "            access_token = token_response.json()['access_token']\n",
    "            api_base_url = f\"https://{target_workspace_url_exec}\"\n",
    "            print(f\"\\n‚úÖ Cross-workspace OAuth authentication successful\")\n",
    "        else:\n",
    "            # Same workspace\n",
    "            access_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "            api_base_url = f\"https://{current_workspace_url_exec}\"\n",
    "            print(f\"\\n‚úÖ Same-workspace authentication successful\")\n",
    "        \n",
    "        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        # Execute revert for each cluster\n",
    "        print(f\"\\nüîÑ Step 2: Reverting {prev_count_batch} cluster configurations...\\n\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        revert_results_final = []\n",
    "        revert_configs = revert_plan_batch.collect()\n",
    "        \n",
    "        for idx, config in enumerate(revert_configs, 1):\n",
    "            cluster_id = config['cluster_id']\n",
    "            cluster_name = config['cluster_name']\n",
    "            \n",
    "            print(f\"\\n[{idx}/{len(revert_configs)}] {cluster_name}\")\n",
    "            print(f\"   Cluster ID: {cluster_id}\")\n",
    "            \n",
    "            # Identify changes\n",
    "            changes = []\n",
    "            if config['curr_driver'] != config['target_driver']:\n",
    "                changes.append(f\"Driver: {config['curr_driver']} ‚Üí {config['target_driver']}\")\n",
    "            if config['curr_worker'] != config['target_worker']:\n",
    "                changes.append(f\"Worker: {config['curr_worker']} ‚Üí {config['target_worker']}\")\n",
    "            if config['curr_policy_id'] != config['target_policy_id']:\n",
    "                changes.append(f\"Policy: {config['curr_policy_id']} ‚Üí {config['target_policy_id']}\")\n",
    "            if config['curr_data_security_mode'] != config['target_data_security_mode']:\n",
    "                changes.append(f\"Security: {config['curr_data_security_mode']} ‚Üí {config['target_data_security_mode']}\")\n",
    "            if config['curr_enable_elastic_disk'] != config['target_enable_elastic_disk']:\n",
    "                changes.append(f\"Elastic Disk: {config['curr_enable_elastic_disk']} ‚Üí {config['target_enable_elastic_disk']}\")\n",
    "            if config['curr_auto_termination'] != config['target_auto_termination']:\n",
    "                changes.append(f\"Auto Term: {config['curr_auto_termination']} ‚Üí {config['target_auto_termination']}\")\n",
    "            \n",
    "            if changes:\n",
    "                print(f\"   Changes ({len(changes)}):\")\n",
    "                for change in changes:\n",
    "                    print(f\"      ‚Ä¢ {change}\")\n",
    "            else:\n",
    "                print(\"   ‚ÑπÔ∏è  No changes needed (already in target state)\")\n",
    "            \n",
    "            try:\n",
    "                # Get current cluster config from API\n",
    "                get_url = f\"{api_base_url}/api/2.0/clusters/get?cluster_id={cluster_id}\"\n",
    "                get_response = requests.get(get_url, headers=headers, timeout=30)\n",
    "                \n",
    "                if get_response.status_code != 200:\n",
    "                    error_msg = f\"Failed to get cluster: {get_response.text[:200]}\"\n",
    "                    print(f\"   ‚ùå {error_msg}\")\n",
    "                    revert_results_final.append({\n",
    "                        'cluster_id': cluster_id, \n",
    "                        'cluster_name': cluster_name, \n",
    "                        'status': 'FAILED', \n",
    "                        'message': error_msg,\n",
    "                        'changes_count': len(changes)\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                cluster_config = get_response.json()\n",
    "                \n",
    "                # Build edit request with COMPLETE previous configuration\n",
    "                edit_request = {\n",
    "                    'cluster_id': cluster_id,\n",
    "                    'spark_version': cluster_config['spark_version'],\n",
    "                    'node_type_id': config['target_driver'],\n",
    "                    'driver_node_type_id': config['target_driver']\n",
    "                }\n",
    "                \n",
    "                # Restore autoscale or fixed workers\n",
    "                if config['target_min_workers'] is not None and config['target_max_workers'] is not None:\n",
    "                    edit_request['autoscale'] = {\n",
    "                        'min_workers': int(config['target_min_workers']), \n",
    "                        'max_workers': int(config['target_max_workers'])\n",
    "                    }\n",
    "                elif config['target_worker_count'] is not None:\n",
    "                    edit_request['num_workers'] = int(config['target_worker_count'])\n",
    "                \n",
    "                # Restore policy\n",
    "                if config['target_policy_id'] is not None:\n",
    "                    edit_request['policy_id'] = config['target_policy_id']\n",
    "                \n",
    "                # Restore data security mode\n",
    "                if config['target_data_security_mode'] is not None:\n",
    "                    edit_request['data_security_mode'] = config['target_data_security_mode']\n",
    "                \n",
    "                # Restore elastic disk\n",
    "                if config['target_enable_elastic_disk'] is not None:\n",
    "                    edit_request['enable_elastic_disk'] = bool(config['target_enable_elastic_disk'])\n",
    "                \n",
    "                # Restore auto termination\n",
    "                if config['target_auto_termination'] is not None:\n",
    "                    edit_request['autotermination_minutes'] = int(config['target_auto_termination'])\n",
    "                \n",
    "                # Restore instance pools\n",
    "                if config['target_driver_pool'] is not None:\n",
    "                    edit_request['driver_instance_pool_id'] = config['target_driver_pool']\n",
    "                if config['target_worker_pool'] is not None:\n",
    "                    edit_request['instance_pool_id'] = config['target_worker_pool']\n",
    "                \n",
    "                # Restore AWS attributes from previous state (CRITICAL for policy compliance)\n",
    "                if config['target_aws_attributes'] is not None:\n",
    "                    aws_attrs = config['target_aws_attributes']\n",
    "                    aws_attrs_dict = {}\n",
    "                    \n",
    "                    if aws_attrs.first_on_demand is not None:\n",
    "                        aws_attrs_dict['first_on_demand'] = int(aws_attrs.first_on_demand)\n",
    "                    if aws_attrs.availability:\n",
    "                        aws_attrs_dict['availability'] = aws_attrs.availability\n",
    "                    if aws_attrs.zone_id:\n",
    "                        aws_attrs_dict['zone_id'] = aws_attrs.zone_id\n",
    "                    if aws_attrs.instance_profile_arn:\n",
    "                        aws_attrs_dict['instance_profile_arn'] = aws_attrs.instance_profile_arn\n",
    "                    if aws_attrs.spot_bid_price_percent is not None:\n",
    "                        aws_attrs_dict['spot_bid_price_percent'] = int(aws_attrs.spot_bid_price_percent)\n",
    "                    if aws_attrs.ebs_volume_type:\n",
    "                        aws_attrs_dict['ebs_volume_type'] = aws_attrs.ebs_volume_type\n",
    "                    if aws_attrs.ebs_volume_count is not None:\n",
    "                        aws_attrs_dict['ebs_volume_count'] = int(aws_attrs.ebs_volume_count)\n",
    "                    if aws_attrs.ebs_volume_size is not None:\n",
    "                        aws_attrs_dict['ebs_volume_size'] = int(aws_attrs.ebs_volume_size)\n",
    "                    \n",
    "                    if aws_attrs_dict:\n",
    "                        edit_request['aws_attributes'] = aws_attrs_dict\n",
    "                \n",
    "                # Preserve user-specific fields from current config\n",
    "                for field in ['cluster_name', 'spark_conf', 'custom_tags', 'single_user_name', 'runtime_engine']:\n",
    "                    if field in cluster_config:\n",
    "                        edit_request[field] = cluster_config[field]\n",
    "                \n",
    "                # Execute cluster edit\n",
    "                edit_url = f\"{api_base_url}/api/2.0/clusters/edit\"\n",
    "                edit_response = requests.post(edit_url, headers=headers, json=edit_request, timeout=30)\n",
    "                \n",
    "                if edit_response.status_code == 200:\n",
    "                    print(f\"   ‚úÖ Successfully reverted ({len(changes)} changes)\")\n",
    "                    revert_results_final.append({\n",
    "                        'cluster_id': cluster_id, \n",
    "                        'cluster_name': cluster_name, \n",
    "                        'status': 'SUCCESS', \n",
    "                        'message': f'Reverted {len(changes)} settings',\n",
    "                        'changes_count': len(changes)\n",
    "                    })\n",
    "                else:\n",
    "                    error_msg = f\"API error: {edit_response.text[:200]}\"\n",
    "                    print(f\"   ‚ùå {error_msg}\")\n",
    "                    revert_results_final.append({\n",
    "                        'cluster_id': cluster_id, \n",
    "                        'cluster_name': cluster_name, \n",
    "                        'status': 'FAILED', \n",
    "                        'message': error_msg,\n",
    "                        'changes_count': len(changes)\n",
    "                    })\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Exception: {str(e)[:200]}\"\n",
    "                print(f\"   ‚ùå {error_msg}\")\n",
    "                revert_results_final.append({\n",
    "                    'cluster_id': cluster_id, \n",
    "                    'cluster_name': cluster_name, \n",
    "                    'status': 'FAILED', \n",
    "                    'message': error_msg,\n",
    "                    'changes_count': len(changes) if 'changes' in locals() else 0\n",
    "                })\n",
    "            \n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"üìà REVERT EXECUTION SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        results_df_final = spark.createDataFrame(revert_results_final)\n",
    "        display(results_df_final)\n",
    "        \n",
    "        success_count = len([r for r in revert_results_final if r['status'] == 'SUCCESS'])\n",
    "        failed_count = len([r for r in revert_results_final if r['status'] == 'FAILED'])\n",
    "        total_changes = sum([r['changes_count'] for r in revert_results_final])\n",
    "        \n",
    "        print(f\"\\nüìä STATISTICS:\")\n",
    "        print(f\"   Total Clusters: {len(revert_results_final)}\")\n",
    "        print(f\"   ‚úÖ Successful: {success_count}\")\n",
    "        print(f\"   ‚ùå Failed: {failed_count}\")\n",
    "        print(f\"   Total Changes Applied: {total_changes}\")\n",
    "        print(f\"   Success Rate: {(success_count/len(revert_results_final)*100):.1f}%\")\n",
    "        \n",
    "        if success_count == len(revert_results_final):\n",
    "            print(f\"\\nüéâ ALL CLUSTERS SUCCESSFULLY REVERTED!\")\n",
    "            print(f\"\\n‚úÖ Restored Configuration Elements:\")\n",
    "            print(f\"   ‚Ä¢ Instance types (driver & worker)\")\n",
    "            print(f\"   ‚Ä¢ Autoscale/worker count settings\")\n",
    "            print(f\"   ‚Ä¢ Cluster policies (governance)\")\n",
    "            print(f\"   ‚Ä¢ Data security modes (compliance)\")\n",
    "            print(f\"   ‚Ä¢ AWS attributes (policy-compliant)\")\n",
    "            print(f\"   ‚Ä¢ Auto-termination settings\")\n",
    "            print(f\"   ‚Ä¢ Elastic disk settings\")\n",
    "            print(f\"\\nüîç NEXT STEPS:\")\n",
    "            print(f\"   1. Verify configurations in workspace: {target_workspace_url_exec}\")\n",
    "            print(f\"   2. Confirm cluster policies are properly applied\")\n",
    "            print(f\"   3. Test cluster functionality if needed\")\n",
    "            print(f\"   4. Document the revert operation for audit purposes\")\n",
    "        elif success_count > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  PARTIAL SUCCESS: {success_count}/{len(revert_results_final)} clusters reverted\")\n",
    "            print(f\"\\nFailed clusters:\")\n",
    "            for result in revert_results_final:\n",
    "                if result['status'] == 'FAILED':\n",
    "                    print(f\"   ‚Ä¢ {result['cluster_name']}\")\n",
    "                    print(f\"     Error: {result['message'][:150]}\")\n",
    "            print(f\"\\nüîç Review the errors above and check:\")\n",
    "            print(f\"   1. Cluster policies exist and are accessible\")\n",
    "            print(f\"   2. Service Principal has edit permissions\")\n",
    "            print(f\"   3. Clusters are not terminated or deleted\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå NO CLUSTERS WERE REVERTED\")\n",
    "            print(f\"\\nPlease review the errors above and verify:\")\n",
    "            print(f\"   1. Authentication is working correctly\")\n",
    "            print(f\"   2. Target workspace is accessible\")\n",
    "            print(f\"   3. Cluster policies and configurations are valid\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå CRITICAL ERROR during revert execution:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        print(f\"\\nPlease check:\")\n",
    "        print(f\"   1. Service Principal credentials are valid\")\n",
    "        print(f\"   2. Network connectivity to target workspace\")\n",
    "        print(f\"   3. Cell 5 was run successfully before this cell\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÅ REVERT OPERATION COMPLETE\")\n",
    "print(f\"Completion Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ec6f33-f3c6-434f-a064-76a71fb7c1f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update revert tracking columns in backup table"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UPDATE REVERT TRACKING IN CLUSTER_CONFIG_BACKUP TABLE\n",
    "# ============================================================================\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "\n",
    "print(\"\\nüìù UPDATING REVERT TRACKING IN BACKUP TABLE...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'revert_results_final' not in locals() or len(revert_results_final) == 0:\n",
    "    print(\"‚ö†Ô∏è  No revert results found. Please run Cell 7 first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Get current user\n",
    "        current_user = spark.sql(\"SELECT current_user() as user\").first()['user']\n",
    "        \n",
    "        # Generate new revert batch ID\n",
    "        import uuid\n",
    "        revert_batch_id = str(uuid.uuid4())\n",
    "        revert_timestamp = datetime.now()\n",
    "        \n",
    "        # Filter successful reverts\n",
    "        successful_reverts = [r for r in revert_results_final if r['status'] == 'SUCCESS']\n",
    "        \n",
    "        if len(successful_reverts) == 0:\n",
    "            print(\"‚ö†Ô∏è  No successful reverts to track.\")\n",
    "        else:\n",
    "            # Get cluster IDs that were successfully reverted\n",
    "            reverted_cluster_ids = [r['cluster_id'] for r in successful_reverts]\n",
    "            \n",
    "            print(f\"Updating tracking for {len(reverted_cluster_ids)} successfully reverted clusters...\")\n",
    "            print(f\"Revert Batch ID: {revert_batch_id}\")\n",
    "            print(f\"Revert Timestamp: {revert_timestamp}\")\n",
    "            print(f\"Reverted By: {current_user}\")\n",
    "            print(f\"Original Batch: {selected_batch_label}\\n\")\n",
    "            \n",
    "            # Get the batch_id from selected_batch_label\n",
    "            batch_info = spark.table(f\"{selected_environment}.billing_forecast.cluster_config_backup\") \\\n",
    "                .filter(col(\"execution_label\") == selected_batch_label) \\\n",
    "                .select(\"batch_id\") \\\n",
    "                .distinct() \\\n",
    "                .first()\n",
    "            \n",
    "            if batch_info is None:\n",
    "                print(f\"‚ùå Could not find batch_id for execution_label: {selected_batch_label}\")\n",
    "            else:\n",
    "                original_batch_id = batch_info['batch_id']\n",
    "                \n",
    "                # Update the backup table for successfully reverted clusters\n",
    "                from delta.tables import DeltaTable\n",
    "                \n",
    "                backup_table = DeltaTable.forName(spark, f\"{selected_environment}.billing_forecast.cluster_config_backup\")\n",
    "                \n",
    "                # Update rows matching the original batch and reverted cluster IDs\n",
    "                backup_table.update(\n",
    "                    condition = (col(\"batch_id\") == original_batch_id) & \n",
    "                                (col(\"cluster_id\").isin(reverted_cluster_ids)),\n",
    "                    set = {\n",
    "                        \"is_reverted\": lit(True),\n",
    "                        \"revert_timestamp\": lit(revert_timestamp),\n",
    "                        \"revert_batch_id\": lit(revert_batch_id),\n",
    "                        \"reverted_by_user\": lit(current_user)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                print(\"‚úÖ Successfully updated revert tracking columns\\n\")\n",
    "                \n",
    "                # Verify the updates\n",
    "                updated_records = spark.table(f\"{selected_environment}.billing_forecast.cluster_config_backup\") \\\n",
    "                    .filter(\n",
    "                        (col(\"batch_id\") == original_batch_id) & \n",
    "                        (col(\"cluster_id\").isin(reverted_cluster_ids)) &\n",
    "                        (col(\"is_reverted\") == True)\n",
    "                    ) \\\n",
    "                    .select(\"cluster_name\", \"cluster_id\", \"is_reverted\", \"revert_timestamp\", \"revert_batch_id\", \"reverted_by_user\")\n",
    "                \n",
    "                print(\"üìä VERIFICATION - Updated Records:\")\n",
    "                display(updated_records)\n",
    "                \n",
    "                updated_count = updated_records.count()\n",
    "                print(f\"\\n‚úÖ Verified: {updated_count} records updated in backup table\")\n",
    "                print(f\"Expected: {len(reverted_cluster_ids)} records\")\n",
    "                \n",
    "                if updated_count == len(reverted_cluster_ids):\n",
    "                    print(\"\\nüéâ ALL REVERT TRACKING UPDATES SUCCESSFUL!\")\n",
    "                else:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Mismatch: Expected {len(reverted_cluster_ids)} but updated {updated_count}\")\n",
    "                \n",
    "                displayHTML(f\"\"\"\n",
    "                <div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "                    <h3 style=\"margin: 0; color: #2e7d32;\">‚úì Revert Tracking Updated</h3>\n",
    "                    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Table:</strong> <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{selected_environment}.billing_forecast.cluster_config_backup</code></p>\n",
    "                    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Records Updated:</strong> {updated_count}</p>\n",
    "                    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Revert Batch ID:</strong> {revert_batch_id}</p>\n",
    "                    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Reverted By:</strong> {current_user}</p>\n",
    "                    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Original Batch:</strong> {selected_batch_label}</p>\n",
    "                </div>\n",
    "                \"\"\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR updating revert tracking:\")\n",
    "        print(f\"   {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÅ REVERT TRACKING UPDATE COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c424b614-465b-45fe-8214-9c223e5915a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Post-Execution Verification\n",
    "\n",
    "### Step 1: Review Execution Summary\n",
    "\n",
    "Check the results table above:\n",
    "* ‚òê Verify success count matches expected\n",
    "* ‚òê Review any failed clusters and error messages\n",
    "* ‚òê Note the total configuration changes applied\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Verify in Target Workspace\n",
    "\n",
    "Navigate to the target workspace and verify:\n",
    "\n",
    "1. **Go to Compute ‚Üí Clusters**\n",
    "2. **For each reverted cluster, check:**\n",
    "   * ‚òê Driver instance type matches target\n",
    "   * ‚òê Worker instance type matches target\n",
    "   * ‚òê Autoscale settings are correct\n",
    "   * ‚òê **Cluster policy is applied** (if applicable)\n",
    "   * ‚òê **Data security mode is set** (if applicable)\n",
    "   * ‚òê Elastic disk setting is correct\n",
    "   * ‚òê Auto-termination is configured\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Test Cluster Functionality (Optional)\n",
    "\n",
    "* ‚òê Start one or more reverted clusters\n",
    "* ‚òê Verify they start successfully\n",
    "* ‚òê Check that policies are enforced\n",
    "* ‚òê Confirm data access works as expected\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Troubleshooting Failed Reverts\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. Policy Not Found Error\n",
    "**Symptom**: `Policy ID XXX not found`\n",
    "\n",
    "**Causes:**\n",
    "* Cluster policy doesn't exist in target workspace\n",
    "* Policy was deleted or renamed\n",
    "* Service Principal doesn't have access to policy\n",
    "\n",
    "**Solutions:**\n",
    "* Verify policy exists: Go to Compute ‚Üí Policies in target workspace\n",
    "* Check Service Principal has `CAN_USE` permission on policy\n",
    "* If policy is missing, recreate it or remove policy requirement\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. AWS Attributes Validation Error\n",
    "**Symptom**: `Invalid AWS attributes` or `Policy validation failed`\n",
    "\n",
    "**Causes:**\n",
    "* AWS attributes from previous state incompatible with current policy\n",
    "* Policy requirements changed since previous configuration\n",
    "* Instance profile or availability zone restrictions\n",
    "\n",
    "**Solutions:**\n",
    "* Check policy definition for AWS attribute requirements\n",
    "* Verify instance profile ARN is valid\n",
    "* Ensure availability zone is allowed by policy\n",
    "* Check `first_on_demand`, `spot_bid_price_percent` settings\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Permission Denied Error\n",
    "**Symptom**: `User does not have permission to edit cluster`\n",
    "\n",
    "**Causes:**\n",
    "* Service Principal lacks `CAN_MANAGE` permission on cluster\n",
    "* Workspace-level permissions insufficient\n",
    "* Secret scope access issues\n",
    "\n",
    "**Solutions:**\n",
    "* Grant Service Principal `CAN_MANAGE` on clusters\n",
    "* Check workspace admin permissions\n",
    "* Verify `sp-oauth` secret scope is accessible\n",
    "* Test authentication in Cell 3\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Cluster Not Found or Terminated\n",
    "**Symptom**: `Cluster XXX not found` or `Cannot edit terminated cluster`\n",
    "\n",
    "**Causes:**\n",
    "* Cluster was deleted after batch update\n",
    "* Cluster is in TERMINATED state\n",
    "* Cluster ID changed\n",
    "\n",
    "**Solutions:**\n",
    "* Check cluster still exists in target workspace\n",
    "* Verify cluster is not permanently deleted\n",
    "* Skip deleted clusters - they don't need revert\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Cross-Workspace Authentication Failed\n",
    "**Symptom**: `OAuth authentication failed` or `401 Unauthorized`\n",
    "\n",
    "**Causes:**\n",
    "* Service Principal credentials invalid or expired\n",
    "* Secret scope not accessible\n",
    "* Network connectivity issues\n",
    "\n",
    "**Solutions:**\n",
    "* Verify Service Principal credentials in `sp-oauth` scope\n",
    "* Test OAuth token generation manually\n",
    "* Ensure classic compute is being used (not serverless)\n",
    "* Check network connectivity to target workspace\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Audit Log Template\n",
    "\n",
    "Use this template to document the revert operation:\n",
    "\n",
    "```\n",
    "CLUSTER CONFIGURATION REVERT - AUDIT LOG\n",
    "=========================================\n",
    "\n",
    "Execution Date: [YYYY-MM-DD HH:MM:SS]\n",
    "Executed By: [Your Name]\n",
    "Environment: [dev_sandbox/qa_sandbox/uat_sandbox/prod_sandbox]\n",
    "Catalog: [Catalog Name]\n",
    "\n",
    "Reference Batch Information:\n",
    "- Batch Label: [Execution Label]\n",
    "- Batch ID: [UUID]\n",
    "- Reference Date: [YYYY-MM-DD HH:MM:SS]\n",
    "- Original Executed By: [User]\n",
    "\n",
    "Revert Details:\n",
    "- Total Clusters Processed: [X]\n",
    "- Successfully Reverted: [Y]\n",
    "- Failed: [Z]\n",
    "- Success Rate: [%]\n",
    "- Target Workspace: [Workspace URL]\n",
    "\n",
    "Configuration Changes Restored:\n",
    "- Instance Types: [Count]\n",
    "- Cluster Policies: [Count]\n",
    "- Security Modes: [Count]\n",
    "- Elastic Disk Settings: [Count]\n",
    "- Auto-Termination Settings: [Count]\n",
    "\n",
    "Verification:\n",
    "- Verified in Target Workspace: [Yes/No]\n",
    "- All Policies Restored: [Yes/No]\n",
    "- Clusters Functional: [Yes/No]\n",
    "- Tests Passed: [Yes/No]\n",
    "\n",
    "Reason for Revert:\n",
    "[Explain why the revert was necessary]\n",
    "\n",
    "Issues Encountered:\n",
    "[Document any failures or problems]\n",
    "\n",
    "Resolution:\n",
    "[How issues were resolved]\n",
    "\n",
    "Approved By: _______________\n",
    "Date: _______________\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Re-running This Tool\n",
    "\n",
    "To revert a different batch:\n",
    "\n",
    "1. **Change Environment**: Use the dropdown widget to select different environment\n",
    "2. **Re-run Cells 3-6**: Execute the workflow cells in sequence\n",
    "3. **Select Different Batch**: Use the batch dropdown to choose a different reference point\n",
    "4. **The tool uses the selected batch as a time reference** - all clusters revert to their state BEFORE that date\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Revert operation complete! Always verify results and maintain audit documentation.**"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "no-auto-run"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Cluster Configuration Revert Tool - Production",
   "widgets": {
    "batch_to_revert": {
     "currentValue": "2025-12-15_21-57_LIVE_Integrated-Dev",
     "nuid": "5ea8bc8c-4e6e-46ab-b7b9-ef898f3102d2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-12-15_21-57_LIVE_Integrated-Dev",
      "label": "Select Batch Reference Point",
      "name": "batch_to_revert",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "2025-12-15_21-57_LIVE_Integrated-Dev",
        "2025-12-12_21-15_LIVE_Integrated-Dev",
        "2025-12-12_04-21_LIVE_Integrated-Dev",
        "2025-12-10_23-28_LIVE_Integrated-Dev",
        "2025-12-10_20-11_LIVE_Integrated-Dev",
        "2025-12-09_17-39_LIVE_Integrated-Dev",
        "2025-12-09_17-30_LIVE_Integrated-Dev"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "2025-12-15_21-57_LIVE_Integrated-Dev",
      "label": "Select Batch Reference Point",
      "name": "batch_to_revert",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "2025-12-15_21-57_LIVE_Integrated-Dev",
        "2025-12-12_21-15_LIVE_Integrated-Dev",
        "2025-12-12_04-21_LIVE_Integrated-Dev",
        "2025-12-10_23-28_LIVE_Integrated-Dev",
        "2025-12-10_20-11_LIVE_Integrated-Dev",
        "2025-12-09_17-39_LIVE_Integrated-Dev",
        "2025-12-09_17-30_LIVE_Integrated-Dev"
       ]
      }
     }
    },
    "environment": {
     "currentValue": "dev_sandbox",
     "nuid": "3bd184bd-93a2-40ec-8a51-6e4db757e59e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev_sandbox",
      "label": "Select Environment",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev_sandbox",
        "qa_sandbox",
        "uat_sandbox",
        "prod_sandbox"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev_sandbox",
      "label": "Select Environment",
      "name": "environment",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev_sandbox",
        "qa_sandbox",
        "uat_sandbox",
        "prod_sandbox"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
