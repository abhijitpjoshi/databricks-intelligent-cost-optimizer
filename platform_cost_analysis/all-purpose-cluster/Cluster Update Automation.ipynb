{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67655755-c252-4214-ae18-528e5b4eef5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîÑ Cluster Update Automation System\n",
    "## Automated Instance Type Updates with Complete Backup & Rollback Capability\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Overview\n",
    "\n",
    "This notebook automates the process of updating Databricks cluster instance types based on optimization recommendations from the cost analysis system. It includes comprehensive safety features, complete configuration backup, and dashboard-friendly change tracking.\n",
    "\n",
    "### Key Capabilities\n",
    "\n",
    "‚úÖ **Safe Updates**: Dry-run mode, validation checks, 10-second countdown  \n",
    "‚úÖ **Complete Backup**: Full cluster configuration snapshots (before/after)  \n",
    "‚úÖ **Rollback Ready**: JSON configs for reverting any cluster  \n",
    "‚úÖ **Dashboard Friendly**: User-readable change summaries and impact analysis  \n",
    "‚úÖ **Batch Tracking**: Execution labels for easy filtering and audit trails  \n",
    "‚úÖ **Cross-Workspace**: Updates clusters across multiple workspaces  \n",
    "‚úÖ **Setting Preservation**: Maintains policies, tags, configs, scripts, security  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Use Cases\n",
    "\n",
    "1. **Cost Optimization**: Apply recommended instance type downsizing\n",
    "2. **Standardization**: Migrate clusters to approved instance types\n",
    "3. **Policy Enforcement**: Update clusters while preserving governance policies\n",
    "4. **Change Tracking**: Build dashboards showing what changed and why\n",
    "5. **Rollback**: Revert clusters to previous configurations if needed\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Input/Output Tables\n",
    "\n",
    "### Input Table\n",
    "* **`cluster_opportunities`** - Optimization recommendations with suggested instance types\n",
    "\n",
    "### Output Tables\n",
    "* **`cluster_update_log`** - Execution tracking (validation, status, errors, savings)\n",
    "* **`cluster_config_backup`** - Complete before/after configs + dashboard fields\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Prerequisites\n",
    "\n",
    "### Required Permissions\n",
    "* **Workspace Access**: Ability to connect to target workspaces\n",
    "* **Cluster Edit**: Permission to modify cluster configurations\n",
    "* **Table Access**: Read from opportunities table, write to log tables\n",
    "* **Token Authentication**: Valid Databricks token in notebook context\n",
    "\n",
    "### Required Data\n",
    "* **cluster_opportunities table** must exist with recommendations\n",
    "* **Clusters must be STOPPED** (cannot update running clusters)\n",
    "* **Non-pooled clusters only** (instance pools define their own types)\n",
    "\n",
    "### Dependencies\n",
    "* **databricks-sdk** Python package (installed in cell 1)\n",
    "* **System tables**: `system.access.workspaces_latest` for workspace list\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Quick Start Guide\n",
    "\n",
    "### Step 1: Configure Parameters\n",
    "Set the widgets at the top:\n",
    "* **dry_run**: `true` (preview) or `false` (live update)\n",
    "* **workspaces**: Specific workspace names or \"All\"\n",
    "* **catalog**: Target catalog name\n",
    "* **schema**: Target schema name\n",
    "\n",
    "### Step 2: Review Preview\n",
    "* Check cluster breakdown by workspace\n",
    "* Review potential savings\n",
    "* Verify configuration settings\n",
    "\n",
    "### Step 3: Execute\n",
    "* **Dry-run first**: Always test with `dry_run=true`\n",
    "* **Review results**: Check validation and update status\n",
    "* **Go live**: Set `dry_run=false` and re-run\n",
    "\n",
    "### Step 4: Monitor Results\n",
    "* View execution summary and status breakdown\n",
    "* Check backup table for change details\n",
    "* Use sample queries for dashboard building\n",
    "\n",
    "---\n",
    "\n",
    "## üõ°Ô∏è Safety Features\n",
    "\n",
    "### Validation Checks\n",
    "* ‚úÖ Cluster must be STOPPED (not RUNNING or PENDING)\n",
    "* ‚úÖ Cluster must NOT use instance pools\n",
    "* ‚úÖ Current config must match expected state (prevents double-updates)\n",
    "* ‚úÖ Workspace must be accessible (handles 403 errors gracefully)\n",
    "* ‚úÖ Deployment name must exist\n",
    "\n",
    "### Dry-Run Mode\n",
    "* Preview all changes without modifying clusters\n",
    "* Creates backup entries showing what WOULD change\n",
    "* Validates all checks without calling clusters.edit()\n",
    "* Safe to run multiple times\n",
    "\n",
    "### Countdown Timer\n",
    "* 10-second countdown before execution\n",
    "* Clear warning about DRY_RUN vs LIVE_UPDATE mode\n",
    "* Opportunity to cancel if needed\n",
    "\n",
    "### Error Handling\n",
    "* Cross-workspace connectivity issues\n",
    "* Certificate validation failures\n",
    "* API rate limiting (pauses every 10 clusters)\n",
    "* Detailed error logging with stack traces\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Dashboard Building\n",
    "\n",
    "The `cluster_config_backup` table includes dashboard-friendly fields:\n",
    "\n",
    "### Change Analysis\n",
    "* `change_summary` - Human-readable description\n",
    "* `change_impact` - MINOR, MODERATE, or MAJOR\n",
    "* `change_categories` - What changed (instance_type, policy, etc.)\n",
    "* `total_changes_count` - Number of settings modified\n",
    "\n",
    "### Boolean Filters\n",
    "* `instance_type_changed`, `policy_changed`, `spark_config_changed`\n",
    "* `tags_changed`, `init_scripts_changed`, `autotermination_changed`\n",
    "* `runtime_engine_changed`, `security_mode_changed`\n",
    "\n",
    "### Before/After Metrics\n",
    "* Instance types, policy IDs, autoscale settings\n",
    "* Spark config counts, custom tag counts, init script counts\n",
    "* Runtime engine, security mode, autotermination minutes\n",
    "\n",
    "**See Cell 17** for 7 sample dashboard queries\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Rollback Process\n",
    "\n",
    "### Using the Backup Table\n",
    "\n",
    "1. **Query backups**:\n",
    "```sql\n",
    "SELECT backup_id, cluster_name, before_config, backup_timestamp\n",
    "FROM {catalog}.{schema}.cluster_config_backup\n",
    "WHERE cluster_name = 'my-cluster'\n",
    "  AND update_status = 'SUCCESS'\n",
    "  AND is_reverted = false\n",
    "ORDER BY backup_timestamp DESC\n",
    "LIMIT 1\n",
    "```\n",
    "\n",
    "2. **Parse JSON config**:\n",
    "```python\n",
    "import json\n",
    "config = json.loads(backup.before_config)\n",
    "```\n",
    "\n",
    "3. **Apply config** using Databricks SDK (in separate revert notebook)\n",
    "\n",
    "4. **Mark as reverted**:\n",
    "```sql\n",
    "UPDATE {catalog}.{schema}.cluster_config_backup\n",
    "SET is_reverted = true,\n",
    "    revert_timestamp = current_timestamp(),\n",
    "    reverted_by_user = current_user()\n",
    "WHERE backup_id = '<backup_id>'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Execution Workflow\n",
    "\n",
    "```\n",
    "1. Setup & Configuration (Cells 1-5)\n",
    "   ‚îú‚îÄ Install SDK\n",
    "   ‚îú‚îÄ Create widgets\n",
    "   ‚îú‚îÄ Display cluster breakdown\n",
    "   ‚îú‚îÄ Review configuration\n",
    "   ‚îî‚îÄ 10-second countdown\n",
    "\n",
    "2. Core Functions (Cells 6-9)\n",
    "   ‚îú‚îÄ Import libraries\n",
    "   ‚îú‚îÄ Define update functions\n",
    "   ‚îú‚îÄ Define dashboard helpers\n",
    "   ‚îî‚îÄ Main orchestration function\n",
    "\n",
    "3. Testing (Cell 10)\n",
    "   ‚îî‚îÄ Unit tests for dry-run safety\n",
    "\n",
    "4. Table Setup (Cells 11-14)\n",
    "   ‚îú‚îÄ Create update log table\n",
    "   ‚îú‚îÄ Create backup table\n",
    "   ‚îî‚îÄ Enhance with dashboard fields\n",
    "\n",
    "5. Execution (Cells 15-16)\n",
    "   ‚îú‚îÄ Execute updates\n",
    "   ‚îî‚îÄ Save results to tables\n",
    "\n",
    "6. Results & Documentation (Cells 17-24)\n",
    "   ‚îú‚îÄ Sample dashboard queries\n",
    "   ‚îú‚îÄ Documentation\n",
    "   ‚îî‚îÄ Interactive result viewers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "### Before Running\n",
    "* ‚úÖ Always test with `dry_run=true` first\n",
    "* ‚úÖ Ensure clusters are STOPPED\n",
    "* ‚úÖ Verify workspace access permissions\n",
    "* ‚úÖ Review cluster breakdown and savings\n",
    "* ‚úÖ Check that cluster_opportunities table is up-to-date\n",
    "\n",
    "### During Execution\n",
    "* ‚è±Ô∏è Processing time: ~1-2 seconds per cluster\n",
    "* üîÑ Rate limiting: Pauses every 10 clusters\n",
    "* üìä Progress displayed for each cluster\n",
    "* ‚ùå Errors logged but don't stop batch\n",
    "\n",
    "### After Execution\n",
    "* üìà Review execution summary\n",
    "* ‚úÖ Check validation and update status\n",
    "* üíæ Backup entries created for all attempts\n",
    "* üîç Use execution_label for filtering\n",
    "\n",
    "---\n",
    "\n",
    "## üÜò Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**\"Cluster in RUNNING state\"**\n",
    "* Stop the cluster before updating\n",
    "* Cannot update running or pending clusters\n",
    "\n",
    "**\"Cluster uses instance pools\"**\n",
    "* Instance pools define node types\n",
    "* Cannot change instance types for pooled clusters\n",
    "\n",
    "**\"Cluster config mismatch\"**\n",
    "* Cluster was already updated\n",
    "* Current config doesn't match expected state\n",
    "* Re-run cost analysis to get fresh recommendations\n",
    "\n",
    "**\"Connectivity failed to workspace\"**\n",
    "* Cross-workspace access denied\n",
    "* Check token permissions\n",
    "* Verify workspace is accessible\n",
    "\n",
    "**\"No deployment name found\"**\n",
    "* Workspace metadata incomplete\n",
    "* Check system.access.workspaces_latest table\n",
    "\n",
    "---\n",
    "\n",
    "## üìû Support\n",
    "\n",
    "For issues or questions:\n",
    "1. Check execution logs in `cluster_update_log` table\n",
    "2. Review error_details column for stack traces\n",
    "3. Use execution_label to filter specific runs\n",
    "4. Check backup table for configuration snapshots\n",
    "\n",
    "---\n",
    "\n",
    "**Version**: 2.0  \n",
    "**Last Updated**: 2025-12-11  \n",
    "**Maintained By**: Platform Engineering Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf96aefd-b81b-4573-94c0-5bb8a740c09d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812ab29f-d98a-4f04-ab08-30aefaff590c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration Widgets"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of workspaces from the system table (available in all workspaces)\n",
    "workspaces_list = spark.table(\"system.access.workspaces_latest\") \\\n",
    "    .select(\"workspace_name\") \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"workspace_name\") \\\n",
    "    .toPandas()['workspace_name'].tolist()\n",
    "\n",
    "# Add \"All\" as the first option\n",
    "workspace_options = [\"All\"] + workspaces_list\n",
    "\n",
    "# Create widgets for configuration parameters\n",
    "dbutils.widgets.dropdown(\"dry_run\", \"true\", [\"true\", \"false\"], \"Dry Run Mode\")\n",
    "dbutils.widgets.dropdown(\"workspaces\", \"All\", workspace_options, \"Target Workspaces\")\n",
    "dbutils.widgets.text(\"catalog\", \"ex_dash_temp\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"billing_forecast\", \"Schema Name\")\n",
    "\n",
    "# Get widget values and create variables for use in all subsequent cells\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "full_schema = f\"{catalog}.{schema}\"\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #2e7d32;\">‚úì Configuration Widgets Created</h3>\n",
    "    <ul style=\"color: #1b5e20;\">\n",
    "        <li><strong>Dry Run Mode:</strong> Set to 'true' for preview only, 'false' for actual updates</li>\n",
    "        <li><strong>Target Workspaces:</strong> Select specific workspace or 'All' to process all workspaces</li>\n",
    "        <li><strong>Catalog Name:</strong> Output catalog for tables (default: ex_dash_temp)</li>\n",
    "        <li><strong>Schema Name:</strong> Output schema for tables (default: billing_forecast)</li>\n",
    "    </ul>\n",
    "    <p style=\"margin-top: 10px; color: #1565c0; font-style: italic;\">‚úì Using system.access.workspaces_latest (available in all workspaces - no cloning needed!)</p>\n",
    "    <p style=\"margin-top: 10px; color: #6a1b9a; font-weight: bold;\">üìä All tables will be read/written to: <code style=\"background-color: #e1bee7; padding: 3px 8px; border-radius: 3px;\">{full_schema}</code></p>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f25489-de47-41c7-9699-f3de73a33e76",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Cluster Breakdown by Workspace"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Get widget values first\n",
    "dry_run_str = dbutils.widgets.get(\"dry_run\")\n",
    "workspaces_str = dbutils.widgets.get(\"workspaces\")\n",
    "\n",
    "# Convert to usable variables\n",
    "DRY_RUN = dry_run_str.lower() == \"true\"\n",
    "WORKSPACES_TO_UPDATE = workspaces_str.strip() if workspaces_str != \"All\" else \"\"\n",
    "\n",
    "# Load opportunities data using full_schema variable\n",
    "opportunities_df = spark.table(f\"{full_schema}.cluster_opportunities\")\n",
    "\n",
    "# Use system.access.workspaces_latest and extract deployment_name from workspace_url\n",
    "# This table is available in ALL workspaces - no need to clone!\n",
    "workspaces_df = spark.table(\"system.access.workspaces_latest\") \\\n",
    "    .withColumn(\n",
    "        \"deployment_name\",\n",
    "        F.regexp_extract(F.col(\"workspace_url\"), r\"https://([^.]+)\\.cloud\\.databricks\\.com\", 1)\n",
    "    ) \\\n",
    "    .select(\n",
    "        F.col(\"workspace_id\").cast(\"long\").alias(\"workspace_id\"),\n",
    "        \"workspace_name\",\n",
    "        \"deployment_name\"\n",
    "    )\n",
    "\n",
    "# Join to get workspace details\n",
    "cluster_data = opportunities_df.join(\n",
    "    workspaces_df,\n",
    "    on=\"workspace_name\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Filter by workspace if specified\n",
    "filtered_cluster_data = cluster_data\n",
    "if WORKSPACES_TO_UPDATE:\n",
    "    workspace_list = [ws.strip() for ws in WORKSPACES_TO_UPDATE.split(\",\")]\n",
    "    filtered_cluster_data = cluster_data.filter(F.col(\"workspace_name\").isin(workspace_list))\n",
    "\n",
    "# Get cluster counts by workspace\n",
    "workspace_counts = (\n",
    "    filtered_cluster_data\n",
    "    .groupBy(\"workspace_name\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"cluster_count\"),\n",
    "        F.sum(\"validated_savings\").alias(\"total_savings\")\n",
    "    )\n",
    "    .orderBy(F.col(\"cluster_count\").desc())\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "total_clusters = sum([row.cluster_count for row in workspace_counts])\n",
    "total_savings = sum([float(row.total_savings) if row.total_savings else 0 for row in workspace_counts])\n",
    "\n",
    "# Build HTML table for workspace breakdown\n",
    "workspace_table_html = f\"\"\"\n",
    "<div style=\"border: 2px solid #1976d2; padding: 20px; margin: 20px 0; background-color: #e3f2fd; border-radius: 8px;\">\n",
    "    <h3 style=\"color: #0d47a1; margin-top: 0;\">üìã Cluster Update Plan by Workspace</h3>\n",
    "    <p style=\"margin: 5px 0; color: #1565c0; font-size: 13px;\">üíæ Source: {full_schema}.cluster_opportunities</p>\n",
    "    <table style=\"width: 100%; border-collapse: collapse; margin-top: 15px;\">\n",
    "        <tr style=\"background-color: #90caf9;\">\n",
    "            <th style=\"padding: 12px; text-align: left; border: 1px solid #64b5f6; color: #0d47a1;\">Workspace</th>\n",
    "            <th style=\"padding: 12px; text-align: center; border: 1px solid #64b5f6; color: #0d47a1;\">Clusters to Update</th>\n",
    "            <th style=\"padding: 12px; text-align: right; border: 1px solid #64b5f6; color: #0d47a1;\">Potential Savings (USD)</th>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "\n",
    "for row in workspace_counts:\n",
    "    savings_display = f\"${row.total_savings:,.2f}\" if row.total_savings else \"$0.00\"\n",
    "    workspace_table_html += f\"\"\"\n",
    "        <tr style=\"background-color: #ffffff;\">\n",
    "            <td style=\"padding: 10px; border: 1px solid #64b5f6; font-weight: bold;\">{row.workspace_name}</td>\n",
    "            <td style=\"padding: 10px; border: 1px solid #64b5f6; text-align: center; font-size: 18px; color: #1976d2;\">{row.cluster_count}</td>\n",
    "            <td style=\"padding: 10px; border: 1px solid #64b5f6; text-align: right; font-weight: bold; color: #2e7d32;\">{savings_display}</td>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "\n",
    "# Add total row\n",
    "workspace_table_html += f\"\"\"\n",
    "        <tr style=\"background-color: #bbdefb; font-weight: bold;\">\n",
    "            <td style=\"padding: 12px; border: 1px solid #64b5f6; color: #0d47a1;\">TOTAL</td>\n",
    "            <td style=\"padding: 12px; border: 1px solid #64b5f6; text-align: center; font-size: 20px; color: #0d47a1;\">{total_clusters}</td>\n",
    "            <td style=\"padding: 12px; border: 1px solid #64b5f6; text-align: right; font-size: 18px; color: #1b5e20;\">${total_savings:,.2f}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "displayHTML(workspace_table_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f781327b-44b0-485e-8b2a-d9a09e457045",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Configuration - REVIEW BEFORE PROCEEDING"
    }
   },
   "outputs": [],
   "source": [
    "# Get widget values\n",
    "dry_run_str = dbutils.widgets.get(\"dry_run\")\n",
    "workspaces_str = dbutils.widgets.get(\"workspaces\")\n",
    "\n",
    "# Convert dry_run to boolean\n",
    "DRY_RUN = dry_run_str.lower() == \"true\"\n",
    "WORKSPACES_TO_UPDATE = workspaces_str.strip() if workspaces_str != \"All\" else \"\"\n",
    "\n",
    "# Determine mode styling\n",
    "if DRY_RUN:\n",
    "    mode_color = \"#4caf50\"\n",
    "    mode_bg = \"#e8f5e9\"\n",
    "    mode_icon = \"üü¢\"\n",
    "    mode_text = \"DRY RUN (PREVIEW ONLY - NO CHANGES)\"\n",
    "else:\n",
    "    mode_color = \"#f44336\"\n",
    "    mode_bg = \"#ffebee\"\n",
    "    mode_icon = \"üî¥\"\n",
    "    mode_text = \"LIVE UPDATE (WILL MODIFY CLUSTERS)\"\n",
    "\n",
    "# Determine workspace display\n",
    "if WORKSPACES_TO_UPDATE:\n",
    "    workspace_display = WORKSPACES_TO_UPDATE\n",
    "else:\n",
    "    workspace_display = \"ALL WORKSPACES\"\n",
    "\n",
    "# Create HTML display\n",
    "html_content = f\"\"\"\n",
    "<div style=\"border: 3px solid {mode_color}; padding: 20px; margin: 20px 0; background-color: {mode_bg}; border-radius: 8px;\">\n",
    "    <h2 style=\"text-align: center; color: {mode_color}; margin-top: 0;\">\n",
    "        ‚ö†Ô∏è CLUSTER UPDATE CONFIGURATION ‚ö†Ô∏è\n",
    "    </h2>\n",
    "    <hr style=\"border: 1px solid {mode_color}; margin: 20px 0;\">\n",
    "    \n",
    "    <div style=\"font-size: 16px; line-height: 2;\">\n",
    "        <div style=\"margin: 15px 0;\">\n",
    "            <strong style=\"font-size: 18px;\">üîß EXECUTION MODE:</strong>\n",
    "            <span style=\"font-size: 20px; font-weight: bold; color: {mode_color}; margin-left: 20px;\">\n",
    "                {mode_icon} {mode_text}\n",
    "            </span>\n",
    "        </div>\n",
    "        \n",
    "        <div style=\"margin: 15px 0;\">\n",
    "            <strong style=\"font-size: 18px;\">üåê TARGET WORKSPACES:</strong>\n",
    "            <span style=\"font-size: 18px; font-weight: bold; color: #1976d2; margin-left: 20px;\">\n",
    "                {workspace_display}\n",
    "            </span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <hr style=\"border: 1px solid {mode_color}; margin: 20px 0;\">\n",
    "    \n",
    "    <div style=\"padding: 15px; background-color: white; border-radius: 5px; margin-top: 15px;\">\n",
    "\"\"\"\n",
    "\n",
    "if not DRY_RUN:\n",
    "    html_content += \"\"\"\n",
    "        <div style=\"color: #d32f2f; font-weight: bold; font-size: 16px;\">\n",
    "            ‚ö†Ô∏è WARNING: Live update mode is enabled!<br>\n",
    "            ‚ö†Ô∏è Clusters will be ACTUALLY MODIFIED after validation.\n",
    "        </div>\n",
    "    \"\"\"\n",
    "else:\n",
    "    html_content += \"\"\"\n",
    "        <div style=\"color: #388e3c; font-weight: bold; font-size: 16px;\">\n",
    "            ‚úì Safe mode: Dry run will only preview changes without modifying clusters.\n",
    "        </div>\n",
    "    \"\"\"\n",
    "\n",
    "html_content += \"\"\"\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"text-align: center; margin-top: 20px; font-size: 18px; font-weight: bold; color: #ff6f00;\">\n",
    "        ‚è±Ô∏è Starting in 10 seconds...\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "displayHTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f907d5d2-ab19-4e12-ae50-a79fea637a6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Countdown Timer"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 10-second countdown\n",
    "for i in range(10, 0, -1):\n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"text-align: center; padding: 20px; background-color: #fff3e0; border: 2px solid #ff9800; border-radius: 8px;\">\n",
    "        <h2 style=\"color: #e65100; margin: 0;\">\n",
    "            ‚è±Ô∏è Starting in <span style=\"font-size: 36px; color: #ff6f00;\">{i}</span> seconds...\n",
    "        </h2>\n",
    "        <p style=\"color: #bf360c; margin-top: 10px; font-size: 14px;\">(Press Stop to cancel)</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    time.sleep(1)\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"text-align: center; padding: 25px; background-color: #e3f2fd; border: 3px solid #2196f3; border-radius: 8px;\">\n",
    "    <h1 style=\"color: #0d47a1; margin: 0;\">\n",
    "        üöÄ STARTING CLUSTER UPDATE PROCESS\n",
    "    </h1>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "962bf7f8-ceb9-41a8-b799-c6c146aad7ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import libraries and setup authentication"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.core import Config\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Get authentication token from notebook context\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "print(\"‚úì Authentication token loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cc11cbc-7ac8-40af-b4c0-ede9befaf457",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define cluster update functions"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import json\n",
    "\n",
    "def get_workspace_client(deployment_name, token):\n",
    "    \"\"\"Create authenticated workspace client for a specific workspace\"\"\"\n",
    "    host = f\"https://{deployment_name}.cloud.databricks.com\"\n",
    "    \n",
    "    # Use token authentication\n",
    "    return WorkspaceClient(\n",
    "        host=host,\n",
    "        token=token\n",
    "    )\n",
    "\n",
    "def serialize_cluster_config(config):\n",
    "    \"\"\"Convert cluster config to JSON string for storage\"\"\"\n",
    "    if \"error\" in config:\n",
    "        return json.dumps({\"error\": config[\"error\"]})\n",
    "    \n",
    "    # Create a serializable version of the config\n",
    "    serializable_config = {}\n",
    "    for key, value in config.items():\n",
    "        if key == \"cluster_object\":\n",
    "            continue  # Skip the full object\n",
    "        elif value is None:\n",
    "            serializable_config[key] = None\n",
    "        elif hasattr(value, 'as_dict'):\n",
    "            # Databricks SDK objects with as_dict method\n",
    "            serializable_config[key] = value.as_dict()\n",
    "        elif isinstance(value, (str, int, float, bool)):\n",
    "            serializable_config[key] = value\n",
    "        elif isinstance(value, dict):\n",
    "            serializable_config[key] = value\n",
    "        elif isinstance(value, list):\n",
    "            serializable_config[key] = [item.as_dict() if hasattr(item, 'as_dict') else item for item in value]\n",
    "        else:\n",
    "            serializable_config[key] = str(value)\n",
    "    \n",
    "    return json.dumps(serializable_config, indent=2)\n",
    "\n",
    "def analyze_config_changes(before_config, after_config):\n",
    "    \"\"\"Analyze changes between before and after configs for dashboard display\n",
    "    \n",
    "    Returns dict with change analysis fields\n",
    "    \"\"\"\n",
    "    if \"error\" in before_config or not after_config:\n",
    "        return {\n",
    "            \"change_categories\": None,\n",
    "            \"total_changes_count\": 0,\n",
    "            \"change_impact\": \"UNKNOWN\",\n",
    "            \"instance_type_changed\": False,\n",
    "            \"policy_changed\": False,\n",
    "            \"spark_config_changed\": False,\n",
    "            \"tags_changed\": False,\n",
    "            \"init_scripts_changed\": False,\n",
    "            \"autotermination_changed\": False,\n",
    "            \"runtime_engine_changed\": False,\n",
    "            \"security_mode_changed\": False,\n",
    "            \"autoscale_changed\": False,\n",
    "            \"change_summary\": \"No changes detected\",\n",
    "            \"change_details\": \"\"\n",
    "        }\n",
    "    \n",
    "    changes = []\n",
    "    change_details = []\n",
    "    change_count = 0\n",
    "    \n",
    "    # Check instance type changes\n",
    "    instance_changed = (\n",
    "        before_config.get(\"driver_instance_type\") != after_config.get(\"driver_instance_type\") or\n",
    "        before_config.get(\"worker_instance_type\") != after_config.get(\"worker_instance_type\")\n",
    "    )\n",
    "    if instance_changed:\n",
    "        changes.append(\"instance_type\")\n",
    "        change_count += 1\n",
    "        change_details.append(\n",
    "            f\"Instance Types: Driver {before_config.get('driver_instance_type')} ‚Üí {after_config.get('driver_instance_type')}, \"\n",
    "            f\"Worker {before_config.get('worker_instance_type')} ‚Üí {after_config.get('worker_instance_type')}\"\n",
    "        )\n",
    "    \n",
    "    # Check policy changes\n",
    "    policy_changed = before_config.get(\"policy_id\") != after_config.get(\"policy_id\")\n",
    "    if policy_changed:\n",
    "        changes.append(\"policy\")\n",
    "        change_count += 1\n",
    "        change_details.append(f\"Policy: {before_config.get('policy_id')} ‚Üí {after_config.get('policy_id')}\")\n",
    "    \n",
    "    # Check Spark config changes\n",
    "    before_spark = before_config.get(\"spark_conf\") or {}\n",
    "    after_spark = after_config.get(\"spark_conf\") or {}\n",
    "    spark_changed = before_spark != after_spark\n",
    "    if spark_changed:\n",
    "        changes.append(\"spark_config\")\n",
    "        change_count += 1\n",
    "        change_details.append(f\"Spark Configs: {len(before_spark)} ‚Üí {len(after_spark)} settings\")\n",
    "    \n",
    "    # Check custom tags changes\n",
    "    before_tags = before_config.get(\"custom_tags\") or {}\n",
    "    after_tags = after_config.get(\"custom_tags\") or {}\n",
    "    tags_changed = before_tags != after_tags\n",
    "    if tags_changed:\n",
    "        changes.append(\"tags\")\n",
    "        change_count += 1\n",
    "        change_details.append(f\"Custom Tags: {len(before_tags)} ‚Üí {len(after_tags)} tags\")\n",
    "    \n",
    "    # Check init scripts changes\n",
    "    before_scripts = before_config.get(\"init_scripts\") or []\n",
    "    after_scripts = after_config.get(\"init_scripts\") or []\n",
    "    scripts_changed = len(before_scripts) != len(after_scripts)\n",
    "    if scripts_changed:\n",
    "        changes.append(\"init_scripts\")\n",
    "        change_count += 1\n",
    "        change_details.append(f\"Init Scripts: {len(before_scripts)} ‚Üí {len(after_scripts)} scripts\")\n",
    "    \n",
    "    # Check autotermination changes\n",
    "    autoterm_changed = before_config.get(\"autotermination_minutes\") != after_config.get(\"autotermination_minutes\")\n",
    "    if autoterm_changed:\n",
    "        changes.append(\"autotermination\")\n",
    "        change_count += 1\n",
    "        change_details.append(\n",
    "            f\"Autotermination: {before_config.get('autotermination_minutes')} ‚Üí {after_config.get('autotermination_minutes')} minutes\"\n",
    "        )\n",
    "    \n",
    "    # Check runtime engine changes\n",
    "    before_engine = str(before_config.get(\"runtime_engine\", \"STANDARD\")).upper()\n",
    "    after_engine = str(after_config.get(\"runtime_engine\", \"STANDARD\")).upper()\n",
    "    engine_changed = before_engine != after_engine\n",
    "    if engine_changed:\n",
    "        changes.append(\"runtime_engine\")\n",
    "        change_count += 1\n",
    "        change_details.append(f\"Runtime Engine: {before_engine} ‚Üí {after_engine}\")\n",
    "    \n",
    "    # Check security mode changes\n",
    "    security_changed = before_config.get(\"data_security_mode\") != after_config.get(\"data_security_mode\")\n",
    "    if security_changed:\n",
    "        changes.append(\"security_mode\")\n",
    "        change_count += 1\n",
    "        change_details.append(\n",
    "            f\"Security Mode: {before_config.get('data_security_mode')} ‚Üí {after_config.get('data_security_mode')}\"\n",
    "        )\n",
    "    \n",
    "    # Check autoscale changes\n",
    "    before_min = before_config.get(\"min_workers\")\n",
    "    after_min = after_config.get(\"min_workers\")\n",
    "    before_max = before_config.get(\"max_workers\")\n",
    "    after_max = after_config.get(\"max_workers\")\n",
    "    autoscale_changed = before_min != after_min or before_max != after_max\n",
    "    if autoscale_changed:\n",
    "        changes.append(\"autoscale\")\n",
    "        change_count += 1\n",
    "        change_details.append(f\"Autoscale: {before_min}-{before_max} ‚Üí {after_min}-{after_max} workers\")\n",
    "    \n",
    "    # Determine impact level\n",
    "    if instance_changed or policy_changed or security_changed:\n",
    "        impact = \"MAJOR\"\n",
    "    elif change_count >= 3:\n",
    "        impact = \"MODERATE\"\n",
    "    elif change_count > 0:\n",
    "        impact = \"MINOR\"\n",
    "    else:\n",
    "        impact = \"NONE\"\n",
    "    \n",
    "    # Build change summary\n",
    "    if not changes:\n",
    "        summary = \"No changes detected\"\n",
    "    else:\n",
    "        summary = \", \".join(change_details[:3])  # First 3 changes\n",
    "        if len(change_details) > 3:\n",
    "            summary += f\" (+{len(change_details)-3} more)\"\n",
    "    \n",
    "    return {\n",
    "        \"change_categories\": \",\".join(changes) if changes else None,\n",
    "        \"total_changes_count\": change_count,\n",
    "        \"change_impact\": impact,\n",
    "        \"instance_type_changed\": instance_changed,\n",
    "        \"policy_changed\": policy_changed,\n",
    "        \"spark_config_changed\": spark_changed,\n",
    "        \"tags_changed\": tags_changed,\n",
    "        \"init_scripts_changed\": scripts_changed,\n",
    "        \"autotermination_changed\": autoterm_changed,\n",
    "        \"runtime_engine_changed\": engine_changed,\n",
    "        \"security_mode_changed\": security_changed,\n",
    "        \"autoscale_changed\": autoscale_changed,\n",
    "        \"change_summary\": summary,\n",
    "        \"change_details\": \" | \".join(change_details)\n",
    "    }\n",
    "\n",
    "def get_cluster_current_config(ws_client, cluster_id):\n",
    "    \"\"\"Fetch current cluster configuration with ALL settings to preserve\"\"\"\n",
    "    try:\n",
    "        cluster = ws_client.clusters.get(cluster_id=cluster_id)\n",
    "        return {\n",
    "            # Basic configuration\n",
    "            \"driver_instance_type\": cluster.driver_node_type_id,\n",
    "            \"worker_instance_type\": cluster.node_type_id,\n",
    "            \"min_workers\": cluster.autoscale.min_workers if cluster.autoscale else None,\n",
    "            \"max_workers\": cluster.autoscale.max_workers if cluster.autoscale else None,\n",
    "            \"num_workers\": cluster.num_workers if not cluster.autoscale else None,\n",
    "            \"state\": cluster.state.value if cluster.state else None,\n",
    "            \"spark_version\": cluster.spark_version,\n",
    "            \"cluster_name\": cluster.cluster_name,\n",
    "            \"autoscale\": cluster.autoscale,\n",
    "            \n",
    "            # CRITICAL: Preserve all cluster settings\n",
    "            \"policy_id\": cluster.policy_id,\n",
    "            \"spark_conf\": cluster.spark_conf,\n",
    "            \"custom_tags\": cluster.custom_tags,\n",
    "            \"init_scripts\": cluster.init_scripts,\n",
    "            \"cluster_log_conf\": cluster.cluster_log_conf,\n",
    "            \"ssh_public_keys\": cluster.ssh_public_keys,\n",
    "            \"aws_attributes\": cluster.aws_attributes,\n",
    "            \"spark_env_vars\": cluster.spark_env_vars,\n",
    "            \"enable_elastic_disk\": cluster.enable_elastic_disk,\n",
    "            \"enable_local_disk_encryption\": cluster.enable_local_disk_encryption,\n",
    "            \"instance_pool_id\": cluster.instance_pool_id,\n",
    "            \"driver_instance_pool_id\": cluster.driver_instance_pool_id,\n",
    "            \"data_security_mode\": cluster.data_security_mode,\n",
    "            \"runtime_engine\": cluster.runtime_engine,\n",
    "            \"autotermination_minutes\": cluster.autotermination_minutes,\n",
    "            \"single_user_name\": cluster.single_user_name,\n",
    "            \"docker_image\": cluster.docker_image,\n",
    "            \n",
    "            \"cluster_object\": cluster  # Store full cluster object for reference\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def validate_cluster_config(current_config, expected_driver, expected_worker, expected_min, expected_max):\n",
    "    \"\"\"Validate if current cluster config matches expected values from opportunities table\"\"\"\n",
    "    if \"error\" in current_config:\n",
    "        return False, f\"Failed to fetch cluster: {current_config['error']}\"\n",
    "    \n",
    "    # Check if instance types match (before update)\n",
    "    driver_matches = current_config.get(\"driver_instance_type\") == expected_driver\n",
    "    worker_matches = current_config.get(\"worker_instance_type\") == expected_worker\n",
    "    \n",
    "    # For validation, we check if current config matches what we expect to change FROM\n",
    "    # This prevents updating clusters that have already been modified\n",
    "    validation_msg = []\n",
    "    if not driver_matches:\n",
    "        validation_msg.append(f\"Driver mismatch: current={current_config.get('driver_instance_type')}, expected={expected_driver}\")\n",
    "    if not worker_matches:\n",
    "        validation_msg.append(f\"Worker mismatch: current={current_config.get('worker_instance_type')}, expected={expected_worker}\")\n",
    "    \n",
    "    is_valid = driver_matches and worker_matches\n",
    "    msg = \"; \".join(validation_msg) if validation_msg else \"Configuration matches expected state\"\n",
    "    \n",
    "    return is_valid, msg\n",
    "\n",
    "print(\"‚úì Cluster update functions defined using Databricks SDK with token authentication\")\n",
    "print(\"‚úì FIXED: Now capturing and preserving ALL cluster settings during updates\")\n",
    "print(\"‚úì NEW: Added config serialization for backup table\")\n",
    "print(\"‚úì NEW: Added change analysis function for dashboard-friendly fields\")\n",
    "print(\"  - Policy, Spark config, custom tags, init scripts, log config\")\n",
    "print(\"  - SSH keys, AWS attributes, environment variables, disk settings\")\n",
    "print(\"  - Instance pools, security mode, runtime engine, autotermination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee146ff3-007c-419d-ba80-93a28c5c6b85",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper function to populate dashboard fields"
    }
   },
   "outputs": [],
   "source": [
    "def populate_dashboard_fields(backup_entry, before_config, after_config):\n",
    "    \"\"\"Populate dashboard-friendly fields in backup entry\n",
    "    \n",
    "    Args:\n",
    "        backup_entry: Dict with basic backup fields\n",
    "        before_config: Before configuration dict\n",
    "        after_config: After configuration dict (can be None for dry-run/failed)\n",
    "    \n",
    "    Returns:\n",
    "        Updated backup_entry with all dashboard fields populated\n",
    "    \"\"\"\n",
    "    # Extract counts from before config\n",
    "    before_spark_conf = before_config.get(\"spark_conf\") or {}\n",
    "    before_tags = before_config.get(\"custom_tags\") or {}\n",
    "    before_scripts = before_config.get(\"init_scripts\") or []\n",
    "    \n",
    "    backup_entry[\"before_spark_config_count\"] = len(before_spark_conf)\n",
    "    backup_entry[\"before_custom_tags_count\"] = len(before_tags)\n",
    "    backup_entry[\"before_init_scripts_count\"] = len(before_scripts)\n",
    "    backup_entry[\"before_autotermination_minutes\"] = before_config.get(\"autotermination_minutes\")\n",
    "    backup_entry[\"before_runtime_engine\"] = str(before_config.get(\"runtime_engine\", \"STANDARD\")).upper() if before_config.get(\"runtime_engine\") else None\n",
    "    backup_entry[\"before_data_security_mode\"] = str(before_config.get(\"data_security_mode\")) if before_config.get(\"data_security_mode\") else None\n",
    "    \n",
    "    # Extract counts from after config (if available)\n",
    "    if after_config and \"error\" not in after_config:\n",
    "        after_spark_conf = after_config.get(\"spark_conf\") or {}\n",
    "        after_tags = after_config.get(\"custom_tags\") or {}\n",
    "        after_scripts = after_config.get(\"init_scripts\") or []\n",
    "        \n",
    "        backup_entry[\"after_spark_config_count\"] = len(after_spark_conf)\n",
    "        backup_entry[\"after_custom_tags_count\"] = len(after_tags)\n",
    "        backup_entry[\"after_init_scripts_count\"] = len(after_scripts)\n",
    "        backup_entry[\"after_autotermination_minutes\"] = after_config.get(\"autotermination_minutes\")\n",
    "        backup_entry[\"after_runtime_engine\"] = str(after_config.get(\"runtime_engine\", \"STANDARD\")).upper() if after_config.get(\"runtime_engine\") else None\n",
    "        backup_entry[\"after_data_security_mode\"] = str(after_config.get(\"data_security_mode\")) if after_config.get(\"data_security_mode\") else None\n",
    "        \n",
    "        # Analyze changes\n",
    "        change_analysis = analyze_config_changes(before_config, after_config)\n",
    "    else:\n",
    "        # No after config (dry-run or failed) - use before config values\n",
    "        backup_entry[\"after_spark_config_count\"] = len(before_spark_conf)\n",
    "        backup_entry[\"after_custom_tags_count\"] = len(before_tags)\n",
    "        backup_entry[\"after_init_scripts_count\"] = len(before_scripts)\n",
    "        backup_entry[\"after_autotermination_minutes\"] = before_config.get(\"autotermination_minutes\")\n",
    "        backup_entry[\"after_runtime_engine\"] = backup_entry[\"before_runtime_engine\"]\n",
    "        backup_entry[\"after_data_security_mode\"] = backup_entry[\"before_data_security_mode\"]\n",
    "        \n",
    "        # For dry-run, simulate the instance type change\n",
    "        simulated_after = before_config.copy()\n",
    "        simulated_after[\"driver_instance_type\"] = backup_entry.get(\"after_driver_instance\")\n",
    "        simulated_after[\"worker_instance_type\"] = backup_entry.get(\"after_worker_instance\")\n",
    "        change_analysis = analyze_config_changes(before_config, simulated_after)\n",
    "    \n",
    "    # Add change analysis fields\n",
    "    backup_entry[\"change_categories\"] = change_analysis[\"change_categories\"]\n",
    "    backup_entry[\"total_changes_count\"] = change_analysis[\"total_changes_count\"]\n",
    "    backup_entry[\"change_impact\"] = change_analysis[\"change_impact\"]\n",
    "    backup_entry[\"instance_type_changed\"] = change_analysis[\"instance_type_changed\"]\n",
    "    backup_entry[\"policy_changed\"] = change_analysis[\"policy_changed\"]\n",
    "    backup_entry[\"spark_config_changed\"] = change_analysis[\"spark_config_changed\"]\n",
    "    backup_entry[\"tags_changed\"] = change_analysis[\"tags_changed\"]\n",
    "    backup_entry[\"init_scripts_changed\"] = change_analysis[\"init_scripts_changed\"]\n",
    "    backup_entry[\"autotermination_changed\"] = change_analysis[\"autotermination_changed\"]\n",
    "    backup_entry[\"runtime_engine_changed\"] = change_analysis[\"runtime_engine_changed\"]\n",
    "    backup_entry[\"security_mode_changed\"] = change_analysis[\"security_mode_changed\"]\n",
    "    backup_entry[\"autoscale_changed\"] = change_analysis[\"autoscale_changed\"]\n",
    "    backup_entry[\"change_summary\"] = change_analysis[\"change_summary\"]\n",
    "    backup_entry[\"change_details\"] = change_analysis[\"change_details\"]\n",
    "    \n",
    "    return backup_entry\n",
    "\n",
    "print(\"‚úì Dashboard field population helper function defined\")\n",
    "print(\"  - Extracts setting counts from configs\")\n",
    "print(\"  - Analyzes changes between before/after\")\n",
    "print(\"  - Generates user-friendly summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fe0711-f097-4776-a2cf-35099e312aec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main update orchestration function"
    }
   },
   "outputs": [],
   "source": [
    "def update_cluster_with_recommendation(row, dry_run=True, batch_metadata=None):\n",
    "    \"\"\"Process a single cluster update based on recommendation\n",
    "    \n",
    "    Args:\n",
    "        row: Cluster data row\n",
    "        dry_run: If True, only preview changes without updating\n",
    "        batch_metadata: Dict containing batch tracking info (batch_id, start_time, etc.)\n",
    "    \"\"\"\n",
    "    # Initialize batch metadata if not provided\n",
    "    if batch_metadata is None:\n",
    "        batch_metadata = {\n",
    "            \"batch_id\": \"unknown\",\n",
    "            \"execution_label\": \"unknown\",\n",
    "            \"batch_start_time\": datetime.now(),\n",
    "            \"batch_end_time\": None,\n",
    "            \"execution_mode\": \"DRY_RUN\" if dry_run else \"LIVE_UPDATE\",\n",
    "            \"workspace_filter_applied\": \"\",\n",
    "            \"total_clusters_in_batch\": 0,\n",
    "            \"executed_by_user\": \"unknown\"\n",
    "        }\n",
    "    \n",
    "    log_entry = {\n",
    "        # Batch metadata\n",
    "        \"batch_id\": batch_metadata[\"batch_id\"],\n",
    "        \"execution_label\": batch_metadata.get(\"execution_label\", \"unknown\"),\n",
    "        \"batch_start_time\": batch_metadata[\"batch_start_time\"],\n",
    "        \"batch_end_time\": batch_metadata.get(\"batch_end_time\"),\n",
    "        \"execution_mode\": batch_metadata[\"execution_mode\"],\n",
    "        \"workspace_filter_applied\": batch_metadata[\"workspace_filter_applied\"],\n",
    "        \"total_clusters_in_batch\": batch_metadata[\"total_clusters_in_batch\"],\n",
    "        \"executed_by_user\": batch_metadata[\"executed_by_user\"],\n",
    "        \n",
    "        # Individual cluster details\n",
    "        \"log_id\": f\"{row.cluster_id}_{int(time.time() * 1000)}\",\n",
    "        \"cluster_id\": row.cluster_id,\n",
    "        \"cluster_name\": row.cluster_name,\n",
    "        \"workspace_name\": row.workspace_name,\n",
    "        \"workspace_id\": row.workspace_id,\n",
    "        \"deployment_url\": f\"https://{row.deployment_name}.cloud.databricks.com\" if row.deployment_name else None,\n",
    "        \"action_type\": row.action_item,\n",
    "        \"recommendation\": row.recommendation,\n",
    "        \"current_driver_instance\": row.driver_instance_type,\n",
    "        \"current_worker_instance\": row.worker_instance_type,\n",
    "        \"suggested_driver_instance\": row.suggested_driver_instance,\n",
    "        \"suggested_worker_instance\": row.suggested_worker_instance,\n",
    "        \"current_min_workers\": row.min_workers,\n",
    "        \"current_max_workers\": row.max_workers,\n",
    "        \"validation_status\": \"PENDING\",\n",
    "        \"validation_message\": \"\",\n",
    "        \"update_status\": \"PENDING\",\n",
    "        \"update_message\": \"\",\n",
    "        \"dry_run\": dry_run,\n",
    "        \"validated_savings\": row.validated_savings,\n",
    "        \"execution_timestamp\": datetime.now(),\n",
    "        \"error_details\": None,\n",
    "        \"implementation_notes\": row.implementation_notes if hasattr(row, 'implementation_notes') else None\n",
    "    }\n",
    "    \n",
    "    # Initialize backup entry (will be populated if update proceeds)\n",
    "    backup_entry = None\n",
    "    \n",
    "    try:\n",
    "        # Skip if no deployment name\n",
    "        if not row.deployment_name:\n",
    "            log_entry[\"validation_status\"] = \"FAILED\"\n",
    "            log_entry[\"validation_message\"] = \"No deployment name found for workspace\"\n",
    "            log_entry[\"update_status\"] = \"SKIPPED\"\n",
    "            log_entry[\"update_message\"] = \"Skipped: No deployment name found for workspace\"\n",
    "            return log_entry, None\n",
    "        \n",
    "        # Create workspace client using token authentication\n",
    "        try:\n",
    "            ws_client = get_workspace_client(row.deployment_name, token)\n",
    "        except Exception as client_error:\n",
    "            error_msg = str(client_error)\n",
    "            log_entry[\"validation_status\"] = \"FAILED\"\n",
    "            if \"403\" in error_msg or \"Cert validation failed\" in error_msg or \"certificate\" in error_msg.lower():\n",
    "                log_entry[\"validation_message\"] = f\"Connectivity failed to workspace '{row.workspace_name}' - Cross-workspace access denied or certificate validation failed\"\n",
    "                log_entry[\"update_message\"] = f\"Skipped: Cannot connect to workspace '{row.workspace_name}' - Cross-workspace access issue\"\n",
    "            else:\n",
    "                log_entry[\"validation_message\"] = f\"Failed to create workspace client: {error_msg[:200]}\"\n",
    "                log_entry[\"update_message\"] = f\"Skipped: Failed to create workspace client - {error_msg[:100]}\"\n",
    "            log_entry[\"update_status\"] = \"SKIPPED\"\n",
    "            log_entry[\"error_details\"] = error_msg\n",
    "            return log_entry, None\n",
    "        \n",
    "        # Get current cluster configuration (BEFORE state)\n",
    "        before_config = get_cluster_current_config(ws_client, row.cluster_id)\n",
    "        \n",
    "        # Check for connectivity errors\n",
    "        if \"error\" in before_config:\n",
    "            error_msg = before_config[\"error\"]\n",
    "            log_entry[\"validation_status\"] = \"FAILED\"\n",
    "            if \"403\" in error_msg or \"Cert validation failed\" in error_msg or \"certificate\" in error_msg.lower():\n",
    "                log_entry[\"validation_message\"] = f\"Connectivity failed to workspace '{row.workspace_name}' - Cross-workspace access denied or certificate validation failed\"\n",
    "                log_entry[\"update_message\"] = f\"Skipped: Cannot fetch cluster from workspace '{row.workspace_name}' - Cross-workspace access issue\"\n",
    "            else:\n",
    "                log_entry[\"validation_message\"] = f\"Failed to fetch cluster: {error_msg[:200]}\"\n",
    "                log_entry[\"update_message\"] = f\"Skipped: Failed to fetch cluster - {error_msg[:100]}\"\n",
    "            log_entry[\"update_status\"] = \"SKIPPED\"\n",
    "            log_entry[\"error_details\"] = error_msg\n",
    "            return log_entry, None\n",
    "        \n",
    "        # Extract cluster creator/owner from cluster object\n",
    "        cluster_creator = None\n",
    "        if \"cluster_object\" in before_config and before_config[\"cluster_object\"]:\n",
    "            cluster_obj = before_config[\"cluster_object\"]\n",
    "            cluster_creator = cluster_obj.creator_user_name if hasattr(cluster_obj, 'creator_user_name') else None\n",
    "        \n",
    "        # Check if cluster uses instance pools - cannot update instance types for pooled clusters\n",
    "        uses_instance_pool = before_config.get(\"instance_pool_id\") or before_config.get(\"driver_instance_pool_id\")\n",
    "        if uses_instance_pool:\n",
    "            log_entry[\"validation_status\"] = \"FAILED\"\n",
    "            log_entry[\"validation_message\"] = \"Cluster uses instance pools - cannot change instance types (pools define the node types)\"\n",
    "            log_entry[\"update_status\"] = \"SKIPPED\"\n",
    "            log_entry[\"update_message\"] = \"Skipped: Cluster uses instance pools - instance types are controlled by the pool configuration\"\n",
    "            return log_entry, None\n",
    "        \n",
    "        # Check if cluster is in RUNNING or PENDING state\n",
    "        cluster_state = before_config.get(\"state\")\n",
    "        if cluster_state in [\"RUNNING\", \"PENDING\"]:\n",
    "            log_entry[\"validation_status\"] = \"FAILED\"\n",
    "            log_entry[\"validation_message\"] = f\"Cluster is in {cluster_state} state - cannot update running or starting clusters\"\n",
    "            log_entry[\"update_status\"] = \"SKIPPED\"\n",
    "            log_entry[\"update_message\"] = f\"Skipped: Cluster in {cluster_state} state - stop cluster before updating\"\n",
    "            return log_entry, None\n",
    "        \n",
    "        # Validate cluster configuration matches expected state\n",
    "        is_valid, validation_msg = validate_cluster_config(\n",
    "            before_config,\n",
    "            row.driver_instance_type,\n",
    "            row.worker_instance_type,\n",
    "            row.min_workers,\n",
    "            row.max_workers\n",
    "        )\n",
    "        \n",
    "        log_entry[\"validation_status\"] = \"PASSED\" if is_valid else \"FAILED\"\n",
    "        log_entry[\"validation_message\"] = validation_msg\n",
    "        \n",
    "        if not is_valid:\n",
    "            log_entry[\"update_status\"] = \"SKIPPED\"\n",
    "            log_entry[\"update_message\"] = f\"Skipped: Cluster config mismatch - {validation_msg[:150]}\"\n",
    "            return log_entry, None\n",
    "        \n",
    "        # Proceed with update if validation passed\n",
    "        if dry_run:\n",
    "            log_entry[\"update_status\"] = \"DRY_RUN\"\n",
    "            preserved_settings = []\n",
    "            if before_config.get('policy_id'):\n",
    "                preserved_settings.append(f\"Policy: {before_config['policy_id']}\")\n",
    "            if before_config.get('spark_conf'):\n",
    "                preserved_settings.append(f\"Spark configs: {len(before_config['spark_conf'])} settings\")\n",
    "            if before_config.get('init_scripts'):\n",
    "                preserved_settings.append(f\"Init scripts: {len(before_config['init_scripts'])}\")\n",
    "            if before_config.get('custom_tags'):\n",
    "                preserved_settings.append(f\"Custom tags: {len(before_config['custom_tags'])}\")\n",
    "            \n",
    "            settings_note = f\" | Preserving: {', '.join(preserved_settings)}\" if preserved_settings else \" | No additional settings to preserve\"\n",
    "            log_entry[\"update_message\"] = f\"Would update: Driver {row.driver_instance_type}‚Üí{row.suggested_driver_instance}, Worker {row.worker_instance_type}‚Üí{row.suggested_worker_instance}{settings_note}\"\n",
    "            \n",
    "            # Create backup entry for dry run (for preview)\n",
    "            backup_entry = {\n",
    "                \"backup_id\": f\"{row.cluster_id}_{int(time.time() * 1000)}_dryrun\",\n",
    "                \"batch_id\": batch_metadata[\"batch_id\"],\n",
    "                \"execution_label\": batch_metadata[\"execution_label\"],\n",
    "                \"backup_timestamp\": datetime.now(),\n",
    "                \"cluster_id\": row.cluster_id,\n",
    "                \"cluster_name\": row.cluster_name,\n",
    "                \"cluster_creator\": cluster_creator,  # ADDED: Cluster owner\n",
    "                \"workspace_name\": row.workspace_name,\n",
    "                \"workspace_id\": row.workspace_id,\n",
    "                \"deployment_url\": log_entry[\"deployment_url\"],\n",
    "                \"update_status\": \"DRY_RUN\",\n",
    "                \"update_reason\": row.recommendation,\n",
    "                \"updated_by_user\": batch_metadata[\"executed_by_user\"],\n",
    "                \"before_config\": serialize_cluster_config(before_config),\n",
    "                \"before_driver_instance\": before_config.get(\"driver_instance_type\"),\n",
    "                \"before_worker_instance\": before_config.get(\"worker_instance_type\"),\n",
    "                \"before_policy_id\": before_config.get(\"policy_id\"),\n",
    "                \"before_autoscale_min\": before_config.get(\"min_workers\"),\n",
    "                \"before_autoscale_max\": before_config.get(\"max_workers\"),\n",
    "                \"before_num_workers\": before_config.get(\"num_workers\"),\n",
    "                \"after_config\": None,  # Not updated in dry run\n",
    "                \"after_driver_instance\": row.suggested_driver_instance,\n",
    "                \"after_worker_instance\": row.suggested_worker_instance,\n",
    "                \"after_policy_id\": before_config.get(\"policy_id\"),  # Would be preserved\n",
    "                \"after_autoscale_min\": before_config.get(\"min_workers\"),\n",
    "                \"after_autoscale_max\": before_config.get(\"max_workers\"),\n",
    "                \"after_num_workers\": before_config.get(\"num_workers\"),\n",
    "                \"is_reverted\": False,\n",
    "                \"revert_timestamp\": None,\n",
    "                \"revert_batch_id\": None,\n",
    "                \"reverted_by_user\": None\n",
    "            }\n",
    "            # Populate dashboard fields\n",
    "            backup_entry = populate_dashboard_fields(backup_entry, before_config, None)\n",
    "            \n",
    "        else:\n",
    "            # Perform actual update with all required parameters\n",
    "            try:\n",
    "                # Build edit parameters with all required fields\n",
    "                edit_params = {\n",
    "                    \"cluster_id\": row.cluster_id,\n",
    "                    \"cluster_name\": before_config[\"cluster_name\"],\n",
    "                    \"spark_version\": before_config[\"spark_version\"],\n",
    "                    \"node_type_id\": row.suggested_worker_instance,\n",
    "                    \"driver_node_type_id\": row.suggested_driver_instance\n",
    "                }\n",
    "                \n",
    "                # CRITICAL FIX: Preserve ALL cluster settings\n",
    "                if before_config.get(\"policy_id\"):\n",
    "                    edit_params[\"policy_id\"] = before_config[\"policy_id\"]\n",
    "                \n",
    "                if before_config.get(\"spark_conf\"):\n",
    "                    edit_params[\"spark_conf\"] = before_config[\"spark_conf\"]\n",
    "                \n",
    "                if before_config.get(\"custom_tags\"):\n",
    "                    edit_params[\"custom_tags\"] = before_config[\"custom_tags\"]\n",
    "                \n",
    "                if before_config.get(\"init_scripts\"):\n",
    "                    edit_params[\"init_scripts\"] = before_config[\"init_scripts\"]\n",
    "                \n",
    "                if before_config.get(\"cluster_log_conf\"):\n",
    "                    edit_params[\"cluster_log_conf\"] = before_config[\"cluster_log_conf\"]\n",
    "                \n",
    "                if before_config.get(\"ssh_public_keys\"):\n",
    "                    edit_params[\"ssh_public_keys\"] = before_config[\"ssh_public_keys\"]\n",
    "                \n",
    "                if before_config.get(\"aws_attributes\"):\n",
    "                    edit_params[\"aws_attributes\"] = before_config[\"aws_attributes\"]\n",
    "                \n",
    "                if before_config.get(\"spark_env_vars\"):\n",
    "                    edit_params[\"spark_env_vars\"] = before_config[\"spark_env_vars\"]\n",
    "                \n",
    "                if before_config.get(\"enable_elastic_disk\") is not None:\n",
    "                    edit_params[\"enable_elastic_disk\"] = before_config[\"enable_elastic_disk\"]\n",
    "                \n",
    "                if before_config.get(\"enable_local_disk_encryption\") is not None:\n",
    "                    edit_params[\"enable_local_disk_encryption\"] = before_config[\"enable_local_disk_encryption\"]\n",
    "                \n",
    "                # NOTE: Do NOT set instance_pool_id or driver_instance_pool_id here\n",
    "                # Instance pools and node_type_id are mutually exclusive\n",
    "                # We already filtered out pooled clusters above\n",
    "                \n",
    "                if before_config.get(\"data_security_mode\"):\n",
    "                    edit_params[\"data_security_mode\"] = before_config[\"data_security_mode\"]\n",
    "                \n",
    "                if before_config.get(\"runtime_engine\"):\n",
    "                    edit_params[\"runtime_engine\"] = before_config[\"runtime_engine\"]\n",
    "                \n",
    "                if before_config.get(\"autotermination_minutes\") is not None:\n",
    "                    edit_params[\"autotermination_minutes\"] = before_config[\"autotermination_minutes\"]\n",
    "                \n",
    "                if before_config.get(\"single_user_name\"):\n",
    "                    edit_params[\"single_user_name\"] = before_config[\"single_user_name\"]\n",
    "                \n",
    "                if before_config.get(\"docker_image\"):\n",
    "                    edit_params[\"docker_image\"] = before_config[\"docker_image\"]\n",
    "                \n",
    "                # Add autoscale or num_workers based on current configuration\n",
    "                if before_config[\"autoscale\"]:\n",
    "                    edit_params[\"autoscale\"] = before_config[\"autoscale\"]\n",
    "                else:\n",
    "                    edit_params[\"num_workers\"] = before_config[\"num_workers\"]\n",
    "                \n",
    "                # PERFORM THE UPDATE\n",
    "                ws_client.clusters.edit(**edit_params)\n",
    "                \n",
    "                # Get AFTER configuration\n",
    "                time.sleep(2)  # Brief pause to let API update\n",
    "                after_config = get_cluster_current_config(ws_client, row.cluster_id)\n",
    "                \n",
    "                # Build success message with preserved settings\n",
    "                preserved_items = []\n",
    "                if before_config.get('policy_id'):\n",
    "                    preserved_items.append(\"policy\")\n",
    "                if before_config.get('spark_conf'):\n",
    "                    preserved_items.append(\"spark configs\")\n",
    "                if before_config.get('init_scripts'):\n",
    "                    preserved_items.append(\"init scripts\")\n",
    "                if before_config.get('custom_tags'):\n",
    "                    preserved_items.append(\"custom tags\")\n",
    "                if before_config.get('autotermination_minutes'):\n",
    "                    preserved_items.append(\"autotermination\")\n",
    "                \n",
    "                preserved_note = f\" | Preserved: {', '.join(preserved_items)}\" if preserved_items else \"\"\n",
    "                log_entry[\"update_status\"] = \"SUCCESS\"\n",
    "                log_entry[\"update_message\"] = f\"Updated: Driver {row.driver_instance_type}‚Üí{row.suggested_driver_instance}, Worker {row.worker_instance_type}‚Üí{row.suggested_worker_instance}{preserved_note}\"\n",
    "                \n",
    "                # Create backup entry with BEFORE and AFTER configs\n",
    "                backup_entry = {\n",
    "                    \"backup_id\": f\"{row.cluster_id}_{int(time.time() * 1000)}\",\n",
    "                    \"batch_id\": batch_metadata[\"batch_id\"],\n",
    "                    \"execution_label\": batch_metadata[\"execution_label\"],\n",
    "                    \"backup_timestamp\": datetime.now(),\n",
    "                    \"cluster_id\": row.cluster_id,\n",
    "                    \"cluster_name\": row.cluster_name,\n",
    "                    \"cluster_creator\": cluster_creator,  # ADDED: Cluster owner\n",
    "                    \"workspace_name\": row.workspace_name,\n",
    "                    \"workspace_id\": row.workspace_id,\n",
    "                    \"deployment_url\": log_entry[\"deployment_url\"],\n",
    "                    \"update_status\": \"SUCCESS\",\n",
    "                    \"update_reason\": row.recommendation,\n",
    "                    \"updated_by_user\": batch_metadata[\"executed_by_user\"],\n",
    "                    \"before_config\": serialize_cluster_config(before_config),\n",
    "                    \"before_driver_instance\": before_config.get(\"driver_instance_type\"),\n",
    "                    \"before_worker_instance\": before_config.get(\"worker_instance_type\"),\n",
    "                    \"before_policy_id\": before_config.get(\"policy_id\"),\n",
    "                    \"before_autoscale_min\": before_config.get(\"min_workers\"),\n",
    "                    \"before_autoscale_max\": before_config.get(\"max_workers\"),\n",
    "                    \"before_num_workers\": before_config.get(\"num_workers\"),\n",
    "                    \"after_config\": serialize_cluster_config(after_config),\n",
    "                    \"after_driver_instance\": after_config.get(\"driver_instance_type\"),\n",
    "                    \"after_worker_instance\": after_config.get(\"worker_instance_type\"),\n",
    "                    \"after_policy_id\": after_config.get(\"policy_id\"),\n",
    "                    \"after_autoscale_min\": after_config.get(\"min_workers\"),\n",
    "                    \"after_autoscale_max\": after_config.get(\"max_workers\"),\n",
    "                    \"after_num_workers\": after_config.get(\"num_workers\"),\n",
    "                    \"is_reverted\": False,\n",
    "                    \"revert_timestamp\": None,\n",
    "                    \"revert_batch_id\": None,\n",
    "                    \"reverted_by_user\": None\n",
    "                }\n",
    "                # Populate dashboard fields\n",
    "                backup_entry = populate_dashboard_fields(backup_entry, before_config, after_config)\n",
    "                \n",
    "            except Exception as update_error:\n",
    "                error_msg = str(update_error)\n",
    "                log_entry[\"update_status\"] = \"FAILED\"\n",
    "                if \"403\" in error_msg or \"Cert validation failed\" in error_msg or \"certificate\" in error_msg.lower():\n",
    "                    log_entry[\"update_message\"] = f\"Failed: Connectivity issue to workspace '{row.workspace_name}' during update\"\n",
    "                else:\n",
    "                    log_entry[\"update_message\"] = f\"Failed: {error_msg[:200]}\"\n",
    "                log_entry[\"error_details\"] = error_msg\n",
    "                \n",
    "                # Create backup entry for failed update (still save before config)\n",
    "                backup_entry = {\n",
    "                    \"backup_id\": f\"{row.cluster_id}_{int(time.time() * 1000)}_failed\",\n",
    "                    \"batch_id\": batch_metadata[\"batch_id\"],\n",
    "                    \"execution_label\": batch_metadata[\"execution_label\"],\n",
    "                    \"backup_timestamp\": datetime.now(),\n",
    "                    \"cluster_id\": row.cluster_id,\n",
    "                    \"cluster_name\": row.cluster_name,\n",
    "                    \"cluster_creator\": cluster_creator,  # ADDED: Cluster owner\n",
    "                    \"workspace_name\": row.workspace_name,\n",
    "                    \"workspace_id\": row.workspace_id,\n",
    "                    \"deployment_url\": log_entry[\"deployment_url\"],\n",
    "                    \"update_status\": \"FAILED\",\n",
    "                    \"update_reason\": row.recommendation,\n",
    "                    \"updated_by_user\": batch_metadata[\"executed_by_user\"],\n",
    "                    \"before_config\": serialize_cluster_config(before_config),\n",
    "                    \"before_driver_instance\": before_config.get(\"driver_instance_type\"),\n",
    "                    \"before_worker_instance\": before_config.get(\"worker_instance_type\"),\n",
    "                    \"before_policy_id\": before_config.get(\"policy_id\"),\n",
    "                    \"before_autoscale_min\": before_config.get(\"min_workers\"),\n",
    "                    \"before_autoscale_max\": before_config.get(\"max_workers\"),\n",
    "                    \"before_num_workers\": before_config.get(\"num_workers\"),\n",
    "                    \"after_config\": None,\n",
    "                    \"after_driver_instance\": None,\n",
    "                    \"after_worker_instance\": None,\n",
    "                    \"after_policy_id\": None,\n",
    "                    \"after_autoscale_min\": None,\n",
    "                    \"after_autoscale_max\": None,\n",
    "                    \"after_num_workers\": None,\n",
    "                    \"is_reverted\": False,\n",
    "                    \"revert_timestamp\": None,\n",
    "                    \"revert_batch_id\": None,\n",
    "                    \"reverted_by_user\": None\n",
    "                }\n",
    "                # Populate dashboard fields\n",
    "                backup_entry = populate_dashboard_fields(backup_entry, before_config, None)\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        log_entry[\"validation_status\"] = \"ERROR\"\n",
    "        log_entry[\"update_status\"] = \"FAILED\"\n",
    "        log_entry[\"error_details\"] = error_msg\n",
    "        if \"403\" in error_msg or \"Cert validation failed\" in error_msg or \"certificate\" in error_msg.lower():\n",
    "            log_entry[\"validation_message\"] = f\"Connectivity failed to workspace '{row.workspace_name}' - Cross-workspace access denied or certificate validation failed\"\n",
    "            log_entry[\"update_message\"] = f\"Failed: Cannot connect to workspace '{row.workspace_name}'\"\n",
    "        else:\n",
    "            log_entry[\"validation_message\"] = f\"Error: {error_msg[:200]}\"\n",
    "            log_entry[\"update_message\"] = f\"Failed: {error_msg[:200]}\"\n",
    "    \n",
    "    return log_entry, backup_entry\n",
    "\n",
    "print(\"‚úì Main orchestration function defined with token authentication and implementation_notes support\")\n",
    "print(\"‚úì PRODUCTION-READY: Now capturing cluster_creator (owner) in all backup entries\")\n",
    "print(\"‚úì FIXED: Now preserving ALL cluster settings during updates\")\n",
    "print(\"‚úì FIXED: Instance pool clusters are now properly detected and skipped\")\n",
    "print(\"‚úì NEW: Now capturing before/after configs for rollback capability\")\n",
    "print(\"‚úì NEW: Dashboard-friendly fields automatically populated for all backups\")\n",
    "print(\"  - Policy, Spark configs, custom tags, init scripts, log config\")\n",
    "print(\"  - SSH keys, AWS attributes, environment variables, disk settings\")\n",
    "print(\"  - Security mode, runtime engine, autotermination\")\n",
    "print(\"  - Change summaries, impact levels, and user-friendly descriptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb51bf2-6182-4e14-9abb-9c19cd866fd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create logging table schema"
    }
   },
   "outputs": [],
   "source": [
    "# Create logging table schema using full_schema variable\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {full_schema}.cluster_update_log (\n",
    "  -- Batch Identification\n",
    "  batch_id STRING COMMENT 'Unique identifier for this execution batch',\n",
    "  execution_label STRING COMMENT 'Human-readable label: YYYY-MM-DD_HH-MM_MODE_WORKSPACES for easy filtering',\n",
    "  batch_start_time TIMESTAMP COMMENT 'When this batch execution started',\n",
    "  batch_end_time TIMESTAMP COMMENT 'When this batch execution completed',\n",
    "  execution_mode STRING COMMENT 'DRY_RUN or LIVE_UPDATE',\n",
    "  workspace_filter_applied STRING COMMENT 'Workspace filter used (empty = all workspaces)',\n",
    "  total_clusters_in_batch LONG COMMENT 'Total number of clusters processed in this batch',\n",
    "  executed_by_user STRING COMMENT 'User who executed this batch',\n",
    "  \n",
    "  -- Individual Cluster Details\n",
    "  log_id STRING COMMENT 'Unique identifier for this specific cluster update',\n",
    "  cluster_id STRING,\n",
    "  cluster_name STRING,\n",
    "  workspace_name STRING,\n",
    "  workspace_id LONG,\n",
    "  deployment_url STRING,\n",
    "  action_type STRING,\n",
    "  recommendation STRING,\n",
    "  current_driver_instance STRING,\n",
    "  current_worker_instance STRING,\n",
    "  suggested_driver_instance STRING,\n",
    "  suggested_worker_instance STRING,\n",
    "  current_min_workers LONG,\n",
    "  current_max_workers LONG,\n",
    "  validation_status STRING,\n",
    "  validation_message STRING,\n",
    "  update_status STRING,\n",
    "  update_message STRING,\n",
    "  dry_run BOOLEAN,\n",
    "  validated_savings DECIMAL(35,2),\n",
    "  execution_timestamp TIMESTAMP COMMENT 'When this specific cluster was processed',\n",
    "  error_details STRING,\n",
    "  implementation_notes STRING COMMENT 'Notes about instance type constraints or requirements'\n",
    ")\n",
    "USING DELTA\n",
    "COMMENT 'Log table for cluster update activities with batch tracking for filtering and analysis'\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #2e7d32;\">‚úì Logging table ready</h3>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Table:</strong> <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{full_schema}.cluster_update_log</code></p>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>New:</strong> Added implementation_notes column for tracking constraints</p>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9064ba6b-1fed-40c8-a1fc-afc9a57646d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create cluster config backup table"
    }
   },
   "outputs": [],
   "source": [
    "# Create cluster configuration backup table for rollback capability\n",
    "# Production-ready: Works in new workspaces, includes all critical fields\n",
    "\n",
    "try:\n",
    "    # Create table with all fields\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {full_schema}.cluster_config_backup (\n",
    "      -- Backup Identification\n",
    "      backup_id STRING COMMENT 'Unique identifier for this backup record',\n",
    "      batch_id STRING COMMENT 'Links to cluster_update_log batch_id',\n",
    "      execution_label STRING COMMENT 'Links to cluster_update_log execution_label',\n",
    "      backup_timestamp TIMESTAMP COMMENT 'When this backup was created',\n",
    "      \n",
    "      -- Cluster Identification\n",
    "      cluster_id STRING COMMENT 'Databricks cluster ID',\n",
    "      cluster_name STRING COMMENT 'Cluster name at time of backup',\n",
    "      cluster_creator STRING COMMENT 'Cluster owner/creator username - CRITICAL for ownership tracking',\n",
    "      workspace_name STRING COMMENT 'Workspace containing the cluster',\n",
    "      workspace_id LONG COMMENT 'Workspace ID - CRITICAL for cross-workspace operations',\n",
    "      deployment_url STRING COMMENT 'Full workspace URL',\n",
    "      \n",
    "      -- Update Context\n",
    "      update_status STRING COMMENT 'SUCCESS, FAILED, DRY_RUN, or SKIPPED - links to cluster_update_log',\n",
    "      update_reason STRING COMMENT 'Why the update was performed',\n",
    "      updated_by_user STRING COMMENT 'User who performed the update',\n",
    "      \n",
    "      -- BEFORE Configuration (Complete JSON)\n",
    "      before_config STRING COMMENT 'Complete cluster configuration BEFORE update (JSON format)',\n",
    "      before_driver_instance STRING COMMENT 'Driver instance type before update',\n",
    "      before_worker_instance STRING COMMENT 'Worker instance type before update',\n",
    "      before_policy_id STRING COMMENT 'Cluster policy ID before update',\n",
    "      before_autoscale_min INT COMMENT 'Min workers before update',\n",
    "      before_autoscale_max INT COMMENT 'Max workers before update',\n",
    "      before_num_workers INT COMMENT 'Fixed workers before update',\n",
    "      before_spark_config_count INT COMMENT 'Number of Spark configs before update',\n",
    "      before_custom_tags_count INT COMMENT 'Number of custom tags before update',\n",
    "      before_init_scripts_count INT COMMENT 'Number of init scripts before update',\n",
    "      before_autotermination_minutes INT COMMENT 'Autotermination setting before update',\n",
    "      before_runtime_engine STRING COMMENT 'Runtime engine before update',\n",
    "      before_data_security_mode STRING COMMENT 'Data security mode before update',\n",
    "      \n",
    "      -- AFTER Configuration (Complete JSON)\n",
    "      after_config STRING COMMENT 'Complete cluster configuration AFTER update (JSON format)',\n",
    "      after_driver_instance STRING COMMENT 'Driver instance type after update',\n",
    "      after_worker_instance STRING COMMENT 'Worker instance type after update',\n",
    "      after_policy_id STRING COMMENT 'Cluster policy ID after update',\n",
    "      after_autoscale_min INT COMMENT 'Min workers after update',\n",
    "      after_autoscale_max INT COMMENT 'Max workers after update',\n",
    "      after_num_workers INT COMMENT 'Fixed workers after update',\n",
    "      after_spark_config_count INT COMMENT 'Number of Spark configs after update',\n",
    "      after_custom_tags_count INT COMMENT 'Number of custom tags after update',\n",
    "      after_init_scripts_count INT COMMENT 'Number of init scripts after update',\n",
    "      after_autotermination_minutes INT COMMENT 'Autotermination setting after update',\n",
    "      after_runtime_engine STRING COMMENT 'Runtime engine after update',\n",
    "      after_data_security_mode STRING COMMENT 'Data security mode after update',\n",
    "      \n",
    "      -- Change Analysis (Dashboard-friendly)\n",
    "      change_categories STRING COMMENT 'Comma-separated list of what changed',\n",
    "      total_changes_count INT COMMENT 'Total number of configuration changes',\n",
    "      change_impact STRING COMMENT 'MINOR, MODERATE, MAJOR, or NONE',\n",
    "      instance_type_changed BOOLEAN COMMENT 'Did instance types change?',\n",
    "      policy_changed BOOLEAN COMMENT 'Did cluster policy change?',\n",
    "      spark_config_changed BOOLEAN COMMENT 'Did Spark configs change?',\n",
    "      tags_changed BOOLEAN COMMENT 'Did custom tags change?',\n",
    "      init_scripts_changed BOOLEAN COMMENT 'Did init scripts change?',\n",
    "      autotermination_changed BOOLEAN COMMENT 'Did autotermination setting change?',\n",
    "      runtime_engine_changed BOOLEAN COMMENT 'Did runtime engine change?',\n",
    "      security_mode_changed BOOLEAN COMMENT 'Did security mode change?',\n",
    "      autoscale_changed BOOLEAN COMMENT 'Did autoscale settings change?',\n",
    "      change_summary STRING COMMENT 'Human-readable summary of changes',\n",
    "      change_details STRING COMMENT 'Detailed list of all changes',\n",
    "      \n",
    "      -- Rollback Tracking\n",
    "      is_reverted BOOLEAN COMMENT 'Has this cluster been reverted to before_config?',\n",
    "      revert_timestamp TIMESTAMP COMMENT 'When the cluster was reverted (if applicable)',\n",
    "      revert_batch_id STRING COMMENT 'Batch ID of the revert operation',\n",
    "      reverted_by_user STRING COMMENT 'User who performed the revert'\n",
    "    )\n",
    "    USING DELTA\n",
    "    COMMENT 'Backup table storing complete before/after cluster configurations for rollback capability'\n",
    "    \"\"\")\n",
    "    \n",
    "    # For existing tables, ensure cluster_creator column exists (added in v2.0)\n",
    "    # This handles upgrades from older versions\n",
    "    try:\n",
    "        existing_columns = spark.sql(f\"DESCRIBE TABLE {full_schema}.cluster_config_backup\").toPandas()\n",
    "        has_cluster_creator = 'cluster_creator' in existing_columns['col_name'].values\n",
    "        \n",
    "        if not has_cluster_creator:\n",
    "            print(\"‚ö†Ô∏è  Upgrading existing table: Adding cluster_creator column...\")\n",
    "            spark.sql(f\"\"\"\n",
    "                ALTER TABLE {full_schema}.cluster_config_backup \n",
    "                ADD COLUMN cluster_creator STRING \n",
    "                COMMENT 'Cluster owner/creator username - CRITICAL for ownership tracking'\n",
    "                AFTER cluster_name\n",
    "            \"\"\")\n",
    "            print(\"‚úÖ Successfully added cluster_creator column to existing table\")\n",
    "    except Exception as alter_error:\n",
    "        if \"already exists\" in str(alter_error).lower():\n",
    "            pass  # Column already exists, no action needed\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Warning during column check: {str(alter_error)}\")\n",
    "    \n",
    "    # Verify table was created successfully\n",
    "    try:\n",
    "        table_check = spark.sql(f\"DESCRIBE TABLE {full_schema}.cluster_config_backup\")\n",
    "        table_exists = table_check.count() > 0\n",
    "    except:\n",
    "        table_exists = False\n",
    "    \n",
    "    if table_exists:\n",
    "        # Get table info for verification\n",
    "        table_info = spark.sql(f\"DESCRIBE TABLE {full_schema}.cluster_config_backup\")\n",
    "        column_count = table_info.count()\n",
    "        \n",
    "        # Verify critical fields\n",
    "        columns_df = table_info.toPandas()\n",
    "        critical_fields = ['cluster_creator', 'workspace_id', 'cluster_id', 'cluster_name']\n",
    "        missing_fields = [f for f in critical_fields if f not in columns_df['col_name'].values]\n",
    "        \n",
    "        if missing_fields:\n",
    "            displayHTML(f\"\"\"\n",
    "            <div style=\"padding: 15px; background-color: #fff3e0; border-left: 5px solid #ff9800; margin: 10px 0;\">\n",
    "                <h3 style=\"margin-top: 0; color: #e65100;\">‚ö†Ô∏è Warning: Missing Critical Fields</h3>\n",
    "                <p style=\"margin: 5px 0; color: #bf360c;\"><strong>Table:</strong> <code style=\"background-color: #ffe0b2; padding: 2px 6px; border-radius: 3px;\">{full_schema}.cluster_config_backup</code></p>\n",
    "                <p style=\"margin: 5px 0; color: #bf360c;\"><strong>Missing Fields:</strong> {', '.join(missing_fields)}</p>\n",
    "                <p style=\"margin: 5px 0; color: #bf360c;\"><strong>Action:</strong> Table may need manual schema update</p>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            displayHTML(f\"\"\"\n",
    "            <div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "                <h3 style=\"margin-top: 0; color: #2e7d32;\">‚úì Cluster Config Backup Table Ready</h3>\n",
    "                <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Table:</strong> <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{full_schema}.cluster_config_backup</code></p>\n",
    "                <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Columns:</strong> {column_count} fields defined</p>\n",
    "                <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Status:</strong> Production-ready ‚úì</p>\n",
    "                <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Critical Fields Verified:</strong></p>\n",
    "                <ul style=\"color: #1b5e20; margin: 5px 0; padding-left: 20px;\">\n",
    "                    <li><strong>cluster_creator</strong> ‚úì - Cluster owner/creator for ownership tracking</li>\n",
    "                    <li><strong>workspace_id</strong> ‚úì - Workspace ID for cross-workspace operations</li>\n",
    "                    <li><strong>cluster_id</strong> ‚úì - Unique cluster identifier</li>\n",
    "                    <li><strong>cluster_name</strong> ‚úì - Human-readable cluster name</li>\n",
    "                </ul>\n",
    "                <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Features:</strong></p>\n",
    "                <ul style=\"color: #1b5e20; margin: 5px 0; padding-left: 20px;\">\n",
    "                    <li>Complete JSON configuration snapshots (before/after)</li>\n",
    "                    <li>Links to update logs via batch_id and execution_label</li>\n",
    "                    <li>Rollback tracking (is_reverted, revert_timestamp)</li>\n",
    "                    <li>Key fields extracted for easy querying</li>\n",
    "                    <li>Dashboard-ready change analysis fields (13 metrics)</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "    else:\n",
    "        raise Exception(f\"Table creation appeared to succeed but table not found in {full_schema}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #ffebee; border-left: 5px solid #f44336; margin: 10px 0;\">\n",
    "        <h3 style=\"margin-top: 0; color: #c62828;\">‚úó Error Creating Backup Table</h3>\n",
    "        <p style=\"margin: 5px 0; color: #b71c1c;\"><strong>Error:</strong> {str(e)}</p>\n",
    "        <p style=\"margin: 5px 0; color: #b71c1c;\"><strong>Table:</strong> {full_schema}.cluster_config_backup</p>\n",
    "        <p style=\"margin: 5px 0; color: #b71c1c;\"><strong>Action Required:</strong> Check catalog/schema permissions and try again</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578ba59a-c1ea-446c-a810-2360a3e2caa9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execute cluster updates with dry_run flag"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Configuration is loaded from widgets (see cells above)\n",
    "# DRY_RUN, WORKSPACES_TO_UPDATE, and cluster_data are already set\n",
    "\n",
    "mode_text = \"DRY RUN (Preview Only)\" if DRY_RUN else \"LIVE UPDATE\"\n",
    "mode_color = \"#4caf50\" if DRY_RUN else \"#f44336\"\n",
    "\n",
    "# Generate batch metadata\n",
    "batch_start_time = datetime.now()\n",
    "batch_id = str(uuid.uuid4())\n",
    "current_user = spark.sql(\"SELECT current_user() as user\").collect()[0][\"user\"]\n",
    "\n",
    "# Create execution label for easy filtering\n",
    "# Format: YYYY-MM-DD_HH-MM_MODE_WORKSPACES\n",
    "date_str = batch_start_time.strftime('%Y-%m-%d')\n",
    "time_str = batch_start_time.strftime('%H-%M')\n",
    "mode_str = \"DRY-RUN\" if DRY_RUN else \"LIVE\"\n",
    "workspace_str = WORKSPACES_TO_UPDATE.replace(\",\", \"+\") if WORKSPACES_TO_UPDATE else \"ALL\"\n",
    "execution_label = f\"{date_str}_{time_str}_{mode_str}_{workspace_str}\"\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e3f2fd; border-left: 5px solid #2196f3; margin: 10px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #1565c0;\">Execution Mode: <span style=\"color: {mode_color};\">{mode_text}</span></h3>\n",
    "    <p style=\"margin: 5px 0; color: #0d47a1;\"><strong>Execution Label:</strong> <code style=\"background-color: #bbdefb; padding: 2px 6px; border-radius: 3px;\">{execution_label}</code></p>\n",
    "    <p style=\"margin: 5px 0; color: #0d47a1;\"><strong>Batch ID:</strong> {batch_id}</p>\n",
    "    <p style=\"margin: 5px 0; color: #0d47a1;\"><strong>Executed By:</strong> {current_user}</p>\n",
    "    <p style=\"margin: 5px 0; color: #0d47a1;\"><strong>Start Time:</strong> {batch_start_time.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Filter by workspace if specified (cluster_data already loaded in previous cell)\n",
    "filtered_cluster_data = cluster_data\n",
    "if WORKSPACES_TO_UPDATE:\n",
    "    workspace_list = [ws.strip() for ws in WORKSPACES_TO_UPDATE.split(\",\")]\n",
    "    filtered_cluster_data = cluster_data.filter(F.col(\"workspace_name\").isin(workspace_list))\n",
    "\n",
    "# Collect cluster data\n",
    "cluster_rows = filtered_cluster_data.collect()\n",
    "total_clusters = len(cluster_rows)\n",
    "\n",
    "# Create batch metadata dictionary\n",
    "batch_metadata = {\n",
    "    \"batch_id\": batch_id,\n",
    "    \"execution_label\": execution_label,\n",
    "    \"batch_start_time\": batch_start_time,\n",
    "    \"batch_end_time\": None,  # Will be set after processing\n",
    "    \"execution_mode\": \"DRY_RUN\" if DRY_RUN else \"LIVE_UPDATE\",\n",
    "    \"workspace_filter_applied\": WORKSPACES_TO_UPDATE if WORKSPACES_TO_UPDATE else \"ALL\",\n",
    "    \"total_clusters_in_batch\": total_clusters,\n",
    "    \"executed_by_user\": current_user\n",
    "}\n",
    "\n",
    "if not cluster_rows:\n",
    "    displayHTML(\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #ffebee; border-left: 5px solid #f44336; margin: 10px 0;\">\n",
    "        <h3 style=\"margin: 0; color: #c62828;\">‚ö† No clusters found matching the filter criteria</h3>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "else:\n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #fff3e0; border-left: 5px solid #ff9800; margin: 10px 0;\">\n",
    "        <h3 style=\"margin: 0; color: #e65100;\">üîÑ Processing {len(cluster_rows)} clusters...</h3>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Process each cluster\n",
    "    log_entries = []\n",
    "    backup_entries = []\n",
    "    for idx, row in enumerate(cluster_rows, 1):\n",
    "        display(f\"Processing {idx}/{len(cluster_rows)}: {row.cluster_name} ({row.cluster_id}) in {row.workspace_name}\")\n",
    "        log_entry, backup_entry = update_cluster_with_recommendation(row, dry_run=DRY_RUN, batch_metadata=batch_metadata)\n",
    "        log_entries.append(log_entry)\n",
    "        if backup_entry:  # Only add if backup was created\n",
    "            backup_entries.append(backup_entry)\n",
    "        \n",
    "        # Brief pause to avoid rate limiting\n",
    "        if idx % 10 == 0:\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Update batch end time\n",
    "    batch_end_time = datetime.now()\n",
    "    batch_metadata[\"batch_end_time\"] = batch_end_time\n",
    "    \n",
    "    # Update all log entries with batch end time\n",
    "    for entry in log_entries:\n",
    "        entry[\"batch_end_time\"] = batch_end_time\n",
    "    \n",
    "    duration_seconds = (batch_end_time - batch_start_time).total_seconds()\n",
    "    \n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "        <h3 style=\"margin: 0; color: #2e7d32;\">‚úì Processed {len(log_entries)} clusters</h3>\n",
    "        <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Batch Duration:</strong> {duration_seconds:.2f} seconds</p>\n",
    "        <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Execution Label:</strong> <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{execution_label}</code></p>\n",
    "        <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Batch ID:</strong> {batch_id}</p>\n",
    "        <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Config Backups Created:</strong> {len(backup_entries)}</p>\n",
    "    </div>\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f978fd-183f-400f-9f86-4fed0739f20c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save results to logging table"
    }
   },
   "outputs": [],
   "source": [
    "# Convert log entries to DataFrame\n",
    "log_schema = StructType([\n",
    "    # Batch metadata\n",
    "    StructField(\"batch_id\", StringType(), True),\n",
    "    StructField(\"execution_label\", StringType(), True),\n",
    "    StructField(\"batch_start_time\", TimestampType(), True),\n",
    "    StructField(\"batch_end_time\", TimestampType(), True),\n",
    "    StructField(\"execution_mode\", StringType(), True),\n",
    "    StructField(\"workspace_filter_applied\", StringType(), True),\n",
    "    StructField(\"total_clusters_in_batch\", LongType(), True),\n",
    "    StructField(\"executed_by_user\", StringType(), True),\n",
    "    \n",
    "    # Individual cluster details\n",
    "    StructField(\"log_id\", StringType(), True),\n",
    "    StructField(\"cluster_id\", StringType(), True),\n",
    "    StructField(\"cluster_name\", StringType(), True),\n",
    "    StructField(\"workspace_name\", StringType(), True),\n",
    "    StructField(\"workspace_id\", LongType(), True),\n",
    "    StructField(\"deployment_url\", StringType(), True),\n",
    "    StructField(\"action_type\", StringType(), True),\n",
    "    StructField(\"recommendation\", StringType(), True),\n",
    "    StructField(\"current_driver_instance\", StringType(), True),\n",
    "    StructField(\"current_worker_instance\", StringType(), True),\n",
    "    StructField(\"suggested_driver_instance\", StringType(), True),\n",
    "    StructField(\"suggested_worker_instance\", StringType(), True),\n",
    "    StructField(\"current_min_workers\", LongType(), True),\n",
    "    StructField(\"current_max_workers\", LongType(), True),\n",
    "    StructField(\"validation_status\", StringType(), True),\n",
    "    StructField(\"validation_message\", StringType(), True),\n",
    "    StructField(\"update_status\", StringType(), True),\n",
    "    StructField(\"update_message\", StringType(), True),\n",
    "    StructField(\"dry_run\", BooleanType(), True),\n",
    "    StructField(\"validated_savings\", DecimalType(35, 2), True),\n",
    "    StructField(\"execution_timestamp\", TimestampType(), True),\n",
    "    StructField(\"error_details\", StringType(), True),\n",
    "    StructField(\"implementation_notes\", StringType(), True)\n",
    "])\n",
    "\n",
    "log_df = spark.createDataFrame(log_entries, schema=log_schema)\n",
    "\n",
    "# Write to logging table using full_schema variable\n",
    "log_df.write.mode(\"append\").saveAsTable(f\"{full_schema}.cluster_update_log\")\n",
    "\n",
    "log_count = log_df.count()\n",
    "\n",
    "# Save backup entries if any exist\n",
    "backup_count = 0\n",
    "if backup_entries:\n",
    "    backup_schema = StructType([\n",
    "        # Basic identification\n",
    "        StructField(\"backup_id\", StringType(), True),\n",
    "        StructField(\"batch_id\", StringType(), True),\n",
    "        StructField(\"execution_label\", StringType(), True),\n",
    "        StructField(\"backup_timestamp\", TimestampType(), True),\n",
    "        StructField(\"cluster_id\", StringType(), True),\n",
    "        StructField(\"cluster_name\", StringType(), True),\n",
    "        StructField(\"cluster_creator\", StringType(), True),  # Added missing field\n",
    "        StructField(\"workspace_name\", StringType(), True),\n",
    "        StructField(\"workspace_id\", LongType(), True),\n",
    "        StructField(\"deployment_url\", StringType(), True),\n",
    "        StructField(\"update_status\", StringType(), True),\n",
    "        StructField(\"update_reason\", StringType(), True),\n",
    "        StructField(\"updated_by_user\", StringType(), True),\n",
    "        \n",
    "        # Before configuration\n",
    "        StructField(\"before_config\", StringType(), True),\n",
    "        StructField(\"before_driver_instance\", StringType(), True),\n",
    "        StructField(\"before_worker_instance\", StringType(), True),\n",
    "        StructField(\"before_policy_id\", StringType(), True),\n",
    "        StructField(\"before_autoscale_min\", IntegerType(), True),\n",
    "        StructField(\"before_autoscale_max\", IntegerType(), True),\n",
    "        StructField(\"before_num_workers\", IntegerType(), True),\n",
    "        \n",
    "        # After configuration\n",
    "        StructField(\"after_config\", StringType(), True),\n",
    "        StructField(\"after_driver_instance\", StringType(), True),\n",
    "        StructField(\"after_worker_instance\", StringType(), True),\n",
    "        StructField(\"after_policy_id\", StringType(), True),\n",
    "        StructField(\"after_autoscale_min\", IntegerType(), True),\n",
    "        StructField(\"after_autoscale_max\", IntegerType(), True),\n",
    "        StructField(\"after_num_workers\", IntegerType(), True),\n",
    "        \n",
    "        # Revert tracking\n",
    "        StructField(\"is_reverted\", BooleanType(), True),\n",
    "        StructField(\"revert_timestamp\", TimestampType(), True),\n",
    "        StructField(\"revert_batch_id\", StringType(), True),\n",
    "        StructField(\"reverted_by_user\", StringType(), True),\n",
    "        \n",
    "        # Dashboard-friendly fields - Change summary\n",
    "        StructField(\"change_categories\", StringType(), True),\n",
    "        StructField(\"total_changes_count\", IntegerType(), True),\n",
    "        StructField(\"change_impact\", StringType(), True),\n",
    "        \n",
    "        # Dashboard-friendly fields - Extracted settings counts\n",
    "        StructField(\"before_spark_config_count\", IntegerType(), True),\n",
    "        StructField(\"after_spark_config_count\", IntegerType(), True),\n",
    "        StructField(\"before_custom_tags_count\", IntegerType(), True),\n",
    "        StructField(\"after_custom_tags_count\", IntegerType(), True),\n",
    "        StructField(\"before_init_scripts_count\", IntegerType(), True),\n",
    "        StructField(\"after_init_scripts_count\", IntegerType(), True),\n",
    "        StructField(\"before_autotermination_minutes\", IntegerType(), True),\n",
    "        StructField(\"after_autotermination_minutes\", IntegerType(), True),\n",
    "        StructField(\"before_runtime_engine\", StringType(), True),\n",
    "        StructField(\"after_runtime_engine\", StringType(), True),\n",
    "        StructField(\"before_data_security_mode\", StringType(), True),\n",
    "        StructField(\"after_data_security_mode\", StringType(), True),\n",
    "        \n",
    "        # Dashboard-friendly fields - Change flags\n",
    "        StructField(\"instance_type_changed\", BooleanType(), True),\n",
    "        StructField(\"policy_changed\", BooleanType(), True),\n",
    "        StructField(\"spark_config_changed\", BooleanType(), True),\n",
    "        StructField(\"tags_changed\", BooleanType(), True),\n",
    "        StructField(\"init_scripts_changed\", BooleanType(), True),\n",
    "        StructField(\"autotermination_changed\", BooleanType(), True),\n",
    "        StructField(\"runtime_engine_changed\", BooleanType(), True),\n",
    "        StructField(\"security_mode_changed\", BooleanType(), True),\n",
    "        StructField(\"autoscale_changed\", BooleanType(), True),\n",
    "        \n",
    "        # Dashboard-friendly fields - User-friendly summaries\n",
    "        StructField(\"change_summary\", StringType(), True),\n",
    "        StructField(\"change_details\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    backup_df = spark.createDataFrame(backup_entries, schema=backup_schema)\n",
    "    backup_df.write.mode(\"append\").saveAsTable(f\"{full_schema}.cluster_config_backup\")\n",
    "    backup_count = backup_df.count()\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "    <h3 style=\"margin: 0; color: #2e7d32;\">‚úì Saved results to logging tables</h3>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Update Logs:</strong> {log_count} entries saved to <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{full_schema}.cluster_update_log</code></p>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Config Backups:</strong> {backup_count} entries saved to <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{full_schema}.cluster_config_backup</code></p>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Dashboard Fields:</strong> Change summaries, impact levels, and user-friendly descriptions included</p>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Execution Label:</strong> <code style=\"background-color: #c8e6c9; padding: 2px 6px; border-radius: 3px;\">{execution_label}</code></p>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Batch ID:</strong> {batch_id}</p>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdec46b-e36a-43c3-ae78-63291c9203f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sample Dashboard Queries for cluster_config_backup"
    }
   },
   "outputs": [],
   "source": [
    "# Sample queries for building dashboards with cluster_config_backup table\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e3f2fd; border-left: 5px solid #2196f3; margin: 10px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #1565c0;\">üìä Sample Dashboard Queries</h3>\n",
    "    <p style=\"margin: 5px 0; color: #0d47a1;\">Use these queries to build user-friendly dashboards showing cluster changes</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 1: User-Friendly Change Summary\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  execution_label,\n",
    "  cluster_name,\n",
    "  workspace_name,\n",
    "  change_summary,\n",
    "  change_impact,\n",
    "  total_changes_count,\n",
    "  backup_timestamp,\n",
    "  updated_by_user\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE update_status = 'SUCCESS'\n",
    "  AND is_reverted = false\n",
    "ORDER BY backup_timestamp DESC\n",
    "LIMIT 50\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 2: Changes by Category (for filtering)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  cluster_name,\n",
    "  workspace_name,\n",
    "  CASE \n",
    "    WHEN instance_type_changed THEN 'Instance Type Changed'\n",
    "    WHEN policy_changed THEN 'Policy Changed'\n",
    "    WHEN spark_config_changed THEN 'Spark Config Changed'\n",
    "    WHEN tags_changed THEN 'Tags Changed'\n",
    "    ELSE 'Other Changes'\n",
    "  END as change_type,\n",
    "  before_driver_instance,\n",
    "  after_driver_instance,\n",
    "  before_worker_instance,\n",
    "  after_worker_instance,\n",
    "  change_details,\n",
    "  backup_timestamp\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE update_status = 'SUCCESS'\n",
    "ORDER BY backup_timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 3: Change Impact Analysis\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  change_impact,\n",
    "  COUNT(*) as cluster_count,\n",
    "  COUNT(DISTINCT workspace_name) as workspace_count,\n",
    "  AVG(total_changes_count) as avg_changes_per_cluster\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE update_status = 'SUCCESS'\n",
    "  AND is_reverted = false\n",
    "GROUP BY change_impact\n",
    "ORDER BY \n",
    "  CASE change_impact \n",
    "    WHEN 'MAJOR' THEN 1 \n",
    "    WHEN 'MODERATE' THEN 2 \n",
    "    WHEN 'MINOR' THEN 3 \n",
    "    ELSE 4 \n",
    "  END\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 4: Before/After Comparison for Specific Cluster\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  cluster_name,\n",
    "  workspace_name,\n",
    "  -- Instance Types\n",
    "  before_driver_instance,\n",
    "  after_driver_instance,\n",
    "  before_worker_instance,\n",
    "  after_worker_instance,\n",
    "  -- Policy\n",
    "  before_policy_id,\n",
    "  after_policy_id,\n",
    "  -- Configurations\n",
    "  before_spark_config_count,\n",
    "  after_spark_config_count,\n",
    "  before_custom_tags_count,\n",
    "  after_custom_tags_count,\n",
    "  before_init_scripts_count,\n",
    "  after_init_scripts_count,\n",
    "  -- Runtime\n",
    "  before_runtime_engine,\n",
    "  after_runtime_engine,\n",
    "  before_autotermination_minutes,\n",
    "  after_autotermination_minutes,\n",
    "  -- Summary\n",
    "  change_summary,\n",
    "  backup_timestamp\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE cluster_name = '<YOUR_CLUSTER_NAME>'\n",
    "ORDER BY backup_timestamp DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 5: Changes by Workspace (for workspace owners)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  workspace_name,\n",
    "  COUNT(*) as total_changes,\n",
    "  SUM(CASE WHEN instance_type_changed THEN 1 ELSE 0 END) as instance_changes,\n",
    "  SUM(CASE WHEN policy_changed THEN 1 ELSE 0 END) as policy_changes,\n",
    "  SUM(CASE WHEN spark_config_changed THEN 1 ELSE 0 END) as config_changes,\n",
    "  SUM(CASE WHEN change_impact = 'MAJOR' THEN 1 ELSE 0 END) as major_changes,\n",
    "  MAX(backup_timestamp) as last_change_date\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE update_status = 'SUCCESS'\n",
    "  AND is_reverted = false\n",
    "GROUP BY workspace_name\n",
    "ORDER BY total_changes DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 6: Recent Changes Timeline\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  DATE(backup_timestamp) as change_date,\n",
    "  COUNT(*) as changes_count,\n",
    "  COUNT(DISTINCT cluster_id) as clusters_affected,\n",
    "  COUNT(DISTINCT workspace_name) as workspaces_affected,\n",
    "  SUM(CASE WHEN change_impact = 'MAJOR' THEN 1 ELSE 0 END) as major_changes,\n",
    "  SUM(CASE WHEN change_impact = 'MODERATE' THEN 1 ELSE 0 END) as moderate_changes,\n",
    "  SUM(CASE WHEN change_impact = 'MINOR' THEN 1 ELSE 0 END) as minor_changes\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE update_status = 'SUCCESS'\n",
    "  AND backup_timestamp >= CURRENT_DATE - INTERVAL 30 DAYS\n",
    "GROUP BY DATE(backup_timestamp)\n",
    "ORDER BY change_date DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUERY 7: Clusters Ready for Revert (if needed)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "SELECT \n",
    "  backup_id,\n",
    "  cluster_name,\n",
    "  workspace_name,\n",
    "  change_summary,\n",
    "  change_impact,\n",
    "  backup_timestamp,\n",
    "  DATEDIFF(CURRENT_DATE, DATE(backup_timestamp)) as days_since_change\n",
    "FROM {full_schema}.cluster_config_backup\n",
    "WHERE update_status = 'SUCCESS'\n",
    "  AND is_reverted = false\n",
    "  AND change_impact IN ('MAJOR', 'MODERATE')\n",
    "ORDER BY backup_timestamp DESC\n",
    "\"\"\")\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #2e7d32;\">‚úì Dashboard Query Examples Ready</h3>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\">Copy these queries to your dashboard or BI tool</p>\n",
    "    <p style=\"margin: 5px 0; color: #1b5e20;\"><strong>Key Features:</strong></p>\n",
    "    <ul style=\"color: #1b5e20; margin: 5px 0;\">\n",
    "        <li>User-friendly change summaries</li>\n",
    "        <li>Filterable by change type, impact, workspace</li>\n",
    "        <li>Before/after comparisons</li>\n",
    "        <li>Timeline analysis</li>\n",
    "        <li>Revert candidates identification</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659b2784-3d6a-47b5-acd7-ddaaaa3d0da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üìã Cluster Config Backup Table - Complete Guide\n",
    "\n",
    "## ‚úÖ ONE Table Serves TWO Purposes\n",
    "\n",
    "The `cluster_config_backup` table is now enhanced to serve both:\n",
    "1. **Reverting clusters** to previous configurations\n",
    "2. **Building dashboards** to show users what changed\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Purpose 1: Revert Capability\n",
    "\n",
    "### Complete JSON Configs\n",
    "* `before_config` - Full cluster configuration before update (JSON)\n",
    "* `after_config` - Full cluster configuration after update (JSON)\n",
    "* Contains ALL settings: policy, Spark configs, tags, init scripts, security, etc.\n",
    "\n",
    "### Revert Tracking\n",
    "* `is_reverted` - Has this cluster been reverted?\n",
    "* `revert_timestamp` - When was it reverted?\n",
    "* `revert_batch_id` - Which batch reverted it?\n",
    "* `reverted_by_user` - Who performed the revert?\n",
    "\n",
    "### Usage in Revert Notebook\n",
    "```python\n",
    "# Query backup to revert\n",
    "backup = spark.sql(f\"\"\"\n",
    "  SELECT backup_id, before_config, cluster_id\n",
    "  FROM {catalog}.{schema}.cluster_config_backup\n",
    "  WHERE cluster_name = 'my-cluster'\n",
    "    AND update_status = 'SUCCESS'\n",
    "    AND is_reverted = false\n",
    "  ORDER BY backup_timestamp DESC\n",
    "  LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "# Parse JSON and revert\n",
    "import json\n",
    "config = json.loads(backup.before_config)\n",
    "# Use Databricks SDK to apply config...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Purpose 2: Dashboard Building\n",
    "\n",
    "### User-Friendly Fields\n",
    "\n",
    "**Change Summary:**\n",
    "* `change_summary` - Human-readable description (e.g., \"Instance Types: Driver m5.xlarge ‚Üí m5.large, Worker m5.xlarge ‚Üí m5.large\")\n",
    "* `change_details` - Detailed breakdown of all changes\n",
    "* `change_categories` - Comma-separated list (e.g., \"instance_type,policy,spark_config\")\n",
    "* `total_changes_count` - Number of settings changed\n",
    "* `change_impact` - MINOR, MODERATE, or MAJOR\n",
    "\n",
    "**Boolean Flags (for filtering):**\n",
    "* `instance_type_changed`\n",
    "* `policy_changed`\n",
    "* `spark_config_changed`\n",
    "* `tags_changed`\n",
    "* `init_scripts_changed`\n",
    "* `autotermination_changed`\n",
    "* `runtime_engine_changed`\n",
    "* `security_mode_changed`\n",
    "* `autoscale_changed`\n",
    "\n",
    "**Extracted Counts (before/after):**\n",
    "* `before_spark_config_count` / `after_spark_config_count`\n",
    "* `before_custom_tags_count` / `after_custom_tags_count`\n",
    "* `before_init_scripts_count` / `after_init_scripts_count`\n",
    "* `before_autotermination_minutes` / `after_autotermination_minutes`\n",
    "* `before_runtime_engine` / `after_runtime_engine`\n",
    "* `before_data_security_mode` / `after_data_security_mode`\n",
    "\n",
    "**Key Instance Fields:**\n",
    "* `before_driver_instance` / `after_driver_instance`\n",
    "* `before_worker_instance` / `after_worker_instance`\n",
    "* `before_policy_id` / `after_policy_id`\n",
    "* `before_autoscale_min/max` / `after_autoscale_min/max`\n",
    "\n",
    "### Dashboard Use Cases\n",
    "\n",
    "1. **Change Timeline** - Show when clusters were updated\n",
    "2. **Impact Analysis** - Filter by MAJOR/MODERATE/MINOR changes\n",
    "3. **Workspace View** - Show changes per workspace for owners\n",
    "4. **Change Type Filter** - Filter by instance type, policy, config changes\n",
    "5. **Before/After Comparison** - Side-by-side view of settings\n",
    "6. **Audit Trail** - Who changed what and when\n",
    "7. **Revert Candidates** - Identify clusters that might need reverting\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Benefits\n",
    "\n",
    "### Single Source of Truth\n",
    "‚úÖ No data duplication  \n",
    "‚úÖ Automatic sync (one write operation)  \n",
    "‚úÖ Easier maintenance  \n",
    "‚úÖ Lower storage costs  \n",
    "\n",
    "### Complete Information\n",
    "‚úÖ Full JSON for technical revert operations  \n",
    "‚úÖ User-friendly fields for business dashboards  \n",
    "‚úÖ Change tracking and audit trail  \n",
    "‚úÖ Revert prevention (is_reverted flag)  \n",
    "\n",
    "### Flexible Querying\n",
    "‚úÖ Filter by change type, impact, workspace  \n",
    "‚úÖ Aggregate by date, user, workspace  \n",
    "‚úÖ Join with other tables for enrichment  \n",
    "‚úÖ Time-series analysis of changes  \n",
    "\n",
    "---\n",
    "\n",
    "## üìù Next Steps\n",
    "\n",
    "1. **Run cluster updates** (dry-run or live) to populate the table\n",
    "2. **Build dashboards** using the sample queries provided\n",
    "3. **Create revert notebook** for rollback capability\n",
    "4. **Set up alerts** for MAJOR impact changes\n",
    "5. **Share with workspace owners** for transparency\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Related Tables\n",
    "\n",
    "* `cluster_update_log` - Execution logs and status\n",
    "* `cluster_opportunities` - Optimization recommendations\n",
    "* `cluster_config_backup` - **This table** (backup + dashboard)\n",
    "\n",
    "---\n",
    "\n",
    "**Table Location:** `{catalog}.{schema}.cluster_config_backup`  \n",
    "**Updated:** Automatically on every cluster update  \n",
    "**Retention:** Permanent (until manually deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ceeafb2-7855-4893-8ef0-fa97052ac9b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display summary statistics"
    }
   },
   "outputs": [],
   "source": [
    "# Verify row counts\n",
    "opportunities_count = spark.table(f\"{full_schema}.cluster_opportunities\").count()\n",
    "log_count = log_df.count()\n",
    "\n",
    "row_match = opportunities_count == log_count\n",
    "match_color = \"#4caf50\" if row_match else \"#f44336\"\n",
    "match_icon = \"‚úì\" if row_match else \"‚úó\"\n",
    "\n",
    "mode_display = \"DRY RUN\" if DRY_RUN else \"LIVE UPDATE\"\n",
    "mode_color = \"#4caf50\" if DRY_RUN else \"#f44336\"\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<div style=\"border: 2px solid #2196f3; padding: 20px; margin: 20px 0; background-color: #e3f2fd; border-radius: 8px;\">\n",
    "    <h2 style=\"text-align: center; color: #1565c0; margin-top: 0;\">\n",
    "        üìä EXECUTION SUMMARY\n",
    "    </h2>\n",
    "    <hr style=\"border: 1px solid #2196f3; margin: 15px 0;\">\n",
    "    \n",
    "    <div style=\"font-size: 16px; line-height: 2; padding: 10px;\">\n",
    "        <div style=\"margin: 10px 0;\">\n",
    "            <strong>Mode:</strong> \n",
    "            <span style=\"color: {mode_color}; font-weight: bold; font-size: 18px;\">{mode_display}</span>\n",
    "        </div>\n",
    "        <div style=\"margin: 10px 0;\">\n",
    "            <strong>Opportunities table rows:</strong> \n",
    "            <span style=\"font-size: 18px; color: #1976d2;\">{opportunities_count}</span>\n",
    "        </div>\n",
    "        <div style=\"margin: 10px 0;\">\n",
    "            <strong>Log table rows (this run):</strong> \n",
    "            <span style=\"font-size: 18px; color: #1976d2;\">{log_count}</span>\n",
    "        </div>\n",
    "        <div style=\"margin: 10px 0;\">\n",
    "            <strong>Row count match:</strong> \n",
    "            <span style=\"color: {match_color}; font-weight: bold; font-size: 18px;\">{match_icon} {'YES' if row_match else 'NO'}</span>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Summary by status\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"padding: 10px; background-color: #fff3e0; border-left: 4px solid #ff9800; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #e65100;\">Status Breakdown:</h3>\n",
    "</div>\n",
    "\"\"\")\n",
    "display(log_df.groupBy(\"validation_status\", \"update_status\").count().orderBy(\"count\", ascending=False))\n",
    "\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"padding: 10px; background-color: #e8f5e9; border-left: 4px solid #4caf50; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #2e7d32;\">Potential Savings Summary:</h3>\n",
    "</div>\n",
    "\"\"\")\n",
    "display(log_df.groupBy(\"update_status\").agg(\n",
    "    F.count(\"*\").alias(\"cluster_count\"),\n",
    "    F.sum(\"validated_savings\").alias(\"total_savings_usd\")\n",
    ").orderBy(\"total_savings_usd\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618a22b5-a9e8-4396-9478-b8056366a593",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View detailed log results"
    }
   },
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"padding: 10px; background-color: #e3f2fd; border-left: 4px solid #2196f3; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #1565c0;\">Detailed Log Entries:</h3>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Check if implementation_notes column exists in log_df\n",
    "if 'implementation_notes' in log_df.columns:\n",
    "    display(log_df.select(\n",
    "        \"cluster_name\",\n",
    "        \"workspace_name\",\n",
    "        \"validation_status\",\n",
    "        \"update_status\",\n",
    "        \"current_driver_instance\",\n",
    "        \"suggested_driver_instance\",\n",
    "        \"current_worker_instance\",\n",
    "        \"suggested_worker_instance\",\n",
    "        \"implementation_notes\",\n",
    "        \"validated_savings\",\n",
    "        \"update_message\"\n",
    "    ).orderBy(F.col(\"validated_savings\").desc()))\n",
    "else:\n",
    "    # Fallback if column doesn't exist (old schema)\n",
    "    display(log_df.select(\n",
    "        \"cluster_name\",\n",
    "        \"workspace_name\",\n",
    "        \"validation_status\",\n",
    "        \"update_status\",\n",
    "        \"current_driver_instance\",\n",
    "        \"suggested_driver_instance\",\n",
    "        \"current_worker_instance\",\n",
    "        \"suggested_worker_instance\",\n",
    "        \"validated_savings\",\n",
    "        \"update_message\"\n",
    "    ).orderBy(F.col(\"validated_savings\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7722f808-2d98-415b-ad2c-f2b4869b612c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Batch Execution History"
    }
   },
   "outputs": [],
   "source": [
    "# Query batch execution history\n",
    "displayHTML(\"\"\"\n",
    "<div style=\"padding: 15px; background-color: #e3f2fd; border-left: 5px solid #2196f3; margin: 15px 0;\">\n",
    "    <h3 style=\"margin-top: 0; color: #1565c0;\">üìã Batch Execution History</h3>\n",
    "    <p style=\"margin: 5px 0; color: #0d47a1;\">Use execution_label for easy filtering in dashboards</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Get batch summary with execution_label\n",
    "batch_summary = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        execution_label,\n",
    "        batch_id,\n",
    "        execution_mode,\n",
    "        workspace_filter_applied,\n",
    "        executed_by_user,\n",
    "        batch_start_time,\n",
    "        batch_end_time,\n",
    "        ROUND((UNIX_TIMESTAMP(batch_end_time) - UNIX_TIMESTAMP(batch_start_time)), 2) as duration_seconds,\n",
    "        total_clusters_in_batch,\n",
    "        COUNT(*) as clusters_processed,\n",
    "        SUM(CASE WHEN update_status = 'SUCCESS' THEN 1 ELSE 0 END) as successful_updates,\n",
    "        SUM(CASE WHEN update_status = 'DRY_RUN' THEN 1 ELSE 0 END) as dry_run_previews,\n",
    "        SUM(CASE WHEN update_status = 'SKIPPED' THEN 1 ELSE 0 END) as skipped_clusters,\n",
    "        SUM(CASE WHEN update_status = 'FAILED' THEN 1 ELSE 0 END) as failed_updates,\n",
    "        SUM(validated_savings) as total_potential_savings\n",
    "    FROM {full_schema}.cluster_update_log\n",
    "    GROUP BY \n",
    "        execution_label,\n",
    "        batch_id,\n",
    "        execution_mode,\n",
    "        workspace_filter_applied,\n",
    "        executed_by_user,\n",
    "        batch_start_time,\n",
    "        batch_end_time,\n",
    "        total_clusters_in_batch\n",
    "    ORDER BY batch_start_time DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(batch_summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HOW TO USE EXECUTION LABEL FILTERING:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n1. Copy an execution_label from the table above (e.g., '2024-11-20_15-30_DRY-RUN_prod')\")\n",
    "print(f\"\\n2. Query specific execution details:\")\n",
    "print(f\"   SELECT * FROM {full_schema}.cluster_update_log\")\n",
    "print(f\"   WHERE execution_label = '<your-execution-label>'\")\n",
    "print(f\"\\n3. Filter by pattern (useful in dashboards):\")\n",
    "print(f\"   WHERE execution_label LIKE '2024-11-20%'  -- All runs on Nov 20\")\n",
    "print(f\"   WHERE execution_label LIKE '%DRY-RUN%'    -- All dry runs\")\n",
    "print(f\"   WHERE execution_label LIKE '%prod%'       -- All prod workspace runs\")\n",
    "print(f\"\\n4. Use in dashboard filters:\")\n",
    "print(f\"   - Add execution_label as a dropdown filter\")\n",
    "print(f\"   - Users can easily select specific runs\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "086d61cc-ceae-4262-8764-ee97420dd5eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Interactive Filter: Query by Execution Label"
    }
   },
   "outputs": [],
   "source": [
    "# Get list of execution labels from the log table\n",
    "execution_labels_df = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT execution_label\n",
    "    FROM {full_schema}.cluster_update_log\n",
    "    WHERE execution_label IS NOT NULL\n",
    "    ORDER BY execution_label DESC\n",
    "\"\"\")\n",
    "\n",
    "execution_labels = (\n",
    "    execution_labels_df\n",
    "    .toPandas()['execution_label']\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "if execution_labels:\n",
    "    # Create widget for execution label selection\n",
    "    dbutils.widgets.dropdown(\n",
    "        \"selected_execution_label\", \n",
    "        execution_labels[0],\n",
    "        execution_labels,\n",
    "        \"Select Execution Run\"\n",
    "    )\n",
    "    \n",
    "    displayHTML(\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #fff3e0; border-left: 5px solid #ff9800; margin: 10px 0;\">\n",
    "        <h3 style=\"margin-top: 0; color: #e65100;\">üîç Interactive Filter</h3>\n",
    "        <p style=\"margin: 5px 0; color: #bf360c;\">Select an execution run from the dropdown above to view its details</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "else:\n",
    "    displayHTML(\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #ffebee; border-left: 5px solid #f44336; margin: 10px 0;\">\n",
    "        <h3 style=\"margin: 0; color: #c62828;\">‚ö† No execution logs found</h3>\n",
    "        <p style=\"margin: 5px 0; color: #b71c1c;\">Run the cluster update process first to generate logs</p>\n",
    "    </div>\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfcdb49-33e8-4c19-a506-cc385b36b503",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Selected Execution Details"
    }
   },
   "outputs": [],
   "source": [
    "# Get catalog and schema from parameters\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "full_schema = f\"{catalog}.{schema}\"\n",
    "\n",
    "# Get selected execution label\n",
    "try:\n",
    "    selected_label = dbutils.widgets.get(\"selected_execution_label\")\n",
    "    \n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #e3f2fd; border-left: 5px solid #2196f3; margin: 15px 0;\">\n",
    "        <h3 style=\"margin-top: 0; color: #1565c0;\">üìä Viewing Execution: <code style=\"background-color: #bbdefb; padding: 2px 6px; border-radius: 3px;\">{selected_label}</code></h3>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Query logs for selected execution\n",
    "    selected_logs = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            execution_label,\n",
    "            execution_mode,\n",
    "            cluster_name,\n",
    "            workspace_name,\n",
    "            validation_status,\n",
    "            update_status,\n",
    "            current_driver_instance,\n",
    "            suggested_driver_instance,\n",
    "            current_worker_instance,\n",
    "            suggested_worker_instance,\n",
    "            validated_savings,\n",
    "            update_message,\n",
    "            execution_timestamp\n",
    "        FROM {full_schema}.cluster_update_log\n",
    "        WHERE execution_label = '{selected_label}'\n",
    "        ORDER BY execution_timestamp\n",
    "    \"\"\")\n",
    "    \n",
    "    # Display summary\n",
    "    summary = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_clusters,\n",
    "            SUM(CASE WHEN update_status = 'SUCCESS' THEN 1 ELSE 0 END) as successful,\n",
    "            SUM(CASE WHEN update_status = 'DRY_RUN' THEN 1 ELSE 0 END) as dry_run,\n",
    "            SUM(CASE WHEN update_status = 'SKIPPED' THEN 1 ELSE 0 END) as skipped,\n",
    "            SUM(CASE WHEN update_status = 'FAILED' THEN 1 ELSE 0 END) as failed,\n",
    "            SUM(validated_savings) as total_savings\n",
    "        FROM {full_schema}.cluster_update_log\n",
    "        WHERE execution_label = '{selected_label}'\n",
    "    \"\"\").collect()[0]\n",
    "    \n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"border: 2px solid #2196f3; padding: 15px; margin: 15px 0; background-color: #e3f2fd; border-radius: 8px;\">\n",
    "        <h4 style=\"margin-top: 0; color: #1565c0;\">Summary Statistics</h4>\n",
    "        <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px;\">\n",
    "            <div style=\"padding: 10px; background-color: white; border-radius: 5px;\">\n",
    "                <strong>Total Clusters:</strong> {summary.total_clusters}\n",
    "            </div>\n",
    "            <div style=\"padding: 10px; background-color: #e8f5e9; border-radius: 5px;\">\n",
    "                <strong>Successful:</strong> {summary.successful}\n",
    "            </div>\n",
    "            <div style=\"padding: 10px; background-color: #e3f2fd; border-radius: 5px;\">\n",
    "                <strong>Dry Run:</strong> {summary.dry_run}\n",
    "            </div>\n",
    "            <div style=\"padding: 10px; background-color: #fff3e0; border-radius: 5px;\">\n",
    "                <strong>Skipped:</strong> {summary.skipped}\n",
    "            </div>\n",
    "            <div style=\"padding: 10px; background-color: #ffebee; border-radius: 5px;\">\n",
    "                <strong>Failed:</strong> {summary.failed}\n",
    "            </div>\n",
    "            <div style=\"padding: 10px; background-color: #e8f5e9; border-radius: 5px;\">\n",
    "                <strong>Total Savings:</strong> ${summary.total_savings:,.2f}\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Display detailed logs\n",
    "    displayHTML(\"\"\"\n",
    "    <div style=\"padding: 10px; background-color: #e8f5e9; border-left: 4px solid #4caf50; margin: 15px 0;\">\n",
    "        <h4 style=\"margin-top: 0; color: #2e7d32;\">Detailed Cluster Updates:</h4>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    display(selected_logs)\n",
    "    \n",
    "except Exception as e:\n",
    "    displayHTML(f\"\"\"\n",
    "    <div style=\"padding: 15px; background-color: #ffebee; border-left: 5px solid #f44336; margin: 15px 0;\">\n",
    "        <h3 style=\"margin: 0; color: #c62828;\">‚ö† Error</h3>\n",
    "        <p style=\"margin: 5px 0; color: #b71c1c;\">{str(e)}</p>\n",
    "        <p style=\"margin: 5px 0; color: #b71c1c;\">Make sure to select an execution label from the dropdown above</p>\n",
    "    </div>\n",
    "    \"\"\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "no-auto-run"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Cluster Update Automation",
   "widgets": {
    "catalog": {
     "currentValue": "dev_sandbox",
     "nuid": "0035eb1e-2569-4858-9eb2-2d8fbc5c6ce0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ex_dash_temp",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "ex_dash_temp",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "dry_run": {
     "currentValue": "true",
     "nuid": "32c40fc9-d026-4683-8f8e-2a217ad24862",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "true",
      "label": "Dry Run Mode",
      "name": "dry_run",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "true",
      "label": "Dry Run Mode",
      "name": "dry_run",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    },
    "schema": {
     "currentValue": "billing_forecast",
     "nuid": "c4af4dac-c759-4963-a76d-f491f05928bc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "billing_forecast",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "billing_forecast",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "selected_execution_label": {
     "currentValue": "2025-12-12_04-21_LIVE_Integrated-Dev",
     "nuid": "9c3d5963-926e-45fa-b986-cb8e39f7223e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2025-12-12_21-00_DRY-RUN_Integrated-Dev",
      "label": "Select Execution Run",
      "name": "selected_execution_label",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "2025-12-12_21-00_DRY-RUN_Integrated-Dev",
        "2025-12-12_04-21_LIVE_Integrated-Dev",
        "2025-12-12_04-15_DRY-RUN_Integrated-Dev",
        "2025-12-10_23-28_LIVE_Integrated-Dev",
        "2025-12-10_20-11_LIVE_Integrated-Dev",
        "2025-12-10_19-49_DRY-RUN_Integrated-Dev",
        "2025-12-09_17-39_LIVE_Integrated-Dev",
        "2025-12-09_17-30_LIVE_Integrated-Dev",
        "2025-12-09_17-26_DRY-RUN_Integrated-Dev"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "2025-12-12_21-00_DRY-RUN_Integrated-Dev",
      "label": "Select Execution Run",
      "name": "selected_execution_label",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "2025-12-12_21-00_DRY-RUN_Integrated-Dev",
        "2025-12-12_04-21_LIVE_Integrated-Dev",
        "2025-12-12_04-15_DRY-RUN_Integrated-Dev",
        "2025-12-10_23-28_LIVE_Integrated-Dev",
        "2025-12-10_20-11_LIVE_Integrated-Dev",
        "2025-12-10_19-49_DRY-RUN_Integrated-Dev",
        "2025-12-09_17-39_LIVE_Integrated-Dev",
        "2025-12-09_17-30_LIVE_Integrated-Dev",
        "2025-12-09_17-26_DRY-RUN_Integrated-Dev"
       ]
      }
     }
    },
    "workspaces": {
     "currentValue": "Integrated-Dev",
     "nuid": "62bd756c-c066-4af8-9791-27cf1d1502c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "All",
      "label": "Target Workspaces",
      "name": "workspaces",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "All",
        "Integrated-Dev",
        "prod",
        "prod-secure",
        "qa",
        "sandbox",
        "sandbox-new",
        "uat"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "All",
      "label": "Target Workspaces",
      "name": "workspaces",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "All",
        "Integrated-Dev",
        "prod",
        "prod-secure",
        "qa",
        "sandbox",
        "sandbox-new",
        "uat"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
